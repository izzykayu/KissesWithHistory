{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying multi-label texts with Logistic Regression\n",
    "Izzy\n",
    "Started on april 10\n",
    "\n",
    "-- inspired by:\n",
    "\n",
    "Sergei Fironov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import re\n",
    "# Fork of Sergei Fironov's script CNN GLOVE300 3-OOF 3 epochs\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input, Concatenate, Conv1D, Activation, TimeDistributed, Flatten, RepeatVector, Permute,multiply\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout, GRU, GlobalAveragePooling1D, MaxPooling1D, SpatialDropout1D, BatchNormalization\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "# def RNN():\n",
    "#     inputs = Input(name='inputs',shape=[max_len])\n",
    "#     layer = Embedding(max_words,100,input_length=max_len)(inputs)\n",
    "#     layer = LSTM(64)(layer)\n",
    "#     layer = Dense(256,name='FC1')(layer)\n",
    "#     layer = Activation('relu')(layer)\n",
    "#     layer = Dropout(0.5)(layer)\n",
    "#     layer = Dense(1,name='out_layer')(layer)\n",
    "#     layer = Activation('sigmoid')(layer)\n",
    "#     model = Model(inputs=inputs,outputs=layer)\n",
    "#     return model\n",
    "# model = RNN()\n",
    "# model.summary()\n",
    "# model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy', recall, precision, fmeasure])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/Users/isabelmetzger/PycharmProjects/n2c2/LSTM_BASELINES/data/train_May4th_binary.csv\")\n",
    "train_with_ID = train[['ID', 'text']]\n",
    "\n",
    "y_train = train[[\"ABDOMINAL\",\"ADVANCED_CAD\",\"ALCOHOL_ABUSE\",\n",
    "               \"ASP_FOR_MI\",\"CREATININE\",\"DIETSUPP_2MOS\",\n",
    "               \"DRUG_ABUSE\",\"ENGLISH\",\"HBA1C\",\"KETO_1YR\",\n",
    "               \"MAJOR_DIABETES\", \"MAKES_DECISIONS\", \"MI_6MOS\"]]\n",
    "test = pd.read_csv(\"/Users/isabelmetzger/PycharmProjects/n2c2/LSTM_BASELINES/data/test_May4th_binary.csv\")\n",
    "test_with_ID = test[['ID', 'text']]\n",
    "\n",
    "y_test = test[[\"ABDOMINAL\",\"ADVANCED_CAD\",\"ALCOHOL_ABUSE\",\n",
    "               \"ASP_FOR_MI\",\"CREATININE\",\"DIETSUPP_2MOS\",\n",
    "               \"DRUG_ABUSE\",\"ENGLISH\",\"HBA1C\",\"KETO_1YR\",\n",
    "               \"MAJOR_DIABETES\", \"MAKES_DECISIONS\", \"MI_6MOS\"]]\n",
    "\n",
    "# train = pd.read_csv(\"/Users/isabelmetzger/PycharmProjects/n2c2/train/train_May_141_samples.csv\")\n",
    "\n",
    "# test = pd.read_csv(\"/Users/isabelmetzger/PycharmProjects/n2c2/train/test_May_samples.csv\")\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def binary_accuracy(y_true, y_pred):\n",
    "    return K.mean(K.equal(y_true, K.round(y_pred)))\n",
    "\n",
    "\n",
    "def categorical_accuracy(y_true, y_pred):\n",
    "    return K.mean(K.equal(K.argmax(y_true, axis=-1),\n",
    "                          K.argmax(y_pred, axis=-1)))\n",
    "\n",
    "\n",
    "def sparse_categorical_accuracy(y_true, y_pred):\n",
    "    return K.mean(K.equal(K.max(y_true, axis=-1),\n",
    "                          K.cast(K.argmax(y_pred, axis=-1), K.floatx())))\n",
    "\n",
    "\n",
    "def top_k_categorical_accuracy(y_true, y_pred, k=5):\n",
    "    return K.mean(K.in_top_k(y_pred, K.argmax(y_true, axis=-1), k))\n",
    "\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred - y_true))\n",
    "\n",
    "\n",
    "def mean_absolute_error(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_pred - y_true))\n",
    "\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    diff = K.abs((y_true - y_pred) / K.clip(K.abs(y_true),\n",
    "                                            K.epsilon(),\n",
    "                                            None))\n",
    "    return 100. * K.mean(diff)\n",
    "\n",
    "\n",
    "def mean_squared_logarithmic_error(y_true, y_pred):\n",
    "    first_log = K.log(K.clip(y_pred, K.epsilon(), None) + 1.)\n",
    "    second_log = K.log(K.clip(y_true, K.epsilon(), None) + 1.)\n",
    "    return K.mean(K.square(first_log - second_log))\n",
    "\n",
    "\n",
    "def hinge(y_true, y_pred):\n",
    "    return K.mean(K.maximum(1. - y_true * y_pred, 0.))\n",
    "\n",
    "\n",
    "def squared_hinge(y_true, y_pred):\n",
    "    return K.mean(K.square(K.maximum(1. - y_true * y_pred, 0.)))\n",
    "\n",
    "\n",
    "def categorical_crossentropy(y_true, y_pred):\n",
    "    return K.mean(K.categorical_crossentropy(y_pred, y_true))\n",
    "\n",
    "\n",
    "def sparse_categorical_crossentropy(y_true, y_pred):\n",
    "    return K.mean(K.sparse_categorical_crossentropy(y_pred, y_true))\n",
    "\n",
    "\n",
    "def binary_crossentropy(y_true, y_pred):\n",
    "    return K.mean(K.binary_crossentropy(y_pred, y_true))\n",
    "\n",
    "\n",
    "def kullback_leibler_divergence(y_true, y_pred):\n",
    "    y_true = K.clip(y_true, K.epsilon(), 1)\n",
    "    y_pred = K.clip(y_pred, K.epsilon(), 1)\n",
    "    return K.mean(K.sum(y_true * K.log(y_true / y_pred), axis=-1))\n",
    "\n",
    "\n",
    "def poisson(y_true, y_pred):\n",
    "    return K.mean(y_pred - y_true * K.log(y_pred + K.epsilon()))\n",
    "\n",
    "\n",
    "def cosine_proximity(y_true, y_pred):\n",
    "    y_true = K.l2_normalize(y_true, axis=-1)\n",
    "    y_pred = K.l2_normalize(y_pred, axis=-1)\n",
    "    return -K.mean(y_true * y_pred)\n",
    "\n",
    "\n",
    "def matthews_correlation(y_true, y_pred):\n",
    "    \"\"\"Matthews correlation metric.\n",
    "    It is only computed as a batch-wise average, not globally.\n",
    "    Computes the Matthews correlation coefficient measure for quality\n",
    "    of binary classification problems.\n",
    "    \"\"\"\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    "\n",
    "    y_pos = K.round(K.clip(y_true, 0, 1))\n",
    "    y_neg = 1 - y_pos\n",
    "\n",
    "    tp = K.sum(y_pos * y_pred_pos)\n",
    "    tn = K.sum(y_neg * y_pred_neg)\n",
    "\n",
    "    fp = K.sum(y_neg * y_pred_pos)\n",
    "    fn = K.sum(y_pos * y_pred_neg)\n",
    "\n",
    "    numerator = (tp * tn - fp * fn)\n",
    "    denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "\n",
    "    return numerator / (denominator + K.epsilon())\n",
    "\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "    Only computes a batch-wise average of precision.\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "    Only computes a batch-wise average of recall.\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def fbeta_score(y_true, y_pred, beta=1):\n",
    "    \"\"\"Computes the F score.\n",
    "    The F score is the weighted harmonic mean of precision and recall.\n",
    "    Here it is only computed as a batch-wise average, not globally.\n",
    "    This is useful for multi-label classification, where input samples can be\n",
    "    classified as sets of labels. By only using accuracy (precision) a model\n",
    "    would achieve a perfect score by simply assigning every class to every\n",
    "    input. In order to avoid this, a metric should penalize incorrect class\n",
    "    assignments as well (recall). The F-beta score (ranged from 0.0 to 1.0)\n",
    "    computes this, as a weighted mean of the proportion of correct class\n",
    "    assignments vs. the proportion of incorrect class assignments.\n",
    "    With beta = 1, this is equivalent to a F-measure. With beta < 1, assigning\n",
    "    correct classes becomes more important, and with beta > 1 the metric is\n",
    "    instead weighted towards penalizing incorrect class assignments.\n",
    "    \"\"\"\n",
    "    if beta < 0:\n",
    "        raise ValueError('The lowest choosable beta is zero (only precision).')\n",
    "\n",
    "    # If there are no true positives, fix the F score at 0 like sklearn.\n",
    "    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:\n",
    "        return 0\n",
    "\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    bb = beta ** 2\n",
    "    fbeta_score = (1 + bb) * (p * r) / (bb * p + r + K.epsilon())\n",
    "    return fbeta_score\n",
    "\n",
    "\n",
    "def fmeasure(y_true, y_pred):\n",
    "    \"\"\"Computes the f-measure, the harmonic mean of precision and recall.\n",
    "    Here it is only computed as a batch-wise average, not globally.\n",
    "    \"\"\"\n",
    "    return fbeta_score(y_true, y_pred, beta=1)\n",
    "\n",
    "\n",
    "# aliases\n",
    "mse = MSE = mean_squared_error\n",
    "mae = MAE = mean_absolute_error\n",
    "mape = MAPE = mean_absolute_percentage_error\n",
    "msle = MSLE = mean_squared_logarithmic_error\n",
    "cosine = cosine_proximity\n",
    "fscore = f1score = fmeasure\n",
    "print(\"K backend imported!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_sentences_train = train[\"text\"].fillna(\"\").values\n",
    "list_classes = [\"\"]\n",
    "print(len(list_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[\"ABDOMINAL\", \"ADVANCED_CAD\", \"CREATININE\", \"HBA1C\",\n",
    "#                 \"MAJOR_DIABETES\", \"MI_6MOS\"]\n",
    "y = train[list_classes].values\n",
    "max_features = 15500 #\n",
    "maxlen = 7000 #padding length\n",
    "num_folds = 2#2 #number of folds\n",
    "list_sentences_test = test[\"text\"].fillna(\"\").values\n",
    "\n",
    "print('mean text len:',train[\"text\"].str.count('\\S+').mean())\n",
    "print('max text len:',train[\"text\"].str.count('\\S+').max())\n",
    "\n",
    "#tokenizer = Tokenizer()\n",
    "#tokenizer.fit_on_texts(list(list_sentences_train)) #  + list(list_sentences_test)\n",
    "#num_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])\n",
    "#print('num_words',num_words)\n",
    "#max_features = num_words\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train)) # + list(list_sentences_test)\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "print('padding sequences')\n",
    "X_train = {}\n",
    "X_test = {}\n",
    "X_train['text'] = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen, padding='post', truncating='post')\n",
    "X_test['text'] = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen, padding='post', truncating='post')\n",
    "\n",
    "print('numerical variables')\n",
    "\n",
    "#train['tx_numberer'] = train.text.apply(lambda s : sia.polarity_scores(s)['compound'])\n",
    "#test['tx_numb'] = test.text.apply(lambda s : sia.polarity_scores(s)['compound'])\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train['num_vars'] = scaler.fit_transform(train[['num_words','num_comas','num_bangs','num_quotas','avg_word', 'sentiment']])\n",
    "X_test['num_vars'] = scaler.transform(test[['num_words','num_comas','num_bangs','num_quotas','avg_word', 'sentiment']])\n",
    "\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "\n",
    "print('create embedding matrix')\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_cnn(X_train):\n",
    "    global embed_size\n",
    "    inp = Input(shape=(maxlen, ), name=\"text\")\n",
    "    num_vars = Input(shape=[X_train[\"num_vars\"].shape[1]], name=\"num_vars\")\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    z = GlobalMaxPool1D()(x)\n",
    "    x = GlobalMaxPool1D()(Conv1D(embed_size, 4, activation=\"relu\")(x))\n",
    "    x = Concatenate()([x,z,num_vars])\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(13, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=[inp,num_vars], outputs=x)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', precision, recall, fmeasure])\n",
    "    return model   \n",
    "\n",
    "print(\"GLOVE KERAS SIMPLE MODEL! WITH FE! 300Dimension Glovee\\n\")\n",
    "print('start modeling')\n",
    "from sklearn import metrics\n",
    "scores = []\n",
    "predict = np.zeros((test.shape[0],13))\n",
    "prediction_class = np.zeros((test.shape[0],13))\n",
    "oof_predict = np.zeros((train.shape[0],13))\n",
    "f1_scores = []\n",
    "acc_scores = []\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=239)\n",
    "for train_index, test_index in kf.split(X_train['num_vars']):\n",
    "    kfold_X_train = {}\n",
    "    kfold_X_valid = {}\n",
    "    y_train,y_test = y[train_index], y[test_index]\n",
    "    for c in ['text','num_vars']:\n",
    "        kfold_X_train[c] = X_train[c][train_index]\n",
    "        kfold_X_valid[c] = X_train[c][test_index]\n",
    "\n",
    "    model = get_model_cnn(X_train)\n",
    "    model.fit(kfold_X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1)\n",
    "    predict += model.predict(X_test, batch_size=1000) / num_folds\n",
    "    model.predict_generator()\n",
    "    #predict_classes += predict.argmax(axis=-1)\n",
    "    oof_predict[test_index] = model.predict(kfold_X_valid, batch_size=1000)\n",
    "    \n",
    "    #cv_score = metrics.roc_auc_score(y_test, oof_predict[test_index])\n",
    "    #fscore =  metrics.f1_score(y_test, oof_predict[test_index])\n",
    "    #metrics.average_precision_score(y_test, oof_predict[test_index])\n",
    "    print(model.metrics_names)#, model.metrics_tensors)\n",
    "    #scores.append(cv_score)\n",
    "    #f1_scores.append(fscore)\n",
    "    #print('cv score: ', cv_score)\n",
    "    #print('f1 score: ',fscore)\n",
    "    #print(classification_report(y_test, oof_predict[test_index]))\n",
    "\n",
    "#print('Total CV score is {}'.format(np.mean(scores)))    \n",
    "#print('Mean f1 score is {}'.format(np.mean(f1_scores))) \n",
    "sample_submission_classes = pd.DataFrame.from_dict({'ID': test['ID']})\n",
    "sample_submission = pd.DataFrame.from_dict({'ID': test['ID']})\n",
    "oof = pd.DataFrame.from_dict({'ID': train['ID']})\n",
    "for c in list_classes:\n",
    "    oof[c] = np.zeros(len(train))\n",
    "    sample_submission[c] = np.zeros(len(test))\n",
    "    #sample_submission_classes[c] = np.zeros(len(test))\n",
    "\n",
    "#sample_submission_classes[list_classes] = prediction_class\n",
    "#sample_submission.to_csv('11submit_cnn_avg_' + str(num_folds) + '_foldsMay13_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "sample_submission[list_classes] = predict\n",
    "    #class_prediction = target_to_change.argmax(axis=-1) #predict.argmax(axis=-1)\n",
    "    #sample_submission_classes[class_name] = class_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_name in list_classes:\n",
    "    pred_targ = sample_submission[class_name]\n",
    "    pred_class = pred_targ\n",
    "    sample_submission_classes[class_name] = pred_class\n",
    "    \n",
    "sample_submission_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction_classes = np.zeros((test.shape[0],13))\n",
    "\n",
    "for ind, column in enumerate(sample_submission.):\n",
    "    print(ind, column)\n",
    "    \n",
    "import sklearn\n",
    "sklearn.utils.\n",
    "#     sample_submission_classes[list_classes] = prediction_classes\n",
    "#sample_submission_classes.to_csv('11CLASSESsubmit_cnn_avg_' + str(num_folds) + '_foldsMay13_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts_classes = predict.argmax(axis=-1)\n",
    "sample_submission_classes[list_classes] = predicts_classes\n",
    "sample_submission_classes.to_csv('11CLASSESsubmit_cnn_avg_' + str(num_folds) + '_foldsMay13_predictions.csv', index=False)\n",
    "\n",
    "\n",
    "oof[list_classes] = oof_predict\n",
    "# oof.to_csv('cnn_'+str(num_folds)+'_oof_May13.csv', index=False)\n",
    "# print(oof.head())\n",
    "# print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.multiclass import LabelBinarizer\n",
    "# from sklearn.cross_validation import train_test_split\n",
    "    \n",
    "#x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "#x_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_target =[\"ABDOMINAL\",\"ADVANCED_CAD\",\"ALCOHOL_ABUSE\",\n",
    "               \"ASP_FOR_MI\",\"CREATININE\",\"DIETSUPP_2MOS\",\n",
    "               \"DRUG_ABUSE\",\"ENGLISH\",\"HBA1C\",\"KETO_1YR\",\n",
    "               \"MAJOR_DIABETES\", \"MAKES_DECISIONS\", \"MI_6MOS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total rows in test is {}'.format(len(test)))\n",
    "print('Total rows in train is {}'.format(len(train)))\n",
    "print(train[cols_target].sum())\n",
    "print(test[cols_target].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the test data using the earlier fitted vocabulary, into a document-term matrix\n",
    "# test_X_dtm = vect.transform(test_X)\n",
    "# # examine the document-term matrix from X_test\n",
    "test_X_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving a multi-label classification problem\n",
    "One way to approach a multi-label classification problem is to transform the problem into separate single-class classifier problems. This is known as 'problem transformation'. There are three methods:\n",
    "\n",
    "Binary Relevance. This is probably the simplest which treats each label as a separate single classification problems. The key assumption here though, is that there are no correlation among the various labels.\n",
    "\n",
    "\n",
    "Classifier Chains. In this method, the first classifier is trained on the input X. Then the subsequent classifiers are trained on the input X and all previous classifiers' predictions in the chain. This method attempts to draw the signals from the correlation among preceding target variables.\n",
    "\n",
    "\n",
    "Label Powerset. This method transforms the problem into a multi-class problem where the multi-class labels are essentially all the unique label combinations. In our case here, where there are six labels, Label Powerset would in effect turn this into a 2^6 or 64-class problem. {Thanks Joshua for pointing out.}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train_X_dtm = vect.transform(test_X)\n",
    "# examine the document-term matrix from X_test\n",
    "train_X_dtm\n",
    "test_X_dtm = vect.transform(test_X)\n",
    "# examine the document-term matrix from X_test\n",
    "test_X_dtm\n",
    "#X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(X_dtm, train_df.text, train_size=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "sample_submission_NAIVEBAYES = pd.DataFrame.from_dict({'ID': test['ID']})\n",
    "print('MULTINOMIAL NAIVE BAYES WITH TF-IDF MULTI LABEL CLASSIFICATION')\n",
    "model_tfidf_NB = MultinomialNB()\n",
    "accuracy_score_list = []\n",
    "f1_score_list = []\n",
    "val_f1_score_list = []\n",
    "val_accuracy_score_list = []\n",
    "for label in cols_target:\n",
    "    print('\\nPROCESSING {}'.format(label))\n",
    "    y = train_df[label]\n",
    "    test_y = test_df[label]\n",
    "    model_tfidf_NB.fit(X_dtm, y)\n",
    "    y_pred_X = model_tfidf_NB.predict(X_dtm)\n",
    "    print('Training accuracy is {}'.format(metrics.accuracy_score(y, y_pred_X)))\n",
    "    print('Training f1 binary is {}'.format(metrics.f1_score(y, y_pred_X, average='binary'))) \n",
    "    print('Training f1 macro is {}'.format(metrics.f1_score(y, y_pred_X, average='macro')))  \n",
    "    print('Training f1 micro is {}'.format(metrics.f1_score(y, y_pred_X, average='micro')))  \n",
    "    print('Training f1 weighted is {}'.format(metrics.f1_score(y, y_pred_X, average='weighted')))  \n",
    "    #print('Training f1 avg=none is {}'.format(metrics.f1_score(y, y_pred_X, average=None)))\n",
    "    test_y_prob = model_tfidf_NB.predict_proba(test_X_dtm)[:,1]\n",
    "    test_y_pred = model_tfidf_NB.predict(test_X_dtm)\n",
    "    #print(test_y_pred)\n",
    "    accuracy_score_list.append(metrics.accuracy_score(y, y_pred_X))\n",
    "    f1_score_list.append(metrics.f1_score(y, y_pred_X, average='binary'))\n",
    "    #print('\\nValidation accuracy is {}'.format(accuracy_score(test_y, test_y_pred)))\n",
    "    \n",
    "    print('\\nVALIDATION CLASSIFICATION REPORT for {}'.format(label))\n",
    "    print(metrics.classification_report(test_y, test_y_pred))\n",
    "    val_accuracy_score_list.append(metrics.accuracy_score(test_y, test_y_pred))\n",
    "    \n",
    "print('MULTINOMIAL NAIVE BAYES WITH TF-IDF')\n",
    "print('*************************************************************')\n",
    "print('Mean Training Accuracy Score is {}'.format(np.mean(accuracy_score_list)))\n",
    "print('Mean Training F1 Score is {}'.format(np.mean(f1_score_list)))\n",
    "print('*************************************************************')\n",
    "print('Mean Validation Accuracy Score is {}'.format(np.mean(val_accuracy_score_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now on to next model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "submission_binary_MultinomialNaiveBayes = pd.DataFrame(columns=cols_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "logreg = LogisticRegression(C=12.0)\n",
    "\n",
    "print('LOGISTIC REGRESSION MODEL')\n",
    "f1_scores_list = []\n",
    "accuracy_list = []\n",
    "val_accuracy_score_list = []\n",
    "for label in cols_target:\n",
    "    print('... Processing {}'.format(label))\n",
    "    y = train_df[label]\n",
    "    test_y = test_df[label]\n",
    "    # train the model using X_dtm & y\n",
    "    logreg.fit(X_dtm, y)\n",
    "    # compute the training accuracy\n",
    "    y_pred_X = logreg.predict(X_dtm)\n",
    "    print('Training accuracy is {}'.format(accuracy_score(y, y_pred_X)))\n",
    "    print('Training f1 score is is {}'.format(f1_score(y, y_pred_X, pos_label=1)))\n",
    "    #print('Training f1 macro score is is {}'.format(f1_score(y, y_pred_X), 'macro'))\n",
    "    #print('Training f1 micro score is is {}'.format(f1_score(y, y_pred_X), 'micro'))\n",
    "    #print('Training f1 weighted score is is {}'.format(f1_score(y, y_pred_X), 'weighted'))\n",
    "    # compute the predicted probabilities for X_test_dtm\n",
    "    test_y_prob = logreg.predict_proba(test_X_dtm)[:,1]\n",
    "    test_y_pred = logreg.predict(test_X_dtm)\n",
    "    submission_binary[label] = test_y_prob\n",
    "    #print(test_y_pred)\n",
    "    accuracy_list.append(metrics.accuracy_score(y, y_pred_X))\n",
    "    f1_scores_list.append(metrics.f1_score(y, y_pred_X, pos_label=1))\n",
    "    \n",
    "    print('\\nVALIDATION CLASSIFICATION REPORT for {}'.format(label), '\\n')\n",
    "    print(metrics.classification_report(test_y, test_y_pred))\n",
    "    val_accuracy_score_list.append(metrics.accuracy_score(test_y, test_y_pred))\n",
    "    \n",
    "\n",
    "print('*************************************************************')\n",
    "print('Mean Training Accuracy Score is {}'.format(np.mean(accuracy_list)))\n",
    "print('Mean Training F1 Score is {}'.format(np.mean(f1_scores_list)))\n",
    "print('*************************************************************')\n",
    "print('\\nMean Validation Accuracy Score is {}'.format(np.mean(val_accuracy_score_list)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_binary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_binary.to_csv(\"submission_binary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create submission file\n",
    "submission_chains = pd.read_csv('submission_binary.csv')\n",
    "\n",
    "# create a function to add features\n",
    "def add_feature(X, feature_to_add):\n",
    "    '''\n",
    "    Returns sparse feature matrix with added feature.\n",
    "    feature_to_add can also be a list of features.\n",
    "    '''\n",
    "    from scipy.sparse import csr_matrix, hstack\n",
    "    return hstack([X, csr_matrix(feature_to_add).T], 'csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for label in cols_target:\n",
    "    print('... Processing {}'.format(label))\n",
    "    y = train_df[label]\n",
    "    # train the model using X_dtm & y\n",
    "    logreg.fit(X_dtm,y)\n",
    "    # compute the training accuracy\n",
    "    y_pred_X = logreg.predict(X_dtm)\n",
    "    print('Training Accuracy is {}'.format(accuracy_score(y,y_pred_X)))\n",
    "    print('Training f1 score is {}'.format(f1_score(y,y_pred_X)))\n",
    "    # make predictions from test_X\n",
    "    test_y = logreg.predict(test_X_dtm)\n",
    "    test_y_prob = logreg.predict_proba(test_X_dtm)[:,1]\n",
    "    submission_chains[label] = test_y_prob\n",
    "    # chain current label to X_dtm\n",
    "    X_dtm = add_feature(X_dtm, y)\n",
    "    print('Shape of X_dtm is now {}'.format(X_dtm.shape))\n",
    "    # chain current label predictions to test_X_dtm\n",
    "    test_X_dtm = add_feature(test_X_dtm, test_y)\n",
    "    print('Shape of test_X_dtm is now {}'.format(test_X_dtm.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_chains.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate submission file\n",
    "submission_chains.to_csv('submission_chains.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_combined = pd.DataFrame(columns=cols_target)\n",
    "for label in cols_target:\n",
    "    submission_combined[label] = 0.5*(submission_chains[label]+submission_binary[label])\n",
    "\n",
    "submission_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_combined.to_csv('submission_combined.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the model\n",
    "We'll start by creating a bag of words representation, as a term document matrix. We'll use ngrams, as suggested in the NBSVM paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'text'\n",
    "train_df = train\n",
    "test_df = test\n",
    "train_df[text].fillna(\"unknown\", inplace=True)\n",
    "test_df[text].fillna(\"unknown\", inplace=True)\n",
    "\n",
    "import re, string\n",
    "re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "def tokenize(s):\n",
    "    return re_tok.sub(r' \\1 ', s).split()\n",
    "\n",
    "import pandas as pd, numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import re, string\n",
    "n = train_df.shape[0]\n",
    "vec = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n",
    "               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
    "               smooth_idf=1, sublinear_tf=1 )\n",
    "trn_term_doc = vec.fit_transform(train_df[text])\n",
    "test_term_doc = vec.transform(test_df[text])\n",
    "re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "def tokenize(s):\n",
    "    return re_tok.sub(r' \\1 ', s).split()\n",
    "n = train_df.shape[0]\n",
    "\n",
    "vec = TfidfVectorizer(ngram_range=(1,3), tokenizer=tokenize,\n",
    "               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
    "               smooth_idf=1, sublinear_tf=1 )\n",
    "trn_term_doc = vec.fit_transform(train_df[text])\n",
    "test_term_doc = vec.transform(test_df[text])\n",
    "trn_term_doc, test_term_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_target = ['ABDOMINAL',\n",
    " 'ADVANCED_CAD',\n",
    " 'ALCOHOL_ABUSE',\n",
    " 'ASP_FOR_MI',\n",
    " 'CREATININE',\n",
    " 'DIETSUPP_2MOS',\n",
    " 'DRUG_ABUSE',\n",
    " 'ENGLISH',\n",
    " 'HBA1C',\n",
    " 'MAJOR_DIABETES',\n",
    " 'MAKES_DECISIONS',\n",
    " 'MI_6MOS']\n",
    "from sklearn import metrics\n",
    "def pr(y_i, y):\n",
    "    p = x[y==y_i].sum(0)\n",
    "    return (p+1) / ((y==y_i).sum()+1)\n",
    "\n",
    "x = trn_term_doc\n",
    "test_x = test_term_doc\n",
    "def get_mdl(y):\n",
    "    y = y.values\n",
    "    r = np.log(pr(1,y) / pr(0,y))\n",
    "    m = LogisticRegression(C=4, dual=True)\n",
    "    x_nb = x.multiply(r)\n",
    "    return m.fit(x_nb, y), r\n",
    "\n",
    "preds = np.zeros((len(test_df), len(cols_target)))\n",
    "\n",
    "acc_list = []\n",
    "f1_list = []\n",
    "for i, j in enumerate(cols_target):\n",
    "    print('... Processing {}'.format(j))\n",
    "    print('fit', j)\n",
    "    m,r = get_mdl(train_df[j])\n",
    "    preds[:,i] = m.predict_proba(test_x.multiply(r))[:,1]\n",
    "    prediction_y = m.predict(test_x.multiply(r))\n",
    "    y_test = test_df[j]\n",
    "    print(metrics.classification_report(y_test, prediction_y))\n",
    "    f1 = metrics.f1_score(y_test, prediction_y, pos_label=1)\n",
    "    f1_list.append(f1)\n",
    "    print(f1)\n",
    "    \n",
    "\n",
    "\n",
    "print('Total f1 score is {}'.format(np.mean(f1_list)))\n",
    "print('Total acc score is {}'.format(np.mean(acc_list)))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions = pd.concat([pd.DataFrame(preds, columns = cols_target)], axis=1)\n",
    "\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tfidf_sklearn_model_predictions = predictions\n",
    "\n",
    "Tfidf_sklearn_model_predictions.to_csv(\"Tfidf_sklearn_model_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using binary relevance\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# initialize binary relevance multi-label classifier\n",
    "# with a gaussian naive bayes base classifier\n",
    "#classifier = BinaryRelevance(GaussianNB())\n",
    "\n",
    "# train\n",
    "# classifier.fit(trn_term_doc, test_term_doc)\n",
    "\n",
    "# # predict\n",
    "# predictions = classifier.predict(trn_term_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using binary relevance\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# initialize binary relevance multi-label classifier\n",
    "# with a gaussian naive bayes base classifier\n",
    "classifier = BinaryRelevance(GaussianNB())\n",
    "for label in cols_target:\n",
    "    print('Processing {}'.format(label))\n",
    "    y = train_df[label]\n",
    "    classifier.fit(X_train_tfidf, y)\n",
    "    y_pred_X = classifier.predict(X_dtm)\n",
    "    print('Training accuracy is {}'.format(accuracy_score(y, y_pred_X)))\n",
    "    print('Training f1 macro is {}'.format(f1_score(y, y_pred_X, average='macro')))  \n",
    "    print('Training f1 micro is {}'.format(f1_score(y, y_pred_X, average='micro')))  \n",
    "    print('Training f1 weighted is {}'.format(f1_score(y, y_pred_X, average='weighted')))  \n",
    "    test_y_prob = classifier.predict_proba(test_X_dtm)[:,1]\n",
    "\n",
    "\n",
    "# train\n",
    "#classifier.fit(X_train_tfidf, y_train_tfidf)\n",
    "\n",
    "# predict\n",
    "predictions = classifier.predict(X_test_tfidf)\n",
    "\n",
    "\n",
    "metrics.accuracy_score(y_test,predictions)\n",
    "from sklearn.metrics import f1_score\n",
    "metrics.f1_score(y_test_tfidf, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
