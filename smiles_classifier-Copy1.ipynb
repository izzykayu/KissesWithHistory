{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ** Isabel Metzger **\n",
    " Last Modified Jun 19th at 07:30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we are importing the dili_mordred file from Zach's notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1242 ----- this is the list of unique smiles\n"
     ]
    }
   ],
   "source": [
    "## lstm simple zerostart no feature engineering\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import bz2\n",
    "\n",
    "# importing train and test files\n",
    "with bz2.open(\"/root/ToxNeuralNets/result_files/DILI_W_MORDRED.csv.bz2\") as f:\n",
    "    full_df = pd.read_csv(f)\n",
    "\n",
    "def get_binary_target1_stats(train):\n",
    "    print('number of labeled 1')\n",
    "    print(train.label.sum())\n",
    "    print('number of rows')\n",
    "    print(train.shape[0])\n",
    "    print('percentage of target')\n",
    "    print(round(train.label.sum()/train.shape[0]*100))\n",
    "    \n",
    "print(len(list(full_df['Smiles'].unique())), '----- this is the list of unique smiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1527, 1617)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'index': [0],\n",
       " 'columns': ['Unnamed: 0',\n",
       "  'Smiles',\n",
       "  'Liver',\n",
       "  'Mol_ID',\n",
       "  'ABC',\n",
       "  'ABCGG',\n",
       "  'nAcid',\n",
       "  'nBase',\n",
       "  'SpAbs_A',\n",
       "  'SpMax_A',\n",
       "  'SpDiam_A',\n",
       "  'SpAD_A',\n",
       "  'SpMAD_A',\n",
       "  'LogEE_A',\n",
       "  'VE1_A',\n",
       "  'VE2_A',\n",
       "  'VE3_A',\n",
       "  'VR1_A',\n",
       "  'VR2_A',\n",
       "  'VR3_A',\n",
       "  'nAromAtom',\n",
       "  'nAromBond',\n",
       "  'nAtom',\n",
       "  'nHeavyAtom',\n",
       "  'nSpiro',\n",
       "  'nBridgehead',\n",
       "  'nHetero',\n",
       "  'nH',\n",
       "  'nB',\n",
       "  'nC',\n",
       "  'nN',\n",
       "  'nO',\n",
       "  'nS',\n",
       "  'nP',\n",
       "  'nF',\n",
       "  'nCl',\n",
       "  'nBr',\n",
       "  'nI',\n",
       "  'nX',\n",
       "  'ATS0dv',\n",
       "  'ATS1dv',\n",
       "  'ATS2dv',\n",
       "  'ATS3dv',\n",
       "  'ATS4dv',\n",
       "  'ATS5dv',\n",
       "  'ATS6dv',\n",
       "  'ATS7dv',\n",
       "  'ATS8dv',\n",
       "  'ATS0d',\n",
       "  'ATS1d',\n",
       "  'ATS2d',\n",
       "  'ATS3d',\n",
       "  'ATS4d',\n",
       "  'ATS5d',\n",
       "  'ATS6d',\n",
       "  'ATS7d',\n",
       "  'ATS8d',\n",
       "  'ATS0s',\n",
       "  'ATS1s',\n",
       "  'ATS2s',\n",
       "  'ATS3s',\n",
       "  'ATS4s',\n",
       "  'ATS5s',\n",
       "  'ATS6s',\n",
       "  'ATS7s',\n",
       "  'ATS8s',\n",
       "  'ATS0Z',\n",
       "  'ATS1Z',\n",
       "  'ATS2Z',\n",
       "  'ATS3Z',\n",
       "  'ATS4Z',\n",
       "  'ATS5Z',\n",
       "  'ATS6Z',\n",
       "  'ATS7Z',\n",
       "  'ATS8Z',\n",
       "  'ATS0m',\n",
       "  'ATS1m',\n",
       "  'ATS2m',\n",
       "  'ATS3m',\n",
       "  'ATS4m',\n",
       "  'ATS5m',\n",
       "  'ATS6m',\n",
       "  'ATS7m',\n",
       "  'ATS8m',\n",
       "  'ATS0v',\n",
       "  'ATS1v',\n",
       "  'ATS2v',\n",
       "  'ATS3v',\n",
       "  'ATS4v',\n",
       "  'ATS5v',\n",
       "  'ATS6v',\n",
       "  'ATS7v',\n",
       "  'ATS8v',\n",
       "  'ATS0se',\n",
       "  'ATS1se',\n",
       "  'ATS2se',\n",
       "  'ATS3se',\n",
       "  'ATS4se',\n",
       "  'ATS5se',\n",
       "  'ATS6se',\n",
       "  'ATS7se',\n",
       "  'ATS8se',\n",
       "  'ATS0pe',\n",
       "  'ATS1pe',\n",
       "  'ATS2pe',\n",
       "  'ATS3pe',\n",
       "  'ATS4pe',\n",
       "  'ATS5pe',\n",
       "  'ATS6pe',\n",
       "  'ATS7pe',\n",
       "  'ATS8pe',\n",
       "  'ATS0are',\n",
       "  'ATS1are',\n",
       "  'ATS2are',\n",
       "  'ATS3are',\n",
       "  'ATS4are',\n",
       "  'ATS5are',\n",
       "  'ATS6are',\n",
       "  'ATS7are',\n",
       "  'ATS8are',\n",
       "  'ATS0p',\n",
       "  'ATS1p',\n",
       "  'ATS2p',\n",
       "  'ATS3p',\n",
       "  'ATS4p',\n",
       "  'ATS5p',\n",
       "  'ATS6p',\n",
       "  'ATS7p',\n",
       "  'ATS8p',\n",
       "  'ATS0i',\n",
       "  'ATS1i',\n",
       "  'ATS2i',\n",
       "  'ATS3i',\n",
       "  'ATS4i',\n",
       "  'ATS5i',\n",
       "  'ATS6i',\n",
       "  'ATS7i',\n",
       "  'ATS8i',\n",
       "  'AATS0dv',\n",
       "  'AATS1dv',\n",
       "  'AATS2dv',\n",
       "  'AATS3dv',\n",
       "  'AATS4dv',\n",
       "  'AATS5dv',\n",
       "  'AATS6dv',\n",
       "  'AATS7dv',\n",
       "  'AATS8dv',\n",
       "  'AATS0d',\n",
       "  'AATS1d',\n",
       "  'AATS2d',\n",
       "  'AATS3d',\n",
       "  'AATS4d',\n",
       "  'AATS5d',\n",
       "  'AATS6d',\n",
       "  'AATS7d',\n",
       "  'AATS8d',\n",
       "  'AATS0s',\n",
       "  'AATS1s',\n",
       "  'AATS2s',\n",
       "  'AATS3s',\n",
       "  'AATS4s',\n",
       "  'AATS5s',\n",
       "  'AATS6s',\n",
       "  'AATS7s',\n",
       "  'AATS8s',\n",
       "  'AATS0Z',\n",
       "  'AATS1Z',\n",
       "  'AATS2Z',\n",
       "  'AATS3Z',\n",
       "  'AATS4Z',\n",
       "  'AATS5Z',\n",
       "  'AATS6Z',\n",
       "  'AATS7Z',\n",
       "  'AATS8Z',\n",
       "  'AATS0m',\n",
       "  'AATS1m',\n",
       "  'AATS2m',\n",
       "  'AATS3m',\n",
       "  'AATS4m',\n",
       "  'AATS5m',\n",
       "  'AATS6m',\n",
       "  'AATS7m',\n",
       "  'AATS8m',\n",
       "  'AATS0v',\n",
       "  'AATS1v',\n",
       "  'AATS2v',\n",
       "  'AATS3v',\n",
       "  'AATS4v',\n",
       "  'AATS5v',\n",
       "  'AATS6v',\n",
       "  'AATS7v',\n",
       "  'AATS8v',\n",
       "  'AATS0se',\n",
       "  'AATS1se',\n",
       "  'AATS2se',\n",
       "  'AATS3se',\n",
       "  'AATS4se',\n",
       "  'AATS5se',\n",
       "  'AATS6se',\n",
       "  'AATS7se',\n",
       "  'AATS8se',\n",
       "  'AATS0pe',\n",
       "  'AATS1pe',\n",
       "  'AATS2pe',\n",
       "  'AATS3pe',\n",
       "  'AATS4pe',\n",
       "  'AATS5pe',\n",
       "  'AATS6pe',\n",
       "  'AATS7pe',\n",
       "  'AATS8pe',\n",
       "  'AATS0are',\n",
       "  'AATS1are',\n",
       "  'AATS2are',\n",
       "  'AATS3are',\n",
       "  'AATS4are',\n",
       "  'AATS5are',\n",
       "  'AATS6are',\n",
       "  'AATS7are',\n",
       "  'AATS8are',\n",
       "  'AATS0p',\n",
       "  'AATS1p',\n",
       "  'AATS2p',\n",
       "  'AATS3p',\n",
       "  'AATS4p',\n",
       "  'AATS5p',\n",
       "  'AATS6p',\n",
       "  'AATS7p',\n",
       "  'AATS8p',\n",
       "  'AATS0i',\n",
       "  'AATS1i',\n",
       "  'AATS2i',\n",
       "  'AATS3i',\n",
       "  'AATS4i',\n",
       "  'AATS5i',\n",
       "  'AATS6i',\n",
       "  'AATS7i',\n",
       "  'AATS8i',\n",
       "  'ATSC0c',\n",
       "  'ATSC1c',\n",
       "  'ATSC2c',\n",
       "  'ATSC3c',\n",
       "  'ATSC4c',\n",
       "  'ATSC5c',\n",
       "  'ATSC6c',\n",
       "  'ATSC7c',\n",
       "  'ATSC8c',\n",
       "  'ATSC0dv',\n",
       "  'ATSC1dv',\n",
       "  'ATSC2dv',\n",
       "  'ATSC3dv',\n",
       "  'ATSC4dv',\n",
       "  'ATSC5dv',\n",
       "  'ATSC6dv',\n",
       "  'ATSC7dv',\n",
       "  'ATSC8dv',\n",
       "  'ATSC0d',\n",
       "  'ATSC1d',\n",
       "  'ATSC2d',\n",
       "  'ATSC3d',\n",
       "  'ATSC4d',\n",
       "  'ATSC5d',\n",
       "  'ATSC6d',\n",
       "  'ATSC7d',\n",
       "  'ATSC8d',\n",
       "  'ATSC0s',\n",
       "  'ATSC1s',\n",
       "  'ATSC2s',\n",
       "  'ATSC3s',\n",
       "  'ATSC4s',\n",
       "  'ATSC5s',\n",
       "  'ATSC6s',\n",
       "  'ATSC7s',\n",
       "  'ATSC8s',\n",
       "  'ATSC0Z',\n",
       "  'ATSC1Z',\n",
       "  'ATSC2Z',\n",
       "  'ATSC3Z',\n",
       "  'ATSC4Z',\n",
       "  'ATSC5Z',\n",
       "  'ATSC6Z',\n",
       "  'ATSC7Z',\n",
       "  'ATSC8Z',\n",
       "  'ATSC0m',\n",
       "  'ATSC1m',\n",
       "  'ATSC2m',\n",
       "  'ATSC3m',\n",
       "  'ATSC4m',\n",
       "  'ATSC5m',\n",
       "  'ATSC6m',\n",
       "  'ATSC7m',\n",
       "  'ATSC8m',\n",
       "  'ATSC0v',\n",
       "  'ATSC1v',\n",
       "  'ATSC2v',\n",
       "  'ATSC3v',\n",
       "  'ATSC4v',\n",
       "  'ATSC5v',\n",
       "  'ATSC6v',\n",
       "  'ATSC7v',\n",
       "  'ATSC8v',\n",
       "  'ATSC0se',\n",
       "  'ATSC1se',\n",
       "  'ATSC2se',\n",
       "  'ATSC3se',\n",
       "  'ATSC4se',\n",
       "  'ATSC5se',\n",
       "  'ATSC6se',\n",
       "  'ATSC7se',\n",
       "  'ATSC8se',\n",
       "  'ATSC0pe',\n",
       "  'ATSC1pe',\n",
       "  'ATSC2pe',\n",
       "  'ATSC3pe',\n",
       "  'ATSC4pe',\n",
       "  'ATSC5pe',\n",
       "  'ATSC6pe',\n",
       "  'ATSC7pe',\n",
       "  'ATSC8pe',\n",
       "  'ATSC0are',\n",
       "  'ATSC1are',\n",
       "  'ATSC2are',\n",
       "  'ATSC3are',\n",
       "  'ATSC4are',\n",
       "  'ATSC5are',\n",
       "  'ATSC6are',\n",
       "  'ATSC7are',\n",
       "  'ATSC8are',\n",
       "  'ATSC0p',\n",
       "  'ATSC1p',\n",
       "  'ATSC2p',\n",
       "  'ATSC3p',\n",
       "  'ATSC4p',\n",
       "  'ATSC5p',\n",
       "  'ATSC6p',\n",
       "  'ATSC7p',\n",
       "  'ATSC8p',\n",
       "  'ATSC0i',\n",
       "  'ATSC1i',\n",
       "  'ATSC2i',\n",
       "  'ATSC3i',\n",
       "  'ATSC4i',\n",
       "  'ATSC5i',\n",
       "  'ATSC6i',\n",
       "  'ATSC7i',\n",
       "  'ATSC8i',\n",
       "  'AATSC0c',\n",
       "  'AATSC1c',\n",
       "  'AATSC2c',\n",
       "  'AATSC3c',\n",
       "  'AATSC4c',\n",
       "  'AATSC5c',\n",
       "  'AATSC6c',\n",
       "  'AATSC7c',\n",
       "  'AATSC8c',\n",
       "  'AATSC0dv',\n",
       "  'AATSC1dv',\n",
       "  'AATSC2dv',\n",
       "  'AATSC3dv',\n",
       "  'AATSC4dv',\n",
       "  'AATSC5dv',\n",
       "  'AATSC6dv',\n",
       "  'AATSC7dv',\n",
       "  'AATSC8dv',\n",
       "  'AATSC0d',\n",
       "  'AATSC1d',\n",
       "  'AATSC2d',\n",
       "  'AATSC3d',\n",
       "  'AATSC4d',\n",
       "  'AATSC5d',\n",
       "  'AATSC6d',\n",
       "  'AATSC7d',\n",
       "  'AATSC8d',\n",
       "  'AATSC0s',\n",
       "  'AATSC1s',\n",
       "  'AATSC2s',\n",
       "  'AATSC3s',\n",
       "  'AATSC4s',\n",
       "  'AATSC5s',\n",
       "  'AATSC6s',\n",
       "  'AATSC7s',\n",
       "  'AATSC8s',\n",
       "  'AATSC0Z',\n",
       "  'AATSC1Z',\n",
       "  'AATSC2Z',\n",
       "  'AATSC3Z',\n",
       "  'AATSC4Z',\n",
       "  'AATSC5Z',\n",
       "  'AATSC6Z',\n",
       "  'AATSC7Z',\n",
       "  'AATSC8Z',\n",
       "  'AATSC0m',\n",
       "  'AATSC1m',\n",
       "  'AATSC2m',\n",
       "  'AATSC3m',\n",
       "  'AATSC4m',\n",
       "  'AATSC5m',\n",
       "  'AATSC6m',\n",
       "  'AATSC7m',\n",
       "  'AATSC8m',\n",
       "  'AATSC0v',\n",
       "  'AATSC1v',\n",
       "  'AATSC2v',\n",
       "  'AATSC3v',\n",
       "  'AATSC4v',\n",
       "  'AATSC5v',\n",
       "  'AATSC6v',\n",
       "  'AATSC7v',\n",
       "  'AATSC8v',\n",
       "  'AATSC0se',\n",
       "  'AATSC1se',\n",
       "  'AATSC2se',\n",
       "  'AATSC3se',\n",
       "  'AATSC4se',\n",
       "  'AATSC5se',\n",
       "  'AATSC6se',\n",
       "  'AATSC7se',\n",
       "  'AATSC8se',\n",
       "  'AATSC0pe',\n",
       "  'AATSC1pe',\n",
       "  'AATSC2pe',\n",
       "  'AATSC3pe',\n",
       "  'AATSC4pe',\n",
       "  'AATSC5pe',\n",
       "  'AATSC6pe',\n",
       "  'AATSC7pe',\n",
       "  'AATSC8pe',\n",
       "  'AATSC0are',\n",
       "  'AATSC1are',\n",
       "  'AATSC2are',\n",
       "  'AATSC3are',\n",
       "  'AATSC4are',\n",
       "  'AATSC5are',\n",
       "  'AATSC6are',\n",
       "  'AATSC7are',\n",
       "  'AATSC8are',\n",
       "  'AATSC0p',\n",
       "  'AATSC1p',\n",
       "  'AATSC2p',\n",
       "  'AATSC3p',\n",
       "  'AATSC4p',\n",
       "  'AATSC5p',\n",
       "  'AATSC6p',\n",
       "  'AATSC7p',\n",
       "  'AATSC8p',\n",
       "  'AATSC0i',\n",
       "  'AATSC1i',\n",
       "  'AATSC2i',\n",
       "  'AATSC3i',\n",
       "  'AATSC4i',\n",
       "  'AATSC5i',\n",
       "  'AATSC6i',\n",
       "  'AATSC7i',\n",
       "  'AATSC8i',\n",
       "  'MATS1c',\n",
       "  'MATS2c',\n",
       "  'MATS3c',\n",
       "  'MATS4c',\n",
       "  'MATS5c',\n",
       "  'MATS6c',\n",
       "  'MATS7c',\n",
       "  'MATS8c',\n",
       "  'MATS1dv',\n",
       "  'MATS2dv',\n",
       "  'MATS3dv',\n",
       "  'MATS4dv',\n",
       "  'MATS5dv',\n",
       "  'MATS6dv',\n",
       "  'MATS7dv',\n",
       "  'MATS8dv',\n",
       "  'MATS1d',\n",
       "  'MATS2d',\n",
       "  'MATS3d',\n",
       "  'MATS4d',\n",
       "  'MATS5d',\n",
       "  'MATS6d',\n",
       "  'MATS7d',\n",
       "  'MATS8d',\n",
       "  'MATS1s',\n",
       "  'MATS2s',\n",
       "  'MATS3s',\n",
       "  'MATS4s',\n",
       "  'MATS5s',\n",
       "  'MATS6s',\n",
       "  'MATS7s',\n",
       "  'MATS8s',\n",
       "  'MATS1Z',\n",
       "  'MATS2Z',\n",
       "  'MATS3Z',\n",
       "  'MATS4Z',\n",
       "  'MATS5Z',\n",
       "  'MATS6Z',\n",
       "  'MATS7Z',\n",
       "  'MATS8Z',\n",
       "  'MATS1m',\n",
       "  'MATS2m',\n",
       "  'MATS3m',\n",
       "  'MATS4m',\n",
       "  'MATS5m',\n",
       "  'MATS6m',\n",
       "  'MATS7m',\n",
       "  'MATS8m',\n",
       "  'MATS1v',\n",
       "  'MATS2v',\n",
       "  'MATS3v',\n",
       "  'MATS4v',\n",
       "  'MATS5v',\n",
       "  'MATS6v',\n",
       "  'MATS7v',\n",
       "  'MATS8v',\n",
       "  'MATS1se',\n",
       "  'MATS2se',\n",
       "  'MATS3se',\n",
       "  'MATS4se',\n",
       "  'MATS5se',\n",
       "  'MATS6se',\n",
       "  'MATS7se',\n",
       "  'MATS8se',\n",
       "  'MATS1pe',\n",
       "  'MATS2pe',\n",
       "  'MATS3pe',\n",
       "  'MATS4pe',\n",
       "  'MATS5pe',\n",
       "  'MATS6pe',\n",
       "  'MATS7pe',\n",
       "  'MATS8pe',\n",
       "  'MATS1are',\n",
       "  'MATS2are',\n",
       "  'MATS3are',\n",
       "  'MATS4are',\n",
       "  'MATS5are',\n",
       "  'MATS6are',\n",
       "  'MATS7are',\n",
       "  'MATS8are',\n",
       "  'MATS1p',\n",
       "  'MATS2p',\n",
       "  'MATS3p',\n",
       "  'MATS4p',\n",
       "  'MATS5p',\n",
       "  'MATS6p',\n",
       "  'MATS7p',\n",
       "  'MATS8p',\n",
       "  'MATS1i',\n",
       "  'MATS2i',\n",
       "  'MATS3i',\n",
       "  'MATS4i',\n",
       "  'MATS5i',\n",
       "  'MATS6i',\n",
       "  'MATS7i',\n",
       "  'MATS8i',\n",
       "  'GATS1c',\n",
       "  'GATS2c',\n",
       "  'GATS3c',\n",
       "  'GATS4c',\n",
       "  'GATS5c',\n",
       "  'GATS6c',\n",
       "  'GATS7c',\n",
       "  'GATS8c',\n",
       "  'GATS1dv',\n",
       "  'GATS2dv',\n",
       "  'GATS3dv',\n",
       "  'GATS4dv',\n",
       "  'GATS5dv',\n",
       "  'GATS6dv',\n",
       "  'GATS7dv',\n",
       "  'GATS8dv',\n",
       "  'GATS1d',\n",
       "  'GATS2d',\n",
       "  'GATS3d',\n",
       "  'GATS4d',\n",
       "  'GATS5d',\n",
       "  'GATS6d',\n",
       "  'GATS7d',\n",
       "  'GATS8d',\n",
       "  'GATS1s',\n",
       "  'GATS2s',\n",
       "  'GATS3s',\n",
       "  'GATS4s',\n",
       "  'GATS5s',\n",
       "  'GATS6s',\n",
       "  'GATS7s',\n",
       "  'GATS8s',\n",
       "  'GATS1Z',\n",
       "  'GATS2Z',\n",
       "  'GATS3Z',\n",
       "  'GATS4Z',\n",
       "  'GATS5Z',\n",
       "  'GATS6Z',\n",
       "  'GATS7Z',\n",
       "  'GATS8Z',\n",
       "  'GATS1m',\n",
       "  'GATS2m',\n",
       "  'GATS3m',\n",
       "  'GATS4m',\n",
       "  'GATS5m',\n",
       "  'GATS6m',\n",
       "  'GATS7m',\n",
       "  'GATS8m',\n",
       "  'GATS1v',\n",
       "  'GATS2v',\n",
       "  'GATS3v',\n",
       "  'GATS4v',\n",
       "  'GATS5v',\n",
       "  'GATS6v',\n",
       "  'GATS7v',\n",
       "  'GATS8v',\n",
       "  'GATS1se',\n",
       "  'GATS2se',\n",
       "  'GATS3se',\n",
       "  'GATS4se',\n",
       "  'GATS5se',\n",
       "  'GATS6se',\n",
       "  'GATS7se',\n",
       "  'GATS8se',\n",
       "  'GATS1pe',\n",
       "  'GATS2pe',\n",
       "  'GATS3pe',\n",
       "  'GATS4pe',\n",
       "  'GATS5pe',\n",
       "  'GATS6pe',\n",
       "  'GATS7pe',\n",
       "  'GATS8pe',\n",
       "  'GATS1are',\n",
       "  'GATS2are',\n",
       "  'GATS3are',\n",
       "  'GATS4are',\n",
       "  'GATS5are',\n",
       "  'GATS6are',\n",
       "  'GATS7are',\n",
       "  'GATS8are',\n",
       "  'GATS1p',\n",
       "  'GATS2p',\n",
       "  'GATS3p',\n",
       "  'GATS4p',\n",
       "  'GATS5p',\n",
       "  'GATS6p',\n",
       "  'GATS7p',\n",
       "  'GATS8p',\n",
       "  'GATS1i',\n",
       "  'GATS2i',\n",
       "  'GATS3i',\n",
       "  'GATS4i',\n",
       "  'GATS5i',\n",
       "  'GATS6i',\n",
       "  'GATS7i',\n",
       "  'GATS8i',\n",
       "  'BCUTc-1h',\n",
       "  'BCUTc-1l',\n",
       "  'BCUTdv-1h',\n",
       "  'BCUTdv-1l',\n",
       "  'BCUTd-1h',\n",
       "  'BCUTd-1l',\n",
       "  'BCUTs-1h',\n",
       "  'BCUTs-1l',\n",
       "  'BCUTZ-1h',\n",
       "  'BCUTZ-1l',\n",
       "  'BCUTm-1h',\n",
       "  'BCUTm-1l',\n",
       "  'BCUTv-1h',\n",
       "  'BCUTv-1l',\n",
       "  'BCUTse-1h',\n",
       "  'BCUTse-1l',\n",
       "  'BCUTpe-1h',\n",
       "  'BCUTpe-1l',\n",
       "  'BCUTare-1h',\n",
       "  'BCUTare-1l',\n",
       "  'BCUTp-1h',\n",
       "  'BCUTp-1l',\n",
       "  'BCUTi-1h',\n",
       "  'BCUTi-1l',\n",
       "  'BalabanJ',\n",
       "  'SpAbs_DzZ',\n",
       "  'SpMax_DzZ',\n",
       "  'SpDiam_DzZ',\n",
       "  'SpAD_DzZ',\n",
       "  'SpMAD_DzZ',\n",
       "  'LogEE_DzZ',\n",
       "  'SM1_DzZ',\n",
       "  'VE1_DzZ',\n",
       "  'VE2_DzZ',\n",
       "  'VE3_DzZ',\n",
       "  'VR1_DzZ',\n",
       "  'VR2_DzZ',\n",
       "  'VR3_DzZ',\n",
       "  'SpAbs_Dzm',\n",
       "  'SpMax_Dzm',\n",
       "  'SpDiam_Dzm',\n",
       "  'SpAD_Dzm',\n",
       "  'SpMAD_Dzm',\n",
       "  'LogEE_Dzm',\n",
       "  'SM1_Dzm',\n",
       "  'VE1_Dzm',\n",
       "  'VE2_Dzm',\n",
       "  'VE3_Dzm',\n",
       "  'VR1_Dzm',\n",
       "  'VR2_Dzm',\n",
       "  'VR3_Dzm',\n",
       "  'SpAbs_Dzv',\n",
       "  'SpMax_Dzv',\n",
       "  'SpDiam_Dzv',\n",
       "  'SpAD_Dzv',\n",
       "  'SpMAD_Dzv',\n",
       "  'LogEE_Dzv',\n",
       "  'SM1_Dzv',\n",
       "  'VE1_Dzv',\n",
       "  'VE2_Dzv',\n",
       "  'VE3_Dzv',\n",
       "  'VR1_Dzv',\n",
       "  'VR2_Dzv',\n",
       "  'VR3_Dzv',\n",
       "  'SpAbs_Dzse',\n",
       "  'SpMax_Dzse',\n",
       "  'SpDiam_Dzse',\n",
       "  'SpAD_Dzse',\n",
       "  'SpMAD_Dzse',\n",
       "  'LogEE_Dzse',\n",
       "  'SM1_Dzse',\n",
       "  'VE1_Dzse',\n",
       "  'VE2_Dzse',\n",
       "  'VE3_Dzse',\n",
       "  'VR1_Dzse',\n",
       "  'VR2_Dzse',\n",
       "  'VR3_Dzse',\n",
       "  'SpAbs_Dzpe',\n",
       "  'SpMax_Dzpe',\n",
       "  'SpDiam_Dzpe',\n",
       "  'SpAD_Dzpe',\n",
       "  'SpMAD_Dzpe',\n",
       "  'LogEE_Dzpe',\n",
       "  'SM1_Dzpe',\n",
       "  'VE1_Dzpe',\n",
       "  'VE2_Dzpe',\n",
       "  'VE3_Dzpe',\n",
       "  'VR1_Dzpe',\n",
       "  'VR2_Dzpe',\n",
       "  'VR3_Dzpe',\n",
       "  'SpAbs_Dzare',\n",
       "  'SpMax_Dzare',\n",
       "  'SpDiam_Dzare',\n",
       "  'SpAD_Dzare',\n",
       "  'SpMAD_Dzare',\n",
       "  'LogEE_Dzare',\n",
       "  'SM1_Dzare',\n",
       "  'VE1_Dzare',\n",
       "  'VE2_Dzare',\n",
       "  'VE3_Dzare',\n",
       "  'VR1_Dzare',\n",
       "  'VR2_Dzare',\n",
       "  'VR3_Dzare',\n",
       "  'SpAbs_Dzp',\n",
       "  'SpMax_Dzp',\n",
       "  'SpDiam_Dzp',\n",
       "  'SpAD_Dzp',\n",
       "  'SpMAD_Dzp',\n",
       "  'LogEE_Dzp',\n",
       "  'SM1_Dzp',\n",
       "  'VE1_Dzp',\n",
       "  'VE2_Dzp',\n",
       "  'VE3_Dzp',\n",
       "  'VR1_Dzp',\n",
       "  'VR2_Dzp',\n",
       "  'VR3_Dzp',\n",
       "  'SpAbs_Dzi',\n",
       "  'SpMax_Dzi',\n",
       "  'SpDiam_Dzi',\n",
       "  'SpAD_Dzi',\n",
       "  'SpMAD_Dzi',\n",
       "  'LogEE_Dzi',\n",
       "  'SM1_Dzi',\n",
       "  'VE1_Dzi',\n",
       "  'VE2_Dzi',\n",
       "  'VE3_Dzi',\n",
       "  'VR1_Dzi',\n",
       "  'VR2_Dzi',\n",
       "  'VR3_Dzi',\n",
       "  'BertzCT',\n",
       "  'nBonds',\n",
       "  'nBondsO',\n",
       "  'nBondsS',\n",
       "  'nBondsD',\n",
       "  'nBondsT',\n",
       "  'nBondsA',\n",
       "  'nBondsM',\n",
       "  'nBondsKS',\n",
       "  'nBondsKD',\n",
       "  'RNCG',\n",
       "  'RPCG',\n",
       "  'C1SP1',\n",
       "  'C2SP1',\n",
       "  'C1SP2',\n",
       "  'C2SP2',\n",
       "  'C3SP2',\n",
       "  'C1SP3',\n",
       "  'C2SP3',\n",
       "  'C3SP3',\n",
       "  'C4SP3',\n",
       "  'HybRatio',\n",
       "  'FCSP3',\n",
       "  'Xch-3d',\n",
       "  'Xch-4d',\n",
       "  'Xch-5d',\n",
       "  'Xch-6d',\n",
       "  'Xch-7d',\n",
       "  'Xch-3dv',\n",
       "  'Xch-4dv',\n",
       "  'Xch-5dv',\n",
       "  'Xch-6dv',\n",
       "  'Xch-7dv',\n",
       "  'Xc-3d',\n",
       "  'Xc-4d',\n",
       "  'Xc-5d',\n",
       "  'Xc-6d',\n",
       "  'Xc-3dv',\n",
       "  'Xc-4dv',\n",
       "  'Xc-5dv',\n",
       "  'Xc-6dv',\n",
       "  'Xpc-4d',\n",
       "  'Xpc-5d',\n",
       "  'Xpc-6d',\n",
       "  'Xpc-4dv',\n",
       "  'Xpc-5dv',\n",
       "  'Xpc-6dv',\n",
       "  'Xp-0d',\n",
       "  'Xp-1d',\n",
       "  'Xp-2d',\n",
       "  'Xp-3d',\n",
       "  'Xp-4d',\n",
       "  'Xp-5d',\n",
       "  'Xp-6d',\n",
       "  'Xp-7d',\n",
       "  'AXp-0d',\n",
       "  'AXp-1d',\n",
       "  'AXp-2d',\n",
       "  'AXp-3d',\n",
       "  'AXp-4d',\n",
       "  'AXp-5d',\n",
       "  'AXp-6d',\n",
       "  'AXp-7d',\n",
       "  'Xp-0dv',\n",
       "  'Xp-1dv',\n",
       "  'Xp-2dv',\n",
       "  'Xp-3dv',\n",
       "  'Xp-4dv',\n",
       "  'Xp-5dv',\n",
       "  'Xp-6dv',\n",
       "  'Xp-7dv',\n",
       "  'AXp-0dv',\n",
       "  'AXp-1dv',\n",
       "  'AXp-2dv',\n",
       "  'AXp-3dv',\n",
       "  'AXp-4dv',\n",
       "  'AXp-5dv',\n",
       "  'AXp-6dv',\n",
       "  'AXp-7dv',\n",
       "  'SZ',\n",
       "  'Sm',\n",
       "  'Sv',\n",
       "  'Sse',\n",
       "  'Spe',\n",
       "  'Sare',\n",
       "  'Sp',\n",
       "  'Si',\n",
       "  'MZ',\n",
       "  'Mm',\n",
       "  'Mv',\n",
       "  'Mse',\n",
       "  'Mpe',\n",
       "  'Mare',\n",
       "  'Mp',\n",
       "  'Mi',\n",
       "  'SpAbs_Dt',\n",
       "  'SpMax_Dt',\n",
       "  'SpDiam_Dt',\n",
       "  'SpAD_Dt',\n",
       "  'SpMAD_Dt',\n",
       "  'LogEE_Dt',\n",
       "  'SM1_Dt',\n",
       "  'VE1_Dt',\n",
       "  'VE2_Dt',\n",
       "  'VE3_Dt',\n",
       "  'VR1_Dt',\n",
       "  'VR2_Dt',\n",
       "  'VR3_Dt',\n",
       "  'DetourIndex',\n",
       "  'SpAbs_D',\n",
       "  'SpMax_D',\n",
       "  'SpDiam_D',\n",
       "  'SpAD_D',\n",
       "  'SpMAD_D',\n",
       "  'LogEE_D',\n",
       "  'VE1_D',\n",
       "  'VE2_D',\n",
       "  'VE3_D',\n",
       "  'VR1_D',\n",
       "  'VR2_D',\n",
       "  'VR3_D',\n",
       "  'NsLi',\n",
       "  'NssBe',\n",
       "  'NssssBe',\n",
       "  'NssBH',\n",
       "  'NsssB',\n",
       "  'NssssB',\n",
       "  'NsCH3',\n",
       "  'NdCH2',\n",
       "  'NssCH2',\n",
       "  'NtCH',\n",
       "  'NdsCH',\n",
       "  'NaaCH',\n",
       "  'NsssCH',\n",
       "  'NddC',\n",
       "  'NtsC',\n",
       "  'NdssC',\n",
       "  'NaasC',\n",
       "  'NaaaC',\n",
       "  'NssssC',\n",
       "  'NsNH3',\n",
       "  'NsNH2',\n",
       "  'NssNH2',\n",
       "  'NdNH',\n",
       "  'NssNH',\n",
       "  'NaaNH',\n",
       "  'NtN',\n",
       "  'NsssNH',\n",
       "  'NdsN',\n",
       "  'NaaN',\n",
       "  'NsssN',\n",
       "  'NddsN',\n",
       "  'NaasN',\n",
       "  'NssssN',\n",
       "  'NsOH',\n",
       "  'NdO',\n",
       "  'NssO',\n",
       "  'NaaO',\n",
       "  'NsF',\n",
       "  'NsSiH3',\n",
       "  'NssSiH2',\n",
       "  'NsssSiH',\n",
       "  'NssssSi',\n",
       "  'NsPH2',\n",
       "  'NssPH',\n",
       "  'NsssP',\n",
       "  'NdsssP',\n",
       "  'NsssssP',\n",
       "  'NsSH',\n",
       "  'NdS',\n",
       "  'NssS',\n",
       "  'NaaS',\n",
       "  'NdssS',\n",
       "  'NddssS',\n",
       "  'NsCl',\n",
       "  'NsGeH3',\n",
       "  'NssGeH2',\n",
       "  'NsssGeH',\n",
       "  'NssssGe',\n",
       "  'NsAsH2',\n",
       "  'NssAsH',\n",
       "  'NsssAs',\n",
       "  'NsssdAs',\n",
       "  'NsssssAs',\n",
       "  'NsSeH',\n",
       "  'NdSe',\n",
       "  'NssSe',\n",
       "  'NaaSe',\n",
       "  'NdssSe',\n",
       "  'NddssSe',\n",
       "  'NsBr',\n",
       "  'NsSnH3',\n",
       "  'NssSnH2',\n",
       "  'NsssSnH',\n",
       "  'NssssSn',\n",
       "  'NsI',\n",
       "  'NsPbH3',\n",
       "  'NssPbH2',\n",
       "  'NsssPbH',\n",
       "  'NssssPb',\n",
       "  'SsLi',\n",
       "  'SssBe',\n",
       "  'SssssBe',\n",
       "  'SssBH',\n",
       "  'SsssB',\n",
       "  'SssssB',\n",
       "  'SsCH3',\n",
       "  'SdCH2',\n",
       "  'SssCH2',\n",
       "  'StCH',\n",
       "  'SdsCH',\n",
       "  'SaaCH',\n",
       "  'SsssCH',\n",
       "  'SddC',\n",
       "  'StsC',\n",
       "  'SdssC',\n",
       "  'SaasC',\n",
       "  'SaaaC',\n",
       "  'SssssC',\n",
       "  'SsNH3',\n",
       "  'SsNH2',\n",
       "  'SssNH2',\n",
       "  'SdNH',\n",
       "  'SssNH',\n",
       "  'SaaNH',\n",
       "  'StN',\n",
       "  ...],\n",
       " 'data': [[0,\n",
       "   'S=C=Nc1c2c(ccc1)cccc2',\n",
       "   'Hepatotoxicity',\n",
       "   'Mol0',\n",
       "   9.818614707571909,\n",
       "   8.499990288372919,\n",
       "   0,\n",
       "   0,\n",
       "   17.130940081758638,\n",
       "   2.368751826842331,\n",
       "   4.737503653684662,\n",
       "   17.130940081758638,\n",
       "   1.3177646216737413,\n",
       "   3.491889522725001,\n",
       "   3.3021491060357477,\n",
       "   0.2540114696950575,\n",
       "   1.4569377652256672,\n",
       "   63.12005881868169,\n",
       "   4.855389139898592,\n",
       "   4.407402872859582,\n",
       "   10,\n",
       "   11,\n",
       "   20,\n",
       "   13,\n",
       "   0,\n",
       "   0,\n",
       "   2,\n",
       "   7,\n",
       "   0,\n",
       "   11,\n",
       "   1,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   152.44444444444446,\n",
       "   167.66666666666669,\n",
       "   214.33333333333331,\n",
       "   209.66666666666663,\n",
       "   146.66666666666666,\n",
       "   75.66666666666666,\n",
       "   18.0,\n",
       "   2.0,\n",
       "   0.0,\n",
       "   71.0,\n",
       "   88.0,\n",
       "   129.0,\n",
       "   139.0,\n",
       "   108.0,\n",
       "   70.0,\n",
       "   33.0,\n",
       "   9.0,\n",
       "   1.0,\n",
       "   60.26371742112483,\n",
       "   68.62962962962963,\n",
       "   94.5,\n",
       "   110.66049382716051,\n",
       "   104.41975308641977,\n",
       "   78.30864197530863,\n",
       "   38.87037037037037,\n",
       "   10.981481481481481,\n",
       "   1.2962962962962965,\n",
       "   708.0,\n",
       "   618.0,\n",
       "   820.0,\n",
       "   840.0,\n",
       "   768.0,\n",
       "   630.0,\n",
       "   410.0,\n",
       "   152.0,\n",
       "   16.0,\n",
       "   2818.057428,\n",
       "   2393.203761,\n",
       "   3119.0016209999994,\n",
       "   3134.504457,\n",
       "   2823.466592,\n",
       "   2281.9227739999997,\n",
       "   1468.0640529999998,\n",
       "   496.16131599999983,\n",
       "   32.31648000000001,\n",
       "   5716.363540247465,\n",
       "   6606.600747169386,\n",
       "   8982.143816279173,\n",
       "   9587.912894309893,\n",
       "   8143.685183472475,\n",
       "   5990.900048530013,\n",
       "   3249.668658147269,\n",
       "   1088.2377936143628,\n",
       "   136.19864577098818,\n",
       "   148.92040900000003,\n",
       "   158.43047,\n",
       "   239.740694,\n",
       "   302.87023,\n",
       "   281.96558799999997,\n",
       "   229.455694,\n",
       "   138.92396200000002,\n",
       "   51.668114,\n",
       "   7.664544,\n",
       "   121.3055,\n",
       "   132.88049999999998,\n",
       "   199.4247,\n",
       "   247.128,\n",
       "   228.6525,\n",
       "   182.8715,\n",
       "   107.7495,\n",
       "   38.89700000000001,\n",
       "   5.676000000000001,\n",
       "   118.0085,\n",
       "   128.7,\n",
       "   193.5908,\n",
       "   241.079,\n",
       "   223.343,\n",
       "   178.555,\n",
       "   104.74,\n",
       "   37.384,\n",
       "   5.367999999999999,\n",
       "   43.41019033394301,\n",
       "   46.98971017,\n",
       "   64.28712034,\n",
       "   71.285390094245,\n",
       "   63.586293799396,\n",
       "   51.143757154244994,\n",
       "   31.936646394245006,\n",
       "   12.646869219698,\n",
       "   1.9336997,\n",
       "   3007.731351169843,\n",
       "   2910.5692240433,\n",
       "   4523.5210831316,\n",
       "   6108.065486199846,\n",
       "   5756.596552447795,\n",
       "   4796.585345561276,\n",
       "   2952.612053345705,\n",
       "   1062.2546887576882,\n",
       "   140.88000546443,\n",
       "   7.622222222222222,\n",
       "   7.984126984126985,\n",
       "   6.697916666666668,\n",
       "   5.113821138211382,\n",
       "   3.859649122807017,\n",
       "   2.440860215053763,\n",
       "   0.9473684210526316,\n",
       "   0.2857142857142857,\n",
       "   0.0,\n",
       "   3.55,\n",
       "   4.190476190476192,\n",
       "   4.03125,\n",
       "   3.3902439024390243,\n",
       "   2.8421052631578947,\n",
       "   2.2580645161290325,\n",
       "   1.7368421052631582,\n",
       "   1.2857142857142858,\n",
       "   1.0,\n",
       "   3.0131858710562414,\n",
       "   3.2680776014109347,\n",
       "   2.9531250000000004,\n",
       "   2.699036434808793,\n",
       "   2.747888239116309,\n",
       "   2.526085225009956,\n",
       "   2.0458089668615984,\n",
       "   1.5687830687830686,\n",
       "   1.2962962962962965,\n",
       "   35.4,\n",
       "   29.42857142857143,\n",
       "   25.625,\n",
       "   20.48780487804878,\n",
       "   20.210526315789476,\n",
       "   20.322580645161292,\n",
       "   21.578947368421048,\n",
       "   21.714285714285715,\n",
       "   16.0,\n",
       "   140.9028714,\n",
       "   113.96208385714284,\n",
       "   97.46880065624998,\n",
       "   76.45132821951219,\n",
       "   74.30175242105263,\n",
       "   73.61041206451613,\n",
       "   77.26652910526316,\n",
       "   70.88018799999998,\n",
       "   32.31648000000001,\n",
       "   285.8181770123732,\n",
       "   314.6000355794945,\n",
       "   280.69199425872415,\n",
       "   233.8515340075583,\n",
       "   214.307504828223,\n",
       "   193.2548402751617,\n",
       "   171.03519253406682,\n",
       "   155.46254194490896,\n",
       "   136.19864577098818,\n",
       "   7.4460204500000025,\n",
       "   7.544308095238095,\n",
       "   7.4918966875,\n",
       "   7.387078780487804,\n",
       "   7.42014705263158,\n",
       "   7.40179658064516,\n",
       "   7.3117874736842134,\n",
       "   7.381159142857142,\n",
       "   7.664544,\n",
       "   6.0652750000000015,\n",
       "   6.327642857142856,\n",
       "   6.232021874999999,\n",
       "   6.027512195121951,\n",
       "   6.0171710526315785,\n",
       "   5.899080645161289,\n",
       "   5.671026315789473,\n",
       "   5.556714285714286,\n",
       "   5.676000000000001,\n",
       "   5.900425,\n",
       "   6.128571428571428,\n",
       "   6.0497125,\n",
       "   5.879975609756098,\n",
       "   5.877447368421053,\n",
       "   5.759838709677418,\n",
       "   5.512631578947369,\n",
       "   5.340571428571429,\n",
       "   5.367999999999999,\n",
       "   2.1705095166971504,\n",
       "   2.2376052461904763,\n",
       "   2.008972510625,\n",
       "   1.7386680510791463,\n",
       "   1.6733235210367368,\n",
       "   1.6497986178788708,\n",
       "   1.6808761260128948,\n",
       "   1.806695602814,\n",
       "   1.9336997,\n",
       "   150.38656755849217,\n",
       "   138.59853447825242,\n",
       "   141.3600338478625,\n",
       "   148.97720698048406,\n",
       "   151.48938295915252,\n",
       "   154.7285595342347,\n",
       "   155.40063438661605,\n",
       "   151.75066982252687,\n",
       "   140.88000546443,\n",
       "   0.09747975508610708,\n",
       "   -0.03775839800520312,\n",
       "   -0.017812920860326986,\n",
       "   0.012844846667688066,\n",
       "   0.010747350420356776,\n",
       "   -0.029957171604205076,\n",
       "   0.0024011436496404834,\n",
       "   0.011159567719183285,\n",
       "   -0.0003642955301869852,\n",
       "   61.422222222222224,\n",
       "   12.217777777777775,\n",
       "   -6.253333333333333,\n",
       "   -6.226666666666668,\n",
       "   -28.835555555555555,\n",
       "   -19.337777777777774,\n",
       "   1.3600000000000003,\n",
       "   13.235555555555555,\n",
       "   3.128888888888889,\n",
       "   9.75,\n",
       "   3.5625,\n",
       "   -0.5,\n",
       "   -4.9375,\n",
       "   -6.625,\n",
       "   -1.3125,\n",
       "   1.9375,\n",
       "   2.4375,\n",
       "   0.5625,\n",
       "   6.483864883401918,\n",
       "   -0.013322187928669252,\n",
       "   -1.6842489711934157,\n",
       "   1.2347402263374485,\n",
       "   -0.4403309327846368,\n",
       "   -2.344435013717421,\n",
       "   -1.0181249999999995,\n",
       "   0.8040012002743486,\n",
       "   0.2197882373113855,\n",
       "   247.2,\n",
       "   2.6400000000000023,\n",
       "   -12.32,\n",
       "   27.84,\n",
       "   6.719999999999995,\n",
       "   -4.5600000000000005,\n",
       "   -11.439999999999992,\n",
       "   -89.91999999999999,\n",
       "   -42.56,\n",
       "   1102.2904512,\n",
       "   13.042506239999893,\n",
       "   -70.05349392000004,\n",
       "   123.35582724000008,\n",
       "   18.25742192000013,\n",
       "   -21.14919015999995,\n",
       "   -38.128976840000036,\n",
       "   -388.29171932000014,\n",
       "   -188.17760076000005,\n",
       "   1052.0105845057365,\n",
       "   1.7360861889489207,\n",
       "   -291.45123381510984,\n",
       "   40.228026730781,\n",
       "   -200.69590135681992,\n",
       "   -8.870191317299359,\n",
       "   103.04853090970171,\n",
       "   -81.20721668742246,\n",
       "   -88.79339290564829,\n",
       "   0.4024589499999998,\n",
       "   0.00982450250000043,\n",
       "   0.0959819300000003,\n",
       "   0.019209452499999682,\n",
       "   -0.06100920500000001,\n",
       "   -0.13916992250000002,\n",
       "   -0.04009900250000036,\n",
       "   -0.05510628249999998,\n",
       "   -0.03086094749999992,\n",
       "   0.9122549999999991,\n",
       "   0.056597249999997476,\n",
       "   -0.015408000000002297,\n",
       "   0.06913724999999983,\n",
       "   -0.20261949999999973,\n",
       "   -0.3350252499999984,\n",
       "   -0.016807249999997553,\n",
       "   0.020065750000001083,\n",
       "   -0.03206774999999988,\n",
       "   0.8320949999999991,\n",
       "   0.051635249999998814,\n",
       "   -0.03467700000000122,\n",
       "   0.04682024999999977,\n",
       "   -0.1925955,\n",
       "   -0.3308422499999992,\n",
       "   -0.020450249999998664,\n",
       "   0.06836175000000072,\n",
       "   -0.0042997499999998775,\n",
       "   6.858732130062951,\n",
       "   -0.08012790330599633,\n",
       "   -2.0833355555520185,\n",
       "   0.4832037391789019,\n",
       "   -0.04503741825210583,\n",
       "   0.8739106864360264,\n",
       "   0.45116606198804704,\n",
       "   -1.9685509035211823,\n",
       "   -1.0605947720031472,\n",
       "   32.236339716186926,\n",
       "   -2.189617364237165,\n",
       "   -13.882733430821881,\n",
       "   -1.7842665743427988,\n",
       "   -4.454656866630716,\n",
       "   7.361466885241299,\n",
       "   4.793828023792726,\n",
       "   -3.3878956528335853,\n",
       "   -2.5742948782613446,\n",
       "   0.004873987754305354,\n",
       "   -0.00179801895262872,\n",
       "   -0.0005566537768852183,\n",
       "   0.00031328894311434313,\n",
       "   0.00028282501106202044,\n",
       "   -0.0009663603743291961,\n",
       "   0.00012637598156002545,\n",
       "   0.0015942239598833266,\n",
       "   -0.0003642955301869852,\n",
       "   3.071111111111112,\n",
       "   0.5817989417989418,\n",
       "   -0.19541666666666666,\n",
       "   -0.15186991869918698,\n",
       "   -0.7588304093567251,\n",
       "   -0.6237992831541218,\n",
       "   0.07157894736842108,\n",
       "   1.8907936507936507,\n",
       "   3.128888888888889,\n",
       "   0.4875,\n",
       "   0.16964285714285715,\n",
       "   -0.015625,\n",
       "   -0.12042682926829268,\n",
       "   -0.17434210526315788,\n",
       "   -0.04233870967741935,\n",
       "   0.10197368421052633,\n",
       "   0.3482142857142857,\n",
       "   0.5625,\n",
       "   0.324193244170096,\n",
       "   -0.0006343899013652025,\n",
       "   -0.05263278034979424,\n",
       "   0.030115615276523133,\n",
       "   -0.011587656125911493,\n",
       "   -0.07562693592636842,\n",
       "   -0.053585526315789465,\n",
       "   0.11485731432490695,\n",
       "   0.2197882373113855,\n",
       "   12.36,\n",
       "   0.12571428571428586,\n",
       "   -0.385,\n",
       "   0.6790243902439024,\n",
       "   0.17684210526315775,\n",
       "   -0.1470967741935484,\n",
       "   -0.6021052631578944,\n",
       "   -12.845714285714285,\n",
       "   -42.56,\n",
       "   55.11452256,\n",
       "   0.6210717257142806,\n",
       "   -2.1891716850000007,\n",
       "   3.0086787131707338,\n",
       "   0.4804584715789508,\n",
       "   -0.6822319406451596,\n",
       "   -2.0067882547368434,\n",
       "   -55.470245617142865,\n",
       "   -188.17760076000005,\n",
       "   52.60052922528682,\n",
       "   0.08267077090232956,\n",
       "   -9.107851056722183,\n",
       "   0.9811713836775856,\n",
       "   -5.281471088337366,\n",
       "   -0.2861352037838503,\n",
       "   5.423606889984302,\n",
       "   -11.601030955346065,\n",
       "   -88.79339290564829,\n",
       "   0.02012294749999999,\n",
       "   0.00046783345238097303,\n",
       "   0.002999435312500009,\n",
       "   0.00046852323170730925,\n",
       "   -0.0016055053947368425,\n",
       "   -0.004489352338709678,\n",
       "   -0.0021104738157894922,\n",
       "   -0.007872326071428569,\n",
       "   -0.03086094749999992,\n",
       "   0.045612749999999966,\n",
       "   0.0026951071428570226,\n",
       "   -0.00048150000000007166,\n",
       "   0.0016862743902438982,\n",
       "   -0.005332092105263151,\n",
       "   -0.010807266129032206,\n",
       "   -0.0008845921052630291,\n",
       "   0.0028665357142858678,\n",
       "   -0.03206774999999988,\n",
       "   0.04160474999999995,\n",
       "   0.002458821428571372,\n",
       "   -0.0010836562500000381,\n",
       "   0.0011419573170731653,\n",
       "   -0.0050683026315789465,\n",
       "   -0.010672330645161264,\n",
       "   -0.0010763289473683507,\n",
       "   0.009765964285714388,\n",
       "   -0.0042997499999998775,\n",
       "   0.3429366065031475,\n",
       "   -0.003815614443142682,\n",
       "   -0.06510423611100058,\n",
       "   0.011785457053143948,\n",
       "   -0.0011851952171606796,\n",
       "   0.02819066730438795,\n",
       "   0.02374558220989721,\n",
       "   -0.2812215576458832,\n",
       "   -1.0605947720031472,\n",
       "   1.6118169858093463,\n",
       "   -0.10426749353510308,\n",
       "   -0.4338354197131837,\n",
       "   -0.04351869693519021,\n",
       "   -0.11722781227975568,\n",
       "   0.23746667371746125,\n",
       "   0.252306738094354,\n",
       "   -0.4839850932619407,\n",
       "   -2.5742948782613446,\n",
       "   -0.3689009991952628,\n",
       "   -0.1142091045250386,\n",
       "   0.06427774522773567,\n",
       "   0.05802743570953616,\n",
       "   -0.19826893768364065,\n",
       "   0.02592866209981619,\n",
       "   0.3270882161070462,\n",
       "   -0.07474280785075649,\n",
       "   0.1894424919026945,\n",
       "   -0.0636306078147612,\n",
       "   -0.04945113126963396,\n",
       "   -0.2470866021783837,\n",
       "   -0.2031184351804304,\n",
       "   0.02330718257293016,\n",
       "   0.6156708703741988,\n",
       "   1.0188133140376263,\n",
       "   0.347985347985348,\n",
       "   -0.03205128205128205,\n",
       "   -0.24702939337085675,\n",
       "   -0.3576248313090418,\n",
       "   -0.086848635235732,\n",
       "   0.20917678812415647,\n",
       "   0.7142857142857142,\n",
       "   1.1538461538461535,\n",
       "   -0.0019568264076235786,\n",
       "   -0.16235002208182708,\n",
       "   0.09289402483884653,\n",
       "   -0.03574305243644048,\n",
       "   -0.23327733469574985,\n",
       "   -0.16528884324213275,\n",
       "   0.3542865756469751,\n",
       "   0.6779544030105333,\n",
       "   0.010171058714748046,\n",
       "   -0.031148867313915862,\n",
       "   0.05493724840161023,\n",
       "   0.014307613694430241,\n",
       "   -0.011901033510804888,\n",
       "   -0.048714018054845834,\n",
       "   -1.0392972723069809,\n",
       "   -3.44336569579288,\n",
       "   0.011268749085835871,\n",
       "   -0.03972041457161815,\n",
       "   0.05458958135571907,\n",
       "   0.008717456838275309,\n",
       "   -0.012378442358862006,\n",
       "   -0.0364112426548224,\n",
       "   -1.006454252719973,\n",
       "   -3.4143015673435606,\n",
       "   0.0015716718466510591,\n",
       "   -0.1731513197845115,\n",
       "   0.018653260682516173,\n",
       "   -0.10040718536722223,\n",
       "   -0.005439778040223511,\n",
       "   0.10310935973201184,\n",
       "   -0.22054970028265536,\n",
       "   -1.6880703333867288,\n",
       "   0.023248753811089217,\n",
       "   0.14905546578104478,\n",
       "   0.02328303205617912,\n",
       "   -0.07978480263573927,\n",
       "   -0.22309616117170109,\n",
       "   -0.1048789604897341,\n",
       "   -0.3912113805111588,\n",
       "   -1.5336196399657618,\n",
       "   0.05908670586309804,\n",
       "   -0.010556258940758276,\n",
       "   0.036969364711487464,\n",
       "   -0.11689915879360825,\n",
       "   -0.23693520186860506,\n",
       "   -0.019393527144560014,\n",
       "   0.06284505350556302,\n",
       "   -0.7030435569002068,\n",
       "   0.05909953619650099,\n",
       "   -0.026046455032178763,\n",
       "   0.02744776298555252,\n",
       "   -0.1218202881060204,\n",
       "   -0.2565171199240778,\n",
       "   -0.025870338059196408,\n",
       "   0.23473195454159435,\n",
       "   -0.10334757449569776,\n",
       "   -0.011126296728861056,\n",
       "   -0.18984335552525228,\n",
       "   0.03436628470001432,\n",
       "   -0.0034560183855723843,\n",
       "   0.08220372736478107,\n",
       "   0.06924189998853117,\n",
       "   -0.8200394834294311,\n",
       "   -3.0926846300189683,\n",
       "   -0.06468941229251716,\n",
       "   -0.26915923056570884,\n",
       "   -0.026999775606247285,\n",
       "   -0.07273022515077401,\n",
       "   0.1473285588923245,\n",
       "   0.15653559946054452,\n",
       "   -0.300272982306931,\n",
       "   -1.5971384474327937,\n",
       "   1.2562234715993918,\n",
       "   0.9970414716552596,\n",
       "   0.8735388377217892,\n",
       "   1.0285900049575274,\n",
       "   1.2788905051166646,\n",
       "   0.7913646546544569,\n",
       "   0.23977848945705235,\n",
       "   0.4524839452144329,\n",
       "   0.5900265315967196,\n",
       "   0.9027620748914614,\n",
       "   1.0151865447742752,\n",
       "   1.2192474674384943,\n",
       "   1.2096190654031092,\n",
       "   1.035636758321273,\n",
       "   0.5032819929708496,\n",
       "   0.0687409551374819,\n",
       "   0.6031746031746031,\n",
       "   1.157051282051282,\n",
       "   1.3783614759224514,\n",
       "   1.2307692307692306,\n",
       "   0.7857733664185276,\n",
       "   0.5641025641025641,\n",
       "   0.27838827838827834,\n",
       "   0.0,\n",
       "   0.7619697417893321,\n",
       "   0.8769353002467347,\n",
       "   0.7732257959858484,\n",
       "   1.091133154739395,\n",
       "   1.413648966257416,\n",
       "   1.2848185455612886,\n",
       "   0.6297268615886902,\n",
       "   0.12862994824654428,\n",
       "   0.5069155493912776,\n",
       "   0.5200116302588997,\n",
       "   0.5286526166232536,\n",
       "   0.7332119741100324,\n",
       "   1.1888636600897795,\n",
       "   1.943770226537217,\n",
       "   4.39204808136847,\n",
       "   8.646844660194175,\n",
       "   0.5160349654103425,\n",
       "   0.5464085080047958,\n",
       "   0.5551544790365748,\n",
       "   0.7559289410453346,\n",
       "   1.1829134848844738,\n",
       "   1.88279625868177,\n",
       "   4.205429174921884,\n",
       "   8.310108899181582,\n",
       "   0.7053679700946955,\n",
       "   0.9254341671264132,\n",
       "   0.8847256603686973,\n",
       "   1.0354679060887246,\n",
       "   1.064058912794529,\n",
       "   1.1557586901625252,\n",
       "   1.6852359813577062,\n",
       "   3.2099534942557075,\n",
       "   0.6878476733613512,\n",
       "   0.5824514447374073,\n",
       "   0.8130497744840447,\n",
       "   1.1447440788681675,\n",
       "   1.4185869944203362,\n",
       "   1.1534455874319598,\n",
       "   1.577858881188822,\n",
       "   3.1447617204189373,\n",
       "   0.6638024925674759,\n",
       "   0.7832445423702799,\n",
       "   0.8913412234571231,\n",
       "   1.1888945525099883,\n",
       "   1.3761273243315348,\n",
       "   1.016492099248565,\n",
       "   0.8280422845429342,\n",
       "   1.5037462113115303,\n",
       "   0.6977396983345825,\n",
       "   0.8229848304580608,\n",
       "   0.9092360027488656,\n",
       "   1.2365475095992648,\n",
       "   1.412020843618779,\n",
       "   0.9090909090909092,\n",
       "   0.4344970912490243,\n",
       "   0.6576172191877123,\n",
       "   0.6073096954482201,\n",
       "   0.7782393608153152,\n",
       "   0.6683755772044364,\n",
       "   0.8198116397634255,\n",
       "   1.0498727467695268,\n",
       "   1.5852613060345904,\n",
       "   3.458972600705468,\n",
       "   6.907767703654387,\n",
       "   0.8492149528291785,\n",
       "   1.0627134229493107,\n",
       "   0.911242107458764,\n",
       "   1.0386545208196312,\n",
       "   0.925286727764986,\n",
       "   0.970183126167103,\n",
       "   1.5888364760177918,\n",
       "   3.0906349692399333,\n",
       "   0.3726370602801016,\n",
       "   -0.3657260275027656,\n",
       "   5.048502298981347,\n",
       "   0.6534549210967411,\n",
       "   3.239106642464767,\n",
       "   0.9562045902385764,\n",
       "   3.079592979707352,\n",
       "   1.2600709989745709,\n",
       "   16.00441214920604,\n",
       "   5.655144821578975,\n",
       "   32.062200419902865,\n",
       "   11.666482167048144,\n",
       "   24.440465788392,\n",
       "   15.588505553136839,\n",
       "   3.3086038575730545,\n",
       "   2.4004359945420224,\n",
       "   3.139464194897357,\n",
       "   2.2044756441207194,\n",
       "   3.1554404371729046,\n",
       "   2.154572507135636,\n",
       "   2.9355182042845933,\n",
       "   1.0191197262241607,\n",
       "   14.549373914145047,\n",
       "   10.312999896344804,\n",
       "   1.940215870555883,\n",
       "   47.517377819657824,\n",
       "   24.022893129705192,\n",
       "   34.34419925883543,\n",
       "   48.04897122625122,\n",
       "   3.6960747097116333,\n",
       "   24.022893129995094,\n",
       "   0.7678571428571429,\n",
       "   3.5669421176413136,\n",
       "   0.2743801628954857,\n",
       "   1.5340729435304454,\n",
       "   53.85040883744974,\n",
       "   4.142339141342288,\n",
       "   4.248574260194267,\n",
       "   47.52227552375084,\n",
       "   24.024943557692453,\n",
       "   34.34754477597383,\n",
       "   48.05387013361518,\n",
       "   3.696451548739629,\n",
       "   24.024943557981768,\n",
       "   0.7678588809151259,\n",
       "   3.566937807771824,\n",
       "   0.2743798313670634,\n",
       "   1.5340717352484068,\n",
       "   53.850661049361754,\n",
       "   4.142358542258596,\n",
       "   4.248578943748707,\n",
       "   55.538286833878324,\n",
       "   27.68827054040318,\n",
       "   40.492957870546974,\n",
       "   55.500960890861734,\n",
       "   4.26930468391244,\n",
       "   27.688270540409842,\n",
       "   -0.16174575307196082,\n",
       "   3.55329777155043,\n",
       "   0.27333059781157154,\n",
       "   1.5302403864805538,\n",
       "   54.52330846255826,\n",
       "   4.19410065096602,\n",
       "   4.260992552920508,\n",
       "   48.98722625427485,\n",
       "   24.599422675773287,\n",
       "   35.171503448522024,\n",
       "   49.16628856735093,\n",
       "   3.7820221974885326,\n",
       "   24.59942267592529,\n",
       "   0.2116190972717064,\n",
       "   3.5623591025032346,\n",
       "   0.2740276232694796,\n",
       "   1.5327872591056004,\n",
       "   54.09586400166114,\n",
       "   4.161220307820087,\n",
       "   4.25312199641147,\n",
       "   48.864729341062066,\n",
       "   24.51877072928256,\n",
       "   35.03062772198605,\n",
       "   49.01095497894926,\n",
       "   3.770073459919174,\n",
       "   24.518770729447017,\n",
       "   0.17281211750306014,\n",
       "   3.562241395521165,\n",
       "   0.27401856888624343,\n",
       "   1.5327542166965242,\n",
       "   54.09942369025362,\n",
       "   4.161494130019508,\n",
       "   4.253187797587653,\n",
       "   48.61892761016279,\n",
       "   24.39000259933568,\n",
       "   34.816478895866645,\n",
       "   48.7552240312085,\n",
       "   3.7504018485544997,\n",
       "   24.39000259952289,\n",
       "   0.16107758850857054,\n",
       "   3.5623901897949057,\n",
       "   0.2740300145996081,\n",
       "   1.532795985669974,\n",
       "   54.08929024271363,\n",
       "   4.1607146340548935,\n",
       "   4.253000468480924,\n",
       "   57.71055677030707,\n",
       "   28.80825644157986,\n",
       "   42.57116339756268,\n",
       "   57.68885433481154,\n",
       "   4.4376041796008865,\n",
       "   28.808256441582074,\n",
       "   -0.09404388714733536,\n",
       "   3.551003787414693,\n",
       "   0.2731541374934379,\n",
       "   1.5295945850048074,\n",
       "   54.612035569795786,\n",
       "   4.200925813061215,\n",
       "   4.262618554608099,\n",
       "   48.234359172953226,\n",
       "   24.18635413157055,\n",
       "   34.47993388292082,\n",
       "   48.351423787727576,\n",
       "   3.71934029136366,\n",
       "   24.186354131800414,\n",
       "   0.13834909018785554,\n",
       "   3.5625716154566653,\n",
       "   0.27404397041974343,\n",
       "   1.5328469124445327,\n",
       "   54.07493090872393,\n",
       "   4.15961006990184,\n",
       "   4.252734958612296,\n",
       "   478.9191904065538,\n",
       "   21,\n",
       "   14,\n",
       "   8,\n",
       "   2,\n",
       "   0,\n",
       "   11,\n",
       "   13,\n",
       "   14,\n",
       "   7,\n",
       "   0.3265100687566797,\n",
       "   0.1375111828419696,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   8,\n",
       "   2,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.15137471507731048,\n",
       "   0.25137565482875424,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.05183403899401218,\n",
       "   0.06846246121695465,\n",
       "   0.4694160968212876,\n",
       "   0.0,\n",
       "   0.13608276348795434,\n",
       "   0.0,\n",
       "   0.22005183941882686,\n",
       "   0.0,\n",
       "   0.04269016102873266,\n",
       "   0.0,\n",
       "   1.3806820952411107,\n",
       "   2.4633868134879453,\n",
       "   3.7847091179911274,\n",
       "   0.5240310835359183,\n",
       "   0.7593999539812538,\n",
       "   0.9529952989848268,\n",
       "   9.096011838247808,\n",
       "   6.4150149001725305,\n",
       "   5.1880497660464835,\n",
       "   4.382666017068749,\n",
       "   3.8008699646685535,\n",
       "   3.067866662162336,\n",
       "   1.7983853095433011,\n",
       "   1.150417574450507,\n",
       "   0.6996932183267545,\n",
       "   0.4582153500123236,\n",
       "   0.2882249870025824,\n",
       "   0.1826110840445312,\n",
       "   0.1226087085376953,\n",
       "   0.08291531519357664,\n",
       "   0.0580124293401065,\n",
       "   0.0396695715327761,\n",
       "   7.713410351218929,\n",
       "   4.380953236241671,\n",
       "   2.883938960386134,\n",
       "   2.0240034802883677,\n",
       "   1.4533278224082111,\n",
       "   0.973751244847558,\n",
       "   0.46242130071077603,\n",
       "   0.24195621959859714,\n",
       "   0.5933392577860714,\n",
       "   0.3129252311601194,\n",
       "   0.16021883113256302,\n",
       "   0.08433347834534864,\n",
       "   0.04688154265832939,\n",
       "   0.02631760121209616,\n",
       "   0.014916816151960515,\n",
       "   0.008343317917193005,\n",
       "   15.999999999999995,\n",
       "   15.422862376155194,\n",
       "   14.841415631996744,\n",
       "   19.84741442097597,\n",
       "   19.24313725490196,\n",
       "   19.364,\n",
       "   16.190150299401196,\n",
       "   21.664299441400317,\n",
       "   0.7999999999999998,\n",
       "   0.7711431188077598,\n",
       "   0.7420707815998373,\n",
       "   0.9923707210487984,\n",
       "   0.962156862745098,\n",
       "   0.9681999999999998,\n",
       "   0.8095075149700598,\n",
       "   1.083214972070016,\n",
       "   192.5135218742421,\n",
       "   96.25676093712107,\n",
       "   127.84080751733362,\n",
       "   192.5135218742421,\n",
       "   14.808732451864774,\n",
       "   96.25676093712107,\n",
       "   0.0,\n",
       "   3.595883097585591,\n",
       "   0.2766063921219685,\n",
       "   1.5421538715344174,\n",
       "   51.460334153849146,\n",
       "   3.95848724260378,\n",
       "   4.203175564767732,\n",
       "   622.0,\n",
       "   75.11434115164528,\n",
       "   37.55717057582263,\n",
       "   53.34634137311522,\n",
       "   75.11434115164526,\n",
       "   5.778026242434251,\n",
       "   37.55717057582263,\n",
       "   3.5529741919473987,\n",
       "   0.2733057070728768,\n",
       "   1.530149317744396,\n",
       "   54.51462697402567,\n",
       "   4.19343284415582,\n",
       "   4.2608333149724285,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   7,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   2,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0.0,\n",
       "   0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   14.031481953892667,\n",
       "   0.0,\n",
       "   2.3844633408919123,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.8825000000000002,\n",
       "   2.2882870370370374,\n",
       "   0.0,\n",
       "   0,\n",
       "   0.0,\n",
       "   0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   ...]]}"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df_with_drops = full_df.head(1241)\n",
    "merged_df_with_drops.head(1).to_dict('split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Smiles</th>\n",
       "      <th>Liver</th>\n",
       "      <th>Mol_ID</th>\n",
       "      <th>ABC</th>\n",
       "      <th>ABCGG</th>\n",
       "      <th>nAcid</th>\n",
       "      <th>nBase</th>\n",
       "      <th>SpAbs_A</th>\n",
       "      <th>SpMax_A</th>\n",
       "      <th>...</th>\n",
       "      <th>SRW10</th>\n",
       "      <th>TSRW10</th>\n",
       "      <th>MW</th>\n",
       "      <th>AMW</th>\n",
       "      <th>WPath</th>\n",
       "      <th>WPol</th>\n",
       "      <th>Zagreb1</th>\n",
       "      <th>Zagreb2</th>\n",
       "      <th>mZagreb1</th>\n",
       "      <th>mZagreb2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1241</th>\n",
       "      <td>1241</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.749825</td>\n",
       "      <td>24.142500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>50.259236</td>\n",
       "      <td>2.609260</td>\n",
       "      <td>...</td>\n",
       "      <td>10.869311</td>\n",
       "      <td>98.746533</td>\n",
       "      <td>558.183853</td>\n",
       "      <td>8.587444</td>\n",
       "      <td>5797</td>\n",
       "      <td>69</td>\n",
       "      <td>222.0</td>\n",
       "      <td>271.0</td>\n",
       "      <td>15.611111</td>\n",
       "      <td>8.472222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242</th>\n",
       "      <td>1242</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.894331</td>\n",
       "      <td>9.741441</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17.838606</td>\n",
       "      <td>2.302776</td>\n",
       "      <td>...</td>\n",
       "      <td>9.201401</td>\n",
       "      <td>46.028465</td>\n",
       "      <td>226.039672</td>\n",
       "      <td>8.693834</td>\n",
       "      <td>410</td>\n",
       "      <td>18</td>\n",
       "      <td>68.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>6.194444</td>\n",
       "      <td>3.527778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243</th>\n",
       "      <td>1243</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.632065</td>\n",
       "      <td>17.220088</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34.514767</td>\n",
       "      <td>2.504469</td>\n",
       "      <td>...</td>\n",
       "      <td>10.152260</td>\n",
       "      <td>76.306186</td>\n",
       "      <td>403.121818</td>\n",
       "      <td>8.062436</td>\n",
       "      <td>1809</td>\n",
       "      <td>42</td>\n",
       "      <td>138.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>9.250000</td>\n",
       "      <td>6.138889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244</th>\n",
       "      <td>1244</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.148902</td>\n",
       "      <td>14.617802</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>32.628353</td>\n",
       "      <td>2.447342</td>\n",
       "      <td>...</td>\n",
       "      <td>9.924025</td>\n",
       "      <td>67.325686</td>\n",
       "      <td>337.204179</td>\n",
       "      <td>6.484696</td>\n",
       "      <td>1662</td>\n",
       "      <td>39</td>\n",
       "      <td>124.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>6.916667</td>\n",
       "      <td>5.694444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1245</th>\n",
       "      <td>1245</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.360915</td>\n",
       "      <td>13.145467</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>27.553627</td>\n",
       "      <td>2.464704</td>\n",
       "      <td>...</td>\n",
       "      <td>9.831293</td>\n",
       "      <td>68.929049</td>\n",
       "      <td>282.100442</td>\n",
       "      <td>8.060013</td>\n",
       "      <td>957</td>\n",
       "      <td>29</td>\n",
       "      <td>108.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>5.916667</td>\n",
       "      <td>4.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1617 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0 Smiles Liver Mol_ID        ABC      ABCGG  nAcid  nBase  \\\n",
       "1241        1241    NaN   NaN    NaN  31.749825  24.142500      1      1   \n",
       "1242        1242    NaN   NaN    NaN  10.894331   9.741441      1      0   \n",
       "1243        1243    NaN   NaN    NaN  20.632065  17.220088      0      0   \n",
       "1244        1244    NaN   NaN    NaN  19.148902  14.617802      1      1   \n",
       "1245        1245    NaN   NaN    NaN  16.360915  13.145467      1      0   \n",
       "\n",
       "        SpAbs_A   SpMax_A  ...      SRW10     TSRW10          MW       AMW  \\\n",
       "1241  50.259236  2.609260  ...  10.869311  98.746533  558.183853  8.587444   \n",
       "1242  17.838606  2.302776  ...   9.201401  46.028465  226.039672  8.693834   \n",
       "1243  34.514767  2.504469  ...  10.152260  76.306186  403.121818  8.062436   \n",
       "1244  32.628353  2.447342  ...   9.924025  67.325686  337.204179  6.484696   \n",
       "1245  27.553627  2.464704  ...   9.831293  68.929049  282.100442  8.060013   \n",
       "\n",
       "      WPath  WPol  Zagreb1  Zagreb2   mZagreb1  mZagreb2  \n",
       "1241   5797    69    222.0    271.0  15.611111  8.472222  \n",
       "1242    410    18     68.0     74.0   6.194444  3.527778  \n",
       "1243   1809    42    138.0    162.0   9.250000  6.138889  \n",
       "1244   1662    39    124.0    142.0   6.916667  5.694444  \n",
       "1245    957    29    108.0    125.0   5.916667  4.666667  \n",
       "\n",
       "[5 rows x 1617 columns]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df[full_df['Smiles'].isna()].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we read in the enriching file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading in the enriching file\n",
      "(1179, 1261)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "PP = \"~/ToxNeuralNets\"\n",
    "print('reading in the enriching file')\n",
    "liu_df = pd.read_csv(os.path.join(PP,'enriching_features/models_data_features_toxsci_trainingset_descriptors_AND_Genescore_AND_Modules_AND_Phenotypes_NaN.csv'), na_values=\"na\",delimiter=\",\", low_memory=False)\n",
    "print(liu_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Intersection(lst1, lst2): \n",
    "    return set(lst1).intersection(lst2) \n",
    "      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we are looking at the redundancy of the features from Zach's files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1617\n",
      "1261\n"
     ]
    }
   ],
   "source": [
    "liu_df = liu_df.rename(columns={\"SMILES\": \"Smiles\"})\n",
    "columnsdf1 = list(merged_df_with_drops.columns)\n",
    "columnsdf2 = list(liu_df.columns)\n",
    "def Union(lst1, lst2): \n",
    "    final_list = sorted(lst1 + lst2) \n",
    "    return set(final_list)\n",
    "print(len(columnsdf1))\n",
    "print(len(columnsdf2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1877"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_names_union_unique =list(Union(columnsdf1, columnsdf2))\n",
    "len(col_names_union_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overlapcols = list(sorted(Intersection(columnsdf1, columnsdf2)))\n",
    "len(overlapcols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(overlapcols)\n",
    "overlapcols1 =['AATS0Z',\n",
    " 'AATS0are',\n",
    " 'AATS0d',\n",
    " 'AATS0dv',\n",
    " 'AATS0i',\n",
    " 'AATS0m',\n",
    " 'AATS0p',\n",
    " 'AATS0pe',\n",
    " 'AATS0s',\n",
    " 'AATS0se',\n",
    " 'AATS0v',\n",
    " 'AATS1Z',\n",
    " 'AATS1are',\n",
    " 'AATS1d',\n",
    " 'AATS1dv',\n",
    " 'AATS1i',\n",
    " 'AATS1m',\n",
    " 'AATS1p',\n",
    " 'AATS1pe',\n",
    " 'AATS1s',\n",
    " 'AATS1se',\n",
    " 'AATS1v',\n",
    " 'AATS2Z',\n",
    " 'AATS2are',\n",
    " 'AATS2d',\n",
    " 'AATS2dv',\n",
    " 'AATS2i',\n",
    " 'AATS2m',\n",
    " 'AATS2p',\n",
    " 'AATS2pe',\n",
    " 'AATS2s',\n",
    " 'AATS2se',\n",
    " 'AATS2v',\n",
    " 'AATS3Z',\n",
    " 'AATS3are',\n",
    " 'AATS3d',\n",
    " 'AATS3dv',\n",
    " 'AATS3i',\n",
    " 'AATS3m',\n",
    " 'AATS3p',\n",
    " 'AATS3pe',\n",
    " 'AATS3s',\n",
    " 'AATS3se',\n",
    " 'AATS3v',\n",
    " 'AATS4Z',\n",
    " 'AATS4are',\n",
    " 'AATS4d',\n",
    " 'AATS4dv',\n",
    " 'AATS4i',\n",
    " 'AATS4m',\n",
    " 'AATS4p',\n",
    " 'AATS4pe',\n",
    " 'AATS4s',\n",
    " 'AATS4se',\n",
    " 'AATS4v',\n",
    " 'AATS5Z',\n",
    " 'AATS5are',\n",
    " 'AATS5d',\n",
    " 'AATS5dv',\n",
    " 'AATS5i',\n",
    " 'AATS5m',\n",
    " 'AATS5p',\n",
    " 'AATS5pe',\n",
    " 'AATS5s',\n",
    " 'AATS5se',\n",
    " 'AATS5v',\n",
    " 'AATS6Z',\n",
    " 'AATS6are',\n",
    " 'AATS6d',\n",
    " 'AATS6dv',\n",
    " 'AATS6i',\n",
    " 'AATS6m',\n",
    " 'AATS6p',\n",
    " 'AATS6pe',\n",
    " 'AATS6s',\n",
    " 'AATS6se',\n",
    " 'AATS6v',\n",
    " 'AATS7Z',\n",
    " 'AATS7are',\n",
    " 'AATS7d',\n",
    " 'AATS7dv',\n",
    " 'AATS7i',\n",
    " 'AATS7m',\n",
    " 'AATS7p',\n",
    " 'AATS7pe',\n",
    " 'AATS7s',\n",
    " 'AATS7se',\n",
    " 'AATS7v',\n",
    " 'AATS8Z',\n",
    " 'AATS8are',\n",
    " 'AATS8d',\n",
    " 'AATS8dv',\n",
    " 'AATS8i',\n",
    " 'AATS8m',\n",
    " 'AATS8p',\n",
    " 'AATS8pe',\n",
    " 'AATS8s',\n",
    " 'AATS8se',\n",
    " 'AATS8v',\n",
    " 'AATSC0Z',\n",
    " 'AATSC0are',\n",
    " 'AATSC0c',\n",
    " 'AATSC0d',\n",
    " 'AATSC0dv',\n",
    " 'AATSC0i',\n",
    " 'AATSC0m',\n",
    " 'AATSC0p',\n",
    " 'AATSC0pe',\n",
    " 'AATSC0s',\n",
    " 'AATSC0se',\n",
    " 'AATSC0v',\n",
    " 'AATSC1Z',\n",
    " 'AATSC1are',\n",
    " 'AATSC1c',\n",
    " 'AATSC1d',\n",
    " 'AATSC1dv',\n",
    " 'AATSC1i',\n",
    " 'AATSC1m',\n",
    " 'AATSC1p',\n",
    " 'AATSC1pe',\n",
    " 'AATSC1s',\n",
    " 'AATSC1se',\n",
    " 'AATSC1v',\n",
    " 'AATSC2Z',\n",
    " 'AATSC2are',\n",
    " 'AATSC2c',\n",
    " 'AATSC2d',\n",
    " 'AATSC2dv',\n",
    " 'AATSC2i',\n",
    " 'AATSC2m',\n",
    " 'AATSC2p',\n",
    " 'AATSC2pe',\n",
    " 'AATSC2s',\n",
    " 'AATSC2se',\n",
    " 'AATSC2v',\n",
    " 'AATSC3Z',\n",
    " 'AATSC3are',\n",
    " 'AATSC3c',\n",
    " 'AATSC3d',\n",
    " 'AATSC3dv',\n",
    " 'AATSC3i',\n",
    " 'AATSC3m',\n",
    " 'AATSC3p',\n",
    " 'AATSC3pe',\n",
    " 'AATSC3s',\n",
    " 'AATSC3se',\n",
    " 'AATSC3v',\n",
    " 'AATSC4Z',\n",
    " 'AATSC4are',\n",
    " 'AATSC4c',\n",
    " 'AATSC4d',\n",
    " 'AATSC4dv',\n",
    " 'AATSC4i',\n",
    " 'AATSC4m',\n",
    " 'AATSC4p',\n",
    " 'AATSC4pe',\n",
    " 'AATSC4s',\n",
    " 'AATSC4se',\n",
    " 'AATSC4v',\n",
    " 'AATSC5Z',\n",
    " 'AATSC5are',\n",
    " 'AATSC5c',\n",
    " 'AATSC5d',\n",
    " 'AATSC5dv',\n",
    " 'AATSC5i',\n",
    " 'AATSC5m',\n",
    " 'AATSC5p',\n",
    " 'AATSC5pe',\n",
    " 'AATSC5s',\n",
    " 'AATSC5se',\n",
    " 'AATSC5v',\n",
    " 'AATSC6Z',\n",
    " 'AATSC6are',\n",
    " 'AATSC6c',\n",
    " 'AATSC6d',\n",
    " 'AATSC6dv',\n",
    " 'AATSC6i',\n",
    " 'AATSC6m',\n",
    " 'AATSC6p',\n",
    " 'AATSC6pe',\n",
    " 'AATSC6s',\n",
    " 'AATSC6se',\n",
    " 'AATSC6v',\n",
    " 'AATSC7Z',\n",
    " 'AATSC7are',\n",
    " 'AATSC7c',\n",
    " 'AATSC7d',\n",
    " 'AATSC7dv',\n",
    " 'AATSC7i',\n",
    " 'AATSC7m',\n",
    " 'AATSC7p',\n",
    " 'AATSC7pe',\n",
    " 'AATSC7s',\n",
    " 'AATSC7se',\n",
    " 'AATSC7v',\n",
    " 'AATSC8Z',\n",
    " 'AATSC8are',\n",
    " 'AATSC8c',\n",
    " 'AATSC8d',\n",
    " 'AATSC8dv',\n",
    " 'AATSC8i',\n",
    " 'AATSC8m',\n",
    " 'AATSC8p',\n",
    " 'AATSC8pe',\n",
    " 'AATSC8s',\n",
    " 'AATSC8se',\n",
    " 'AATSC8v',\n",
    " 'ABC',\n",
    " 'ABCGG',\n",
    " 'ATS0Z',\n",
    " 'ATS0are',\n",
    " 'ATS0d',\n",
    " 'ATS0dv',\n",
    " 'ATS0i',\n",
    " 'ATS0m',\n",
    " 'ATS0p',\n",
    " 'ATS0pe',\n",
    " 'ATS0s',\n",
    " 'ATS0se',\n",
    " 'ATS0v',\n",
    " 'ATS1Z',\n",
    " 'ATS1are',\n",
    " 'ATS1d',\n",
    " 'ATS1dv',\n",
    " 'ATS1i',\n",
    " 'ATS1m',\n",
    " 'ATS1p',\n",
    " 'ATS1pe',\n",
    " 'ATS1s',\n",
    " 'ATS1se',\n",
    " 'ATS1v',\n",
    " 'ATS2Z',\n",
    " 'ATS2are',\n",
    " 'ATS2d',\n",
    " 'ATS2dv',\n",
    " 'ATS2i',\n",
    " 'ATS2m',\n",
    " 'ATS2p',\n",
    " 'ATS2pe',\n",
    " 'ATS2s',\n",
    " 'ATS2se',\n",
    " 'ATS2v',\n",
    " 'ATS3Z',\n",
    " 'ATS3are',\n",
    " 'ATS3d',\n",
    " 'ATS3dv',\n",
    " 'ATS3i',\n",
    " 'ATS3m',\n",
    " 'ATS3p',\n",
    " 'ATS3pe',\n",
    " 'ATS3s',\n",
    " 'ATS3se',\n",
    " 'ATS3v',\n",
    " 'ATS4Z',\n",
    " 'ATS4are',\n",
    " 'ATS4d',\n",
    " 'ATS4dv',\n",
    " 'ATS4i',\n",
    " 'ATS4m',\n",
    " 'ATS4p',\n",
    " 'ATS4pe',\n",
    " 'ATS4s',\n",
    " 'ATS4se',\n",
    " 'ATS4v',\n",
    " 'ATS5Z',\n",
    " 'ATS5are',\n",
    " 'ATS5d',\n",
    " 'ATS5dv',\n",
    " 'ATS5i',\n",
    " 'ATS5m',\n",
    " 'ATS5p',\n",
    " 'ATS5pe',\n",
    " 'ATS5s',\n",
    " 'ATS5se',\n",
    " 'ATS5v',\n",
    " 'ATS6Z',\n",
    " 'ATS6are',\n",
    " 'ATS6d',\n",
    " 'ATS6dv',\n",
    " 'ATS6i',\n",
    " 'ATS6m',\n",
    " 'ATS6p',\n",
    " 'ATS6pe',\n",
    " 'ATS6s',\n",
    " 'ATS6se',\n",
    " 'ATS6v',\n",
    " 'ATS7Z',\n",
    " 'ATS7are',\n",
    " 'ATS7d',\n",
    " 'ATS7dv',\n",
    " 'ATS7i',\n",
    " 'ATS7m',\n",
    " 'ATS7p',\n",
    " 'ATS7pe',\n",
    " 'ATS7s',\n",
    " 'ATS7se',\n",
    " 'ATS7v',\n",
    " 'ATS8Z',\n",
    " 'ATS8are',\n",
    " 'ATS8d',\n",
    " 'ATS8dv',\n",
    " 'ATS8i',\n",
    " 'ATS8m',\n",
    " 'ATS8p',\n",
    " 'ATS8pe',\n",
    " 'ATS8s',\n",
    " 'ATS8se',\n",
    " 'ATS8v',\n",
    " 'ATSC0Z',\n",
    " 'ATSC0are',\n",
    " 'ATSC0c',\n",
    " 'ATSC0d',\n",
    " 'ATSC0dv',\n",
    " 'ATSC0i',\n",
    " 'ATSC0m',\n",
    " 'ATSC0p',\n",
    " 'ATSC0pe',\n",
    " 'ATSC0s',\n",
    " 'ATSC0se',\n",
    " 'ATSC0v',\n",
    " 'ATSC1Z',\n",
    " 'ATSC1are',\n",
    " 'ATSC1c',\n",
    " 'ATSC1d',\n",
    " 'ATSC1dv',\n",
    " 'ATSC1i',\n",
    " 'ATSC1m',\n",
    " 'ATSC1p',\n",
    " 'ATSC1pe',\n",
    " 'ATSC1s',\n",
    " 'ATSC1se',\n",
    " 'ATSC1v',\n",
    " 'ATSC2Z',\n",
    " 'ATSC2are',\n",
    " 'ATSC2c',\n",
    " 'ATSC2d',\n",
    " 'ATSC2dv',\n",
    " 'ATSC2i',\n",
    " 'ATSC2m',\n",
    " 'ATSC2p',\n",
    " 'ATSC2pe',\n",
    " 'ATSC2s',\n",
    " 'ATSC2se',\n",
    " 'ATSC2v',\n",
    " 'ATSC3Z',\n",
    " 'ATSC3are',\n",
    " 'ATSC3c',\n",
    " 'ATSC3d',\n",
    " 'ATSC3dv',\n",
    " 'ATSC3i',\n",
    " 'ATSC3m',\n",
    " 'ATSC3p',\n",
    " 'ATSC3pe',\n",
    " 'ATSC3s',\n",
    " 'ATSC3se',\n",
    " 'ATSC3v',\n",
    " 'ATSC4Z',\n",
    " 'ATSC4are',\n",
    " 'ATSC4c',\n",
    " 'ATSC4d',\n",
    " 'ATSC4dv',\n",
    " 'ATSC4i',\n",
    " 'ATSC4m',\n",
    " 'ATSC4p',\n",
    " 'ATSC4pe',\n",
    " 'ATSC4s',\n",
    " 'ATSC4se',\n",
    " 'ATSC4v',\n",
    " 'ATSC5Z',\n",
    " 'ATSC5are',\n",
    " 'ATSC5c',\n",
    " 'ATSC5d',\n",
    " 'ATSC5dv',\n",
    " 'ATSC5i',\n",
    " 'ATSC5m',\n",
    " 'ATSC5p',\n",
    " 'ATSC5pe',\n",
    " 'ATSC5s',\n",
    " 'ATSC5se',\n",
    " 'ATSC5v',\n",
    " 'ATSC6Z',\n",
    " 'ATSC6are',\n",
    " 'ATSC6c',\n",
    " 'ATSC6d',\n",
    " 'ATSC6dv',\n",
    " 'ATSC6i',\n",
    " 'ATSC6m',\n",
    " 'ATSC6p',\n",
    " 'ATSC6pe',\n",
    " 'ATSC6s',\n",
    " 'ATSC6se',\n",
    " 'ATSC6v',\n",
    " 'ATSC7Z',\n",
    " 'ATSC7are',\n",
    " 'ATSC7c',\n",
    " 'ATSC7d',\n",
    " 'ATSC7dv',\n",
    " 'ATSC7i',\n",
    " 'ATSC7m',\n",
    " 'ATSC7p',\n",
    " 'ATSC7pe',\n",
    " 'ATSC7s',\n",
    " 'ATSC7se',\n",
    " 'ATSC7v',\n",
    " 'ATSC8Z',\n",
    " 'ATSC8are',\n",
    " 'ATSC8c',\n",
    " 'ATSC8d',\n",
    " 'ATSC8dv',\n",
    " 'ATSC8i',\n",
    " 'ATSC8m',\n",
    " 'ATSC8p',\n",
    " 'ATSC8pe',\n",
    " 'ATSC8s',\n",
    " 'ATSC8se',\n",
    " 'ATSC8v',\n",
    " 'AXp-0d',\n",
    " 'AXp-0dv',\n",
    " 'AXp-1d',\n",
    " 'AXp-1dv',\n",
    " 'AXp-2d',\n",
    " 'AXp-2dv',\n",
    " 'AXp-3d',\n",
    " 'AXp-3dv',\n",
    " 'AXp-4d',\n",
    " 'AXp-4dv',\n",
    " 'AXp-5d',\n",
    " 'AXp-5dv',\n",
    " 'AXp-6d',\n",
    " 'AXp-6dv',\n",
    " 'AXp-7d',\n",
    " 'AXp-7dv',\n",
    " 'BCUTZ-1h',\n",
    " 'BCUTZ-1l',\n",
    " 'BCUTare-1h',\n",
    " 'BCUTare-1l',\n",
    " 'BCUTc-1h',\n",
    " 'BCUTc-1l',\n",
    " 'BCUTd-1h',\n",
    " 'BCUTd-1l',\n",
    " 'BCUTdv-1h',\n",
    " 'BCUTdv-1l',\n",
    " 'BCUTi-1h',\n",
    " 'BCUTi-1l',\n",
    " 'BCUTm-1h',\n",
    " 'BCUTm-1l',\n",
    " 'BCUTp-1h',\n",
    " 'BCUTp-1l',\n",
    " 'BCUTpe-1h',\n",
    " 'BCUTpe-1l',\n",
    " 'BCUTs-1h',\n",
    " 'BCUTs-1l',\n",
    " 'BCUTse-1h',\n",
    " 'BCUTse-1l',\n",
    " 'BCUTv-1h',\n",
    " 'BCUTv-1l',\n",
    " 'BalabanJ',\n",
    " 'BertzCT',\n",
    " 'C1SP1',\n",
    " 'C1SP2',\n",
    " 'C1SP3',\n",
    " 'C2SP1',\n",
    " 'C2SP2',\n",
    " 'C2SP3',\n",
    " 'C3SP2',\n",
    " 'C3SP3',\n",
    " 'C4SP3',\n",
    " 'DetourIndex',\n",
    " 'FCSP3',\n",
    " 'GATS1Z',\n",
    " 'GATS1are',\n",
    " 'GATS1c',\n",
    " 'GATS1d',\n",
    " 'GATS1dv',\n",
    " 'GATS1i',\n",
    " 'GATS1m',\n",
    " 'GATS1p',\n",
    " 'GATS1pe',\n",
    " 'GATS1s',\n",
    " 'GATS1se',\n",
    " 'GATS1v',\n",
    " 'GATS2Z',\n",
    " 'GATS2are',\n",
    " 'GATS2c',\n",
    " 'GATS2d',\n",
    " 'GATS2dv',\n",
    " 'GATS2i',\n",
    " 'GATS2m',\n",
    " 'GATS2p',\n",
    " 'GATS2pe',\n",
    " 'GATS2s',\n",
    " 'GATS2se',\n",
    " 'GATS2v',\n",
    " 'GATS3Z',\n",
    " 'GATS3are',\n",
    " 'GATS3c',\n",
    " 'GATS3d',\n",
    " 'GATS3dv',\n",
    " 'GATS3i',\n",
    " 'GATS3m',\n",
    " 'GATS3p',\n",
    " 'GATS3pe',\n",
    " 'GATS3s',\n",
    " 'GATS3se',\n",
    " 'GATS3v',\n",
    " 'GATS4Z',\n",
    " 'GATS4are',\n",
    " 'GATS4c',\n",
    " 'GATS4d',\n",
    " 'GATS4dv',\n",
    " 'GATS4i',\n",
    " 'GATS4m',\n",
    " 'GATS4p',\n",
    " 'GATS4pe',\n",
    " 'GATS4s',\n",
    " 'GATS4se',\n",
    " 'GATS4v',\n",
    " 'GATS5Z',\n",
    " 'GATS5are',\n",
    " 'GATS5c',\n",
    " 'GATS5d',\n",
    " 'GATS5dv',\n",
    " 'GATS5i',\n",
    " 'GATS5m',\n",
    " 'GATS5p',\n",
    " 'GATS5pe',\n",
    " 'GATS5s',\n",
    " 'GATS5se',\n",
    " 'GATS5v',\n",
    " 'GATS6Z',\n",
    " 'GATS6are',\n",
    " 'GATS6c',\n",
    " 'GATS6d',\n",
    " 'GATS6dv',\n",
    " 'GATS6i',\n",
    " 'GATS6m',\n",
    " 'GATS6p',\n",
    " 'GATS6pe',\n",
    " 'GATS6s',\n",
    " 'GATS6se',\n",
    " 'GATS6v',\n",
    " 'GATS7Z',\n",
    " 'GATS7are',\n",
    " 'GATS7c',\n",
    " 'GATS7d',\n",
    " 'GATS7dv',\n",
    " 'GATS7i',\n",
    " 'GATS7m',\n",
    " 'GATS7p',\n",
    " 'GATS7pe',\n",
    " 'GATS7s',\n",
    " 'GATS7se',\n",
    " 'GATS7v',\n",
    " 'GATS8Z',\n",
    " 'GATS8are',\n",
    " 'GATS8c',\n",
    " 'GATS8d',\n",
    " 'GATS8dv',\n",
    " 'GATS8i',\n",
    " 'GATS8m',\n",
    " 'GATS8p',\n",
    " 'GATS8pe',\n",
    " 'GATS8s',\n",
    " 'GATS8se',\n",
    " 'GATS8v',\n",
    " 'HybRatio',\n",
    " 'LogEE_A',\n",
    " 'LogEE_D',\n",
    " 'LogEE_Dt',\n",
    " 'LogEE_DzZ',\n",
    " 'LogEE_Dzare',\n",
    " 'LogEE_Dzi',\n",
    " 'LogEE_Dzm',\n",
    " 'LogEE_Dzp',\n",
    " 'LogEE_Dzpe',\n",
    " 'LogEE_Dzse',\n",
    " 'LogEE_Dzv',\n",
    " 'MATS1Z',\n",
    " 'MATS1are',\n",
    " 'MATS1c',\n",
    " 'MATS1d',\n",
    " 'MATS1dv',\n",
    " 'MATS1i',\n",
    " 'MATS1m',\n",
    " 'MATS1p',\n",
    " 'MATS1pe',\n",
    " 'MATS1s',\n",
    " 'MATS1se',\n",
    " 'MATS1v',\n",
    " 'MATS2Z',\n",
    " 'MATS2are',\n",
    " 'MATS2c',\n",
    " 'MATS2d',\n",
    " 'MATS2dv',\n",
    " 'MATS2i',\n",
    " 'MATS2m',\n",
    " 'MATS2p',\n",
    " 'MATS2pe',\n",
    " 'MATS2s',\n",
    " 'MATS2se',\n",
    " 'MATS2v',\n",
    " 'MATS3Z',\n",
    " 'MATS3are',\n",
    " 'MATS3c',\n",
    " 'MATS3d',\n",
    " 'MATS3dv',\n",
    " 'MATS3i',\n",
    " 'MATS3m',\n",
    " 'MATS3p',\n",
    " 'MATS3pe',\n",
    " 'MATS3s',\n",
    " 'MATS3se',\n",
    " 'MATS3v',\n",
    " 'MATS4Z',\n",
    " 'MATS4are',\n",
    " 'MATS4c',\n",
    " 'MATS4d',\n",
    " 'MATS4dv',\n",
    " 'MATS4i',\n",
    " 'MATS4m',\n",
    " 'MATS4p',\n",
    " 'MATS4pe',\n",
    " 'MATS4s',\n",
    " 'MATS4se',\n",
    " 'MATS4v',\n",
    " 'MATS5Z',\n",
    " 'MATS5are',\n",
    " 'MATS5c',\n",
    " 'MATS5d',\n",
    " 'MATS5dv',\n",
    " 'MATS5i',\n",
    " 'MATS5m',\n",
    " 'MATS5p',\n",
    " 'MATS5pe',\n",
    " 'MATS5s',\n",
    " 'MATS5se',\n",
    " 'MATS5v',\n",
    " 'MATS6Z',\n",
    " 'MATS6are',\n",
    " 'MATS6c',\n",
    " 'MATS6d',\n",
    " 'MATS6dv',\n",
    " 'MATS6i',\n",
    " 'MATS6m',\n",
    " 'MATS6p',\n",
    " 'MATS6pe',\n",
    " 'MATS6s',\n",
    " 'MATS6se',\n",
    " 'MATS6v',\n",
    " 'MATS7Z',\n",
    " 'MATS7are',\n",
    " 'MATS7c',\n",
    " 'MATS7d',\n",
    " 'MATS7dv',\n",
    " 'MATS7i',\n",
    " 'MATS7m',\n",
    " 'MATS7p',\n",
    " 'MATS7pe',\n",
    " 'MATS7s',\n",
    " 'MATS7se',\n",
    " 'MATS7v',\n",
    " 'MATS8Z',\n",
    " 'MATS8are',\n",
    " 'MATS8c',\n",
    " 'MATS8d',\n",
    " 'MATS8dv',\n",
    " 'MATS8i',\n",
    " 'MATS8m',\n",
    " 'MATS8p',\n",
    " 'MATS8pe',\n",
    " 'MATS8s',\n",
    " 'MATS8se',\n",
    " 'MATS8v',\n",
    " 'MZ',\n",
    " 'Mare',\n",
    " 'Mi',\n",
    " 'Mm',\n",
    " 'Mp',\n",
    " 'Mpe',\n",
    " 'Mse',\n",
    " 'Mv',\n",
    " 'NaaCH',\n",
    " 'NaaN',\n",
    " 'NaaNH',\n",
    " 'NaaO',\n",
    " 'NaaS',\n",
    " 'NaaSe',\n",
    " 'NaaaC',\n",
    " 'NaasC',\n",
    " 'NaasN',\n",
    " 'NdCH2',\n",
    " 'NdNH',\n",
    " 'NdO',\n",
    " 'NdS',\n",
    " 'NdSe',\n",
    " 'NddC',\n",
    " 'NddsN',\n",
    " 'NddssS',\n",
    " 'NddssSe',\n",
    " 'NdsCH',\n",
    " 'NdsN',\n",
    " 'NdssC',\n",
    " 'NdssS',\n",
    " 'NdssSe',\n",
    " 'NdsssP',\n",
    " 'NsAsH2',\n",
    " 'NsBr',\n",
    " 'NsCH3',\n",
    " 'NsCl',\n",
    " 'NsF',\n",
    " 'NsGeH3',\n",
    " 'NsI',\n",
    " 'NsLi',\n",
    " 'NsNH2',\n",
    " 'NsNH3',\n",
    " 'NsOH',\n",
    " 'NsPH2',\n",
    " 'NsPbH3',\n",
    " 'NsSH',\n",
    " 'NsSeH',\n",
    " 'NsSiH3',\n",
    " 'NsSnH3',\n",
    " 'NssAsH',\n",
    " 'NssBH',\n",
    " 'NssBe',\n",
    " 'NssCH2',\n",
    " 'NssGeH2',\n",
    " 'NssNH',\n",
    " 'NssNH2',\n",
    " 'NssO',\n",
    " 'NssPH',\n",
    " 'NssPbH2',\n",
    " 'NssS',\n",
    " 'NssSe',\n",
    " 'NssSiH2',\n",
    " 'NssSnH2',\n",
    " 'NsssAs',\n",
    " 'NsssB',\n",
    " 'NsssCH',\n",
    " 'NsssGeH',\n",
    " 'NsssN',\n",
    " 'NsssNH',\n",
    " 'NsssP',\n",
    " 'NsssPbH',\n",
    " 'NsssSiH',\n",
    " 'NsssSnH',\n",
    " 'NsssdAs',\n",
    " 'NssssB',\n",
    " 'NssssBe',\n",
    " 'NssssC',\n",
    " 'NssssGe',\n",
    " 'NssssN',\n",
    " 'NssssPb',\n",
    " 'NssssSi',\n",
    " 'NssssSn',\n",
    " 'NsssssAs',\n",
    " 'NsssssP',\n",
    " 'NtCH',\n",
    " 'NtN',\n",
    " 'NtsC',\n",
    " 'RNCG',\n",
    " 'RPCG',\n",
    " 'SM1_Dt',\n",
    " 'SM1_DzZ',\n",
    " 'SM1_Dzare',\n",
    " 'SM1_Dzi',\n",
    " 'SM1_Dzm',\n",
    " 'SM1_Dzp',\n",
    " 'SM1_Dzpe',\n",
    " 'SM1_Dzse',\n",
    " 'SM1_Dzv',\n",
    " 'SZ',\n",
    " 'SaaCH',\n",
    " 'SaaN',\n",
    " 'SaaNH',\n",
    " 'SaaaC',\n",
    " 'SaasC',\n",
    " 'Sare',\n",
    " 'SdCH2',\n",
    " 'SdNH',\n",
    " 'SddC',\n",
    " 'SdsCH',\n",
    " 'SdsN',\n",
    " 'SdssC',\n",
    " 'Si',\n",
    " 'Sm',\n",
    " 'Smiles',\n",
    " 'Sp',\n",
    " 'SpAD_A',\n",
    " 'SpAD_D',\n",
    " 'SpAD_Dt',\n",
    " 'SpAD_DzZ',\n",
    " 'SpAD_Dzare',\n",
    " 'SpAD_Dzi',\n",
    " 'SpAD_Dzm',\n",
    " 'SpAD_Dzp',\n",
    " 'SpAD_Dzpe',\n",
    " 'SpAD_Dzse',\n",
    " 'SpAD_Dzv',\n",
    " 'SpAbs_A',\n",
    " 'SpAbs_D',\n",
    " 'SpAbs_Dt',\n",
    " 'SpAbs_DzZ',\n",
    " 'SpAbs_Dzare',\n",
    " 'SpAbs_Dzi',\n",
    " 'SpAbs_Dzm',\n",
    " 'SpAbs_Dzp',\n",
    " 'SpAbs_Dzpe',\n",
    " 'SpAbs_Dzse',\n",
    " 'SpAbs_Dzv',\n",
    " 'SpDiam_A',\n",
    " 'SpDiam_D',\n",
    " 'SpDiam_Dt',\n",
    " 'SpDiam_DzZ',\n",
    " 'SpDiam_Dzare',\n",
    " 'SpDiam_Dzi',\n",
    " 'SpDiam_Dzm',\n",
    " 'SpDiam_Dzp',\n",
    " 'SpDiam_Dzpe',\n",
    " 'SpDiam_Dzse',\n",
    " 'SpDiam_Dzv',\n",
    " 'SpMAD_A',\n",
    " 'SpMAD_D',\n",
    " 'SpMAD_Dt',\n",
    " 'SpMAD_DzZ',\n",
    " 'SpMAD_Dzare',\n",
    " 'SpMAD_Dzi',\n",
    " 'SpMAD_Dzm',\n",
    " 'SpMAD_Dzp',\n",
    " 'SpMAD_Dzpe',\n",
    " 'SpMAD_Dzse',\n",
    " 'SpMAD_Dzv',\n",
    " 'SpMax_A',\n",
    " 'SpMax_D',\n",
    " 'SpMax_Dt',\n",
    " 'SpMax_DzZ',\n",
    " 'SpMax_Dzare',\n",
    " 'SpMax_Dzi',\n",
    " 'SpMax_Dzm',\n",
    " 'SpMax_Dzp',\n",
    " 'SpMax_Dzpe',\n",
    " 'SpMax_Dzse',\n",
    " 'SpMax_Dzv',\n",
    " 'Spe',\n",
    " 'SsCH3',\n",
    " 'SsLi',\n",
    " 'SsNH2',\n",
    " 'SsNH3',\n",
    " 'Sse',\n",
    " 'SssBH',\n",
    " 'SssBe',\n",
    " 'SssCH2',\n",
    " 'SssNH',\n",
    " 'SssNH2',\n",
    " 'SsssB',\n",
    " 'SsssCH',\n",
    " 'SsssN',\n",
    " 'SsssNH',\n",
    " 'SssssB',\n",
    " 'SssssBe',\n",
    " 'SssssC',\n",
    " 'StCH',\n",
    " 'StN',\n",
    " 'StsC',\n",
    " 'Sv',\n",
    " 'VE1_A',\n",
    " 'VE1_D',\n",
    " 'VE1_Dt',\n",
    " 'VE1_DzZ',\n",
    " 'VE1_Dzare',\n",
    " 'VE1_Dzi',\n",
    " 'VE1_Dzm',\n",
    " 'VE1_Dzp',\n",
    " 'VE1_Dzpe',\n",
    " 'VE1_Dzse',\n",
    " 'VE1_Dzv',\n",
    " 'VE2_A',\n",
    " 'VE2_D',\n",
    " 'VE2_Dt',\n",
    " 'VE2_DzZ',\n",
    " 'VE2_Dzare',\n",
    " 'VE2_Dzi',\n",
    " 'VE2_Dzm',\n",
    " 'VE2_Dzp',\n",
    " 'VE2_Dzpe',\n",
    " 'VE2_Dzse',\n",
    " 'VE2_Dzv',\n",
    " 'VE3_A',\n",
    " 'VE3_D',\n",
    " 'VE3_Dt',\n",
    " 'VE3_DzZ',\n",
    " 'VE3_Dzare',\n",
    " 'VE3_Dzi',\n",
    " 'VE3_Dzm',\n",
    " 'VE3_Dzp',\n",
    " 'VE3_Dzpe',\n",
    " 'VE3_Dzse',\n",
    " 'VE3_Dzv',\n",
    " 'VR1_A',\n",
    " 'VR1_D',\n",
    " 'VR1_Dt',\n",
    " 'VR1_DzZ',\n",
    " 'VR1_Dzare',\n",
    " 'VR1_Dzi',\n",
    " 'VR1_Dzm',\n",
    " 'VR1_Dzp',\n",
    " 'VR1_Dzpe',\n",
    " 'VR1_Dzse',\n",
    " 'VR1_Dzv',\n",
    " 'VR2_A',\n",
    " 'VR2_D',\n",
    " 'VR2_Dt',\n",
    " 'VR2_DzZ',\n",
    " 'VR2_Dzare',\n",
    " 'VR2_Dzi',\n",
    " 'VR2_Dzm',\n",
    " 'VR2_Dzp',\n",
    " 'VR2_Dzpe',\n",
    " 'VR2_Dzse',\n",
    " 'VR2_Dzv',\n",
    " 'VR3_A',\n",
    " 'VR3_D',\n",
    " 'VR3_Dt',\n",
    " 'VR3_DzZ',\n",
    " 'VR3_Dzare',\n",
    " 'VR3_Dzi',\n",
    " 'VR3_Dzm',\n",
    " 'VR3_Dzp',\n",
    " 'VR3_Dzpe',\n",
    " 'VR3_Dzse',\n",
    " 'VR3_Dzv',\n",
    " 'Xc-3d',\n",
    " 'Xc-3dv',\n",
    " 'Xc-4d',\n",
    " 'Xc-4dv',\n",
    " 'Xc-5d',\n",
    " 'Xc-5dv',\n",
    " 'Xc-6d',\n",
    " 'Xc-6dv',\n",
    " 'Xch-3d',\n",
    " 'Xch-3dv',\n",
    " 'Xch-4d',\n",
    " 'Xch-4dv',\n",
    " 'Xch-5d',\n",
    " 'Xch-5dv',\n",
    " 'Xch-6d',\n",
    " 'Xch-6dv',\n",
    " 'Xch-7d',\n",
    " 'Xch-7dv',\n",
    " 'Xp-0d',\n",
    " 'Xp-0dv',\n",
    " 'Xp-1d',\n",
    " 'Xp-1dv',\n",
    " 'Xp-2d',\n",
    " 'Xp-2dv',\n",
    " 'Xp-3d',\n",
    " 'Xp-3dv',\n",
    " 'Xp-4d',\n",
    " 'Xp-4dv',\n",
    " 'Xp-5d',\n",
    " 'Xp-5dv',\n",
    " 'Xp-6d',\n",
    " 'Xp-6dv',\n",
    " 'Xp-7d',\n",
    " 'Xp-7dv',\n",
    " 'Xpc-4d',\n",
    " 'Xpc-4dv',\n",
    " 'Xpc-5d',\n",
    " 'Xpc-5dv',\n",
    " 'Xpc-6d',\n",
    " 'Xpc-6dv',\n",
    " 'nAcid',\n",
    " 'nAromAtom',\n",
    " 'nAromBond',\n",
    " 'nAtom',\n",
    " 'nB',\n",
    " 'nBase',\n",
    " 'nBonds',\n",
    " 'nBondsA',\n",
    " 'nBondsD',\n",
    " 'nBondsKD',\n",
    " 'nBondsKS',\n",
    " 'nBondsM',\n",
    " 'nBondsO',\n",
    " 'nBondsS',\n",
    " 'nBondsT',\n",
    " 'nBr',\n",
    " 'nBridgehead',\n",
    " 'nC',\n",
    " 'nCl',\n",
    " 'nF',\n",
    " 'nH',\n",
    " 'nHeavyAtom',\n",
    " 'nHetero',\n",
    " 'nI',\n",
    " 'nN',\n",
    " 'nO',\n",
    " 'nP',\n",
    " 'nS',\n",
    " 'nSpiro', 'nX']\n",
    "len(overlapcols1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The columns above are those that are redundant in the dataset, think ABC_x, ABC_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Smiles</th>\n",
       "      <th>Sum of Reciprical of rank</th>\n",
       "      <th>genescore_times_floorlog10ofpvalue</th>\n",
       "      <th>ABC</th>\n",
       "      <th>ABCGG</th>\n",
       "      <th>nAcid</th>\n",
       "      <th>nBase</th>\n",
       "      <th>SpAbs_A</th>\n",
       "      <th>SpMax_A</th>\n",
       "      <th>SpDiam_A</th>\n",
       "      <th>...</th>\n",
       "      <th>Hypertrophy &gt;= 1.33, 2) no other finding</th>\n",
       "      <th>Increased glycogen, 2) any other finding</th>\n",
       "      <th>Increased mitosis, 1) no other findings</th>\n",
       "      <th>Necrosis, 1) no other findings</th>\n",
       "      <th>Single cell necrosis, 2) allowing hypertrophy</th>\n",
       "      <th>Tbili &gt;= 100%, 2) with hyperplasia at any grade, any other path at any grade</th>\n",
       "      <th>Trigs &lt; -60%, 1) no path findings and FC &gt;-15%</th>\n",
       "      <th>Trigs &gt; 80%, 1) no path findings</th>\n",
       "      <th>Vacuolation, 2) allowing hypertrophy at any grade</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>Clc1ccc(C(=O)n2c(c(c3c2ccc(OC)c3)CC(=O)OCC(=O)...</td>\n",
       "      <td>1.362178</td>\n",
       "      <td>17.754620</td>\n",
       "      <td>22.293568</td>\n",
       "      <td>18.187943</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.578780</td>\n",
       "      <td>2.545171</td>\n",
       "      <td>4.912104</td>\n",
       "      <td>...</td>\n",
       "      <td>2.947130</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.506682</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.045471</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.519317</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>Clc1ccc(C[C@@H](NC(=O)[C@H](NC(=O)C)Cc2cc3c(cc...</td>\n",
       "      <td>2.591504</td>\n",
       "      <td>42.693249</td>\n",
       "      <td>76.862682</td>\n",
       "      <td>58.368692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>125.113479</td>\n",
       "      <td>2.412042</td>\n",
       "      <td>4.800123</td>\n",
       "      <td>...</td>\n",
       "      <td>5.378359</td>\n",
       "      <td>0.262984</td>\n",
       "      <td>1.064018</td>\n",
       "      <td>0.428908</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.930087</td>\n",
       "      <td>0.083993</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003934</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>O(C[C@@H](O)CNC(C)C)c1c(cc(NC(=O)CCC)cc1)C(=O)C</td>\n",
       "      <td>0.099660</td>\n",
       "      <td>0.760064</td>\n",
       "      <td>17.546021</td>\n",
       "      <td>14.908963</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.601421</td>\n",
       "      <td>2.354497</td>\n",
       "      <td>4.708995</td>\n",
       "      <td>...</td>\n",
       "      <td>0.190239</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.131764</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.155351</td>\n",
       "      <td>0.018390</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>O([C@H]1[C@H](O)[C@@H](O)[C@H](O[C@@H]1CO)O[C@...</td>\n",
       "      <td>4.044450</td>\n",
       "      <td>121.783605</td>\n",
       "      <td>33.790265</td>\n",
       "      <td>26.700638</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55.609056</td>\n",
       "      <td>2.483834</td>\n",
       "      <td>4.967669</td>\n",
       "      <td>...</td>\n",
       "      <td>0.173598</td>\n",
       "      <td>0.000899</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.985869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.018156</td>\n",
       "      <td>0.006916</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.016143</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>Oc1cc2c(cc1)cccc2</td>\n",
       "      <td>0.125454</td>\n",
       "      <td>1.675356</td>\n",
       "      <td>8.554231</td>\n",
       "      <td>7.309128</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.426921</td>\n",
       "      <td>2.333244</td>\n",
       "      <td>4.666488</td>\n",
       "      <td>...</td>\n",
       "      <td>0.072155</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.019041</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.069446</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945</th>\n",
       "      <td>S(=O)(=O)(N)c1sc(NC(=O)C)nn1</td>\n",
       "      <td>0.011161</td>\n",
       "      <td>0.173860</td>\n",
       "      <td>9.826314</td>\n",
       "      <td>9.493560</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.562939</td>\n",
       "      <td>2.359004</td>\n",
       "      <td>4.584174</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013530</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.011367</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000676</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1105</th>\n",
       "      <td>S=C=Nc1c2c(ccc1)cccc2</td>\n",
       "      <td>0.026605</td>\n",
       "      <td>0.107716</td>\n",
       "      <td>9.818615</td>\n",
       "      <td>8.499990</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.130940</td>\n",
       "      <td>2.368752</td>\n",
       "      <td>4.737504</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064164</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.010211</td>\n",
       "      <td>0.010211</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001686</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1127</th>\n",
       "      <td>c1(c(cc(cc1)[N+](=O)[O-])[N+](=O)[O-])O</td>\n",
       "      <td>0.734969</td>\n",
       "      <td>6.781955</td>\n",
       "      <td>9.618017</td>\n",
       "      <td>9.150803</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.158804</td>\n",
       "      <td>2.337747</td>\n",
       "      <td>4.675495</td>\n",
       "      <td>...</td>\n",
       "      <td>1.470138</td>\n",
       "      <td>0.083020</td>\n",
       "      <td>0.254973</td>\n",
       "      <td>0.010825</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.311952</td>\n",
       "      <td>0.197157</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.050395</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1128</th>\n",
       "      <td>c1(c(cc(cc1[N+](=O)[O-])[N+](=O)[O-])[N+](=O)[...</td>\n",
       "      <td>0.064130</td>\n",
       "      <td>0.651977</td>\n",
       "      <td>11.877237</td>\n",
       "      <td>11.411786</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.380381</td>\n",
       "      <td>2.411142</td>\n",
       "      <td>4.822284</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010342</td>\n",
       "      <td>0.008537</td>\n",
       "      <td>0.128815</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.056213</td>\n",
       "      <td>0.137352</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9 rows × 1261 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Smiles  \\\n",
       "129   Clc1ccc(C(=O)n2c(c(c3c2ccc(OC)c3)CC(=O)OCC(=O)...   \n",
       "132   Clc1ccc(C[C@@H](NC(=O)[C@H](NC(=O)C)Cc2cc3c(cc...   \n",
       "434     O(C[C@@H](O)CNC(C)C)c1c(cc(NC(=O)CCC)cc1)C(=O)C   \n",
       "503   O([C@H]1[C@H](O)[C@@H](O)[C@H](O[C@@H]1CO)O[C@...   \n",
       "910                                   Oc1cc2c(cc1)cccc2   \n",
       "945                        S(=O)(=O)(N)c1sc(NC(=O)C)nn1   \n",
       "1105                              S=C=Nc1c2c(ccc1)cccc2   \n",
       "1127            c1(c(cc(cc1)[N+](=O)[O-])[N+](=O)[O-])O   \n",
       "1128  c1(c(cc(cc1[N+](=O)[O-])[N+](=O)[O-])[N+](=O)[...   \n",
       "\n",
       "      Sum of Reciprical of rank  genescore_times_floorlog10ofpvalue  \\\n",
       "129                    1.362178                           17.754620   \n",
       "132                    2.591504                           42.693249   \n",
       "434                    0.099660                            0.760064   \n",
       "503                    4.044450                          121.783605   \n",
       "910                    0.125454                            1.675356   \n",
       "945                    0.011161                            0.173860   \n",
       "1105                   0.026605                            0.107716   \n",
       "1127                   0.734969                            6.781955   \n",
       "1128                   0.064130                            0.651977   \n",
       "\n",
       "            ABC      ABCGG  nAcid  nBase     SpAbs_A   SpMax_A  SpDiam_A  ...  \\\n",
       "129   22.293568  18.187943    1.0    0.0   36.578780  2.545171  4.912104  ...   \n",
       "132   76.862682  58.368692    0.0    1.0  125.113479  2.412042  4.800123  ...   \n",
       "434   17.546021  14.908963    0.0    1.0   28.601421  2.354497  4.708995  ...   \n",
       "503   33.790265  26.700638    0.0    1.0   55.609056  2.483834  4.967669  ...   \n",
       "910    8.554231   7.309128    0.0    0.0   14.426921  2.333244  4.666488  ...   \n",
       "945    9.826314   9.493560    0.0    0.0   14.562939  2.359004  4.584174  ...   \n",
       "1105   9.818615   8.499990    0.0    0.0   17.130940  2.368752  4.737504  ...   \n",
       "1127   9.618017   9.150803    0.0    0.0   15.158804  2.337747  4.675495  ...   \n",
       "1128  11.877237  11.411786    0.0    0.0   18.380381  2.411142  4.822284  ...   \n",
       "\n",
       "      Hypertrophy >= 1.33, 2) no other finding  \\\n",
       "129                                   2.947130   \n",
       "132                                   5.378359   \n",
       "434                                   0.190239   \n",
       "503                                   0.173598   \n",
       "910                                   0.072155   \n",
       "945                                   0.013530   \n",
       "1105                                  0.064164   \n",
       "1127                                  1.470138   \n",
       "1128                                  0.010342   \n",
       "\n",
       "      Increased glycogen, 2) any other finding  \\\n",
       "129                                        NaN   \n",
       "132                                   0.262984   \n",
       "434                                        NaN   \n",
       "503                                   0.000899   \n",
       "910                                        NaN   \n",
       "945                                        NaN   \n",
       "1105                                       NaN   \n",
       "1127                                  0.083020   \n",
       "1128                                  0.008537   \n",
       "\n",
       "      Increased mitosis, 1) no other findings  Necrosis, 1) no other findings  \\\n",
       "129                                       NaN                        1.506682   \n",
       "132                                  1.064018                        0.428908   \n",
       "434                                       NaN                        0.131764   \n",
       "503                                       NaN                        6.985869   \n",
       "910                                       NaN                        0.019041   \n",
       "945                                       NaN                        0.011367   \n",
       "1105                                 0.010211                        0.010211   \n",
       "1127                                 0.254973                        0.010825   \n",
       "1128                                 0.128815                             NaN   \n",
       "\n",
       "      Single cell necrosis, 2) allowing hypertrophy  \\\n",
       "129                                             NaN   \n",
       "132                                             NaN   \n",
       "434                                             NaN   \n",
       "503                                             NaN   \n",
       "910                                             NaN   \n",
       "945                                             NaN   \n",
       "1105                                            NaN   \n",
       "1127                                       0.000476   \n",
       "1128                                            NaN   \n",
       "\n",
       "      Tbili >= 100%, 2) with hyperplasia at any grade, any other path at any grade  \\\n",
       "129                                            2.045471                              \n",
       "132                                            0.930087                              \n",
       "434                                            0.155351                              \n",
       "503                                            7.018156                              \n",
       "910                                            0.069446                              \n",
       "945                                            0.000676                              \n",
       "1105                                           0.001686                              \n",
       "1127                                           0.311952                              \n",
       "1128                                           0.056213                              \n",
       "\n",
       "      Trigs < -60%, 1) no path findings and FC >-15%  \\\n",
       "129                                              NaN   \n",
       "132                                         0.083993   \n",
       "434                                         0.018390   \n",
       "503                                         0.006916   \n",
       "910                                              NaN   \n",
       "945                                              NaN   \n",
       "1105                                             NaN   \n",
       "1127                                        0.197157   \n",
       "1128                                        0.137352   \n",
       "\n",
       "      Trigs > 80%, 1) no path findings  \\\n",
       "129                                NaN   \n",
       "132                                NaN   \n",
       "434                                NaN   \n",
       "503                                NaN   \n",
       "910                                NaN   \n",
       "945                                NaN   \n",
       "1105                               NaN   \n",
       "1127                               NaN   \n",
       "1128                               NaN   \n",
       "\n",
       "      Vacuolation, 2) allowing hypertrophy at any grade  label  \n",
       "129                                            0.519317      1  \n",
       "132                                            0.003934      1  \n",
       "434                                                 NaN      1  \n",
       "503                                            0.016143      1  \n",
       "910                                                 NaN      1  \n",
       "945                                                 NaN      0  \n",
       "1105                                                NaN      1  \n",
       "1127                                           0.050395      1  \n",
       "1128                                                NaN      1  \n",
       "\n",
       "[9 rows x 1261 columns]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_list = list(merged_df_with_drops['Smiles'].unique())[0:10]\n",
    "liu_list = list(liu_df['Smiles'].unique())\n",
    "liu_df[liu_df['Smiles'].isin(first_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Smiles</th>\n",
       "      <th>Liver</th>\n",
       "      <th>Mol_ID</th>\n",
       "      <th>ABC</th>\n",
       "      <th>ABCGG</th>\n",
       "      <th>nAcid</th>\n",
       "      <th>nBase</th>\n",
       "      <th>SpAbs_A</th>\n",
       "      <th>SpMax_A</th>\n",
       "      <th>...</th>\n",
       "      <th>SRW10</th>\n",
       "      <th>TSRW10</th>\n",
       "      <th>MW</th>\n",
       "      <th>AMW</th>\n",
       "      <th>WPath</th>\n",
       "      <th>WPol</th>\n",
       "      <th>Zagreb1</th>\n",
       "      <th>Zagreb2</th>\n",
       "      <th>mZagreb1</th>\n",
       "      <th>mZagreb2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>S=C=Nc1c2c(ccc1)cccc2</td>\n",
       "      <td>Hepatotoxicity</td>\n",
       "      <td>Mol0</td>\n",
       "      <td>9.818615</td>\n",
       "      <td>8.499990</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17.130940</td>\n",
       "      <td>2.368752</td>\n",
       "      <td>...</td>\n",
       "      <td>9.382527</td>\n",
       "      <td>44.188427</td>\n",
       "      <td>185.029920</td>\n",
       "      <td>9.251496</td>\n",
       "      <td>236</td>\n",
       "      <td>18</td>\n",
       "      <td>64.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>3.583333</td>\n",
       "      <td>3.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>c1(c(cc(cc1[N+](=O)[O-])[N+](=O)[O-])[N+](=O)[...</td>\n",
       "      <td>Hepatotoxicity</td>\n",
       "      <td>Mol1</td>\n",
       "      <td>11.877237</td>\n",
       "      <td>11.411786</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18.380381</td>\n",
       "      <td>2.411142</td>\n",
       "      <td>...</td>\n",
       "      <td>9.637763</td>\n",
       "      <td>48.280750</td>\n",
       "      <td>227.017835</td>\n",
       "      <td>10.810373</td>\n",
       "      <td>408</td>\n",
       "      <td>25</td>\n",
       "      <td>78.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>8.277778</td>\n",
       "      <td>3.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>c1(c(cc(cc1)[N+](=O)[O-])[N+](=O)[O-])O</td>\n",
       "      <td>Hepatotoxicity</td>\n",
       "      <td>Mol2</td>\n",
       "      <td>9.618017</td>\n",
       "      <td>9.150803</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.158804</td>\n",
       "      <td>2.337747</td>\n",
       "      <td>...</td>\n",
       "      <td>9.300821</td>\n",
       "      <td>43.952649</td>\n",
       "      <td>184.012021</td>\n",
       "      <td>10.824237</td>\n",
       "      <td>240</td>\n",
       "      <td>18</td>\n",
       "      <td>62.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>6.305556</td>\n",
       "      <td>2.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Oc1cc2c(cc1)cccc2</td>\n",
       "      <td>Hepatotoxicity</td>\n",
       "      <td>Mol4</td>\n",
       "      <td>8.554231</td>\n",
       "      <td>7.309128</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14.426921</td>\n",
       "      <td>2.333244</td>\n",
       "      <td>...</td>\n",
       "      <td>9.225721</td>\n",
       "      <td>41.498380</td>\n",
       "      <td>144.057515</td>\n",
       "      <td>7.581974</td>\n",
       "      <td>144</td>\n",
       "      <td>14</td>\n",
       "      <td>56.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>3.083333</td>\n",
       "      <td>2.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Clc1ccc(C[C@@H](NC(=O)[C@H](NC(=O)C)Cc2cc3c(cc...</td>\n",
       "      <td>Hepatotoxicity</td>\n",
       "      <td>Mol5</td>\n",
       "      <td>76.862682</td>\n",
       "      <td>58.368692</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>125.113479</td>\n",
       "      <td>2.412042</td>\n",
       "      <td>...</td>\n",
       "      <td>11.307904</td>\n",
       "      <td>155.806250</td>\n",
       "      <td>1414.684071</td>\n",
       "      <td>7.217776</td>\n",
       "      <td>70741</td>\n",
       "      <td>156</td>\n",
       "      <td>502.0</td>\n",
       "      <td>572.0</td>\n",
       "      <td>38.527778</td>\n",
       "      <td>22.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>O([C@H]1[C@H](O)[C@@H](O)[C@H](O[C@@H]1CO)O[C@...</td>\n",
       "      <td>Hepatotoxicity</td>\n",
       "      <td>Mol6</td>\n",
       "      <td>33.790265</td>\n",
       "      <td>26.700638</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>55.609056</td>\n",
       "      <td>2.483834</td>\n",
       "      <td>...</td>\n",
       "      <td>10.886783</td>\n",
       "      <td>82.039467</td>\n",
       "      <td>645.248014</td>\n",
       "      <td>7.416644</td>\n",
       "      <td>7413</td>\n",
       "      <td>86</td>\n",
       "      <td>234.0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>18.722222</td>\n",
       "      <td>9.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>O(C[C@@H](O)CNC(C)C)c1c(cc(NC(=O)CCC)cc1)C(=O)C</td>\n",
       "      <td>Hepatotoxicity</td>\n",
       "      <td>Mol7</td>\n",
       "      <td>17.546021</td>\n",
       "      <td>14.908963</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>28.601421</td>\n",
       "      <td>2.354497</td>\n",
       "      <td>...</td>\n",
       "      <td>9.663897</td>\n",
       "      <td>57.384332</td>\n",
       "      <td>336.204907</td>\n",
       "      <td>6.465479</td>\n",
       "      <td>1568</td>\n",
       "      <td>31</td>\n",
       "      <td>110.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>10.277778</td>\n",
       "      <td>5.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Clc1ccc(C(=O)n2c(c(c3c2ccc(OC)c3)CC(=O)OCC(=O)...</td>\n",
       "      <td>Hepatotoxicity</td>\n",
       "      <td>Mol8</td>\n",
       "      <td>22.293568</td>\n",
       "      <td>18.187943</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>36.578780</td>\n",
       "      <td>2.545171</td>\n",
       "      <td>...</td>\n",
       "      <td>10.280656</td>\n",
       "      <td>79.078849</td>\n",
       "      <td>415.082265</td>\n",
       "      <td>8.831538</td>\n",
       "      <td>2238</td>\n",
       "      <td>47</td>\n",
       "      <td>150.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>10.972222</td>\n",
       "      <td>6.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>S(=O)(=O)(N)c1sc(NC(=O)C)nn1</td>\n",
       "      <td>NonHepatotoxicity</td>\n",
       "      <td>Mol9</td>\n",
       "      <td>9.826314</td>\n",
       "      <td>9.493560</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14.562939</td>\n",
       "      <td>2.359004</td>\n",
       "      <td>...</td>\n",
       "      <td>9.256938</td>\n",
       "      <td>57.567931</td>\n",
       "      <td>221.988132</td>\n",
       "      <td>11.683586</td>\n",
       "      <td>257</td>\n",
       "      <td>14</td>\n",
       "      <td>64.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>6.395833</td>\n",
       "      <td>2.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9 rows × 1617 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                             Smiles  \\\n",
       "0           0                              S=C=Nc1c2c(ccc1)cccc2   \n",
       "1           1  c1(c(cc(cc1[N+](=O)[O-])[N+](=O)[O-])[N+](=O)[...   \n",
       "2           2            c1(c(cc(cc1)[N+](=O)[O-])[N+](=O)[O-])O   \n",
       "4           4                                  Oc1cc2c(cc1)cccc2   \n",
       "5           5  Clc1ccc(C[C@@H](NC(=O)[C@H](NC(=O)C)Cc2cc3c(cc...   \n",
       "6           6  O([C@H]1[C@H](O)[C@@H](O)[C@H](O[C@@H]1CO)O[C@...   \n",
       "7           7    O(C[C@@H](O)CNC(C)C)c1c(cc(NC(=O)CCC)cc1)C(=O)C   \n",
       "8           8  Clc1ccc(C(=O)n2c(c(c3c2ccc(OC)c3)CC(=O)OCC(=O)...   \n",
       "9           9                       S(=O)(=O)(N)c1sc(NC(=O)C)nn1   \n",
       "\n",
       "               Liver Mol_ID        ABC      ABCGG  nAcid  nBase     SpAbs_A  \\\n",
       "0     Hepatotoxicity   Mol0   9.818615   8.499990      0      0   17.130940   \n",
       "1     Hepatotoxicity   Mol1  11.877237  11.411786      0      0   18.380381   \n",
       "2     Hepatotoxicity   Mol2   9.618017   9.150803      0      0   15.158804   \n",
       "4     Hepatotoxicity   Mol4   8.554231   7.309128      0      0   14.426921   \n",
       "5     Hepatotoxicity   Mol5  76.862682  58.368692      0      1  125.113479   \n",
       "6     Hepatotoxicity   Mol6  33.790265  26.700638      0      1   55.609056   \n",
       "7     Hepatotoxicity   Mol7  17.546021  14.908963      0      1   28.601421   \n",
       "8     Hepatotoxicity   Mol8  22.293568  18.187943      1      0   36.578780   \n",
       "9  NonHepatotoxicity   Mol9   9.826314   9.493560      0      0   14.562939   \n",
       "\n",
       "    SpMax_A  ...      SRW10      TSRW10           MW        AMW  WPath  WPol  \\\n",
       "0  2.368752  ...   9.382527   44.188427   185.029920   9.251496    236    18   \n",
       "1  2.411142  ...   9.637763   48.280750   227.017835  10.810373    408    25   \n",
       "2  2.337747  ...   9.300821   43.952649   184.012021  10.824237    240    18   \n",
       "4  2.333244  ...   9.225721   41.498380   144.057515   7.581974    144    14   \n",
       "5  2.412042  ...  11.307904  155.806250  1414.684071   7.217776  70741   156   \n",
       "6  2.483834  ...  10.886783   82.039467   645.248014   7.416644   7413    86   \n",
       "7  2.354497  ...   9.663897   57.384332   336.204907   6.465479   1568    31   \n",
       "8  2.545171  ...  10.280656   79.078849   415.082265   8.831538   2238    47   \n",
       "9  2.359004  ...   9.256938   57.567931   221.988132  11.683586    257    14   \n",
       "\n",
       "   Zagreb1  Zagreb2   mZagreb1   mZagreb2  \n",
       "0     64.0     74.0   3.583333   3.055556  \n",
       "1     78.0     90.0   8.277778   3.555556  \n",
       "2     62.0     70.0   6.305556   2.916667  \n",
       "4     56.0     64.0   3.083333   2.444444  \n",
       "5    502.0    572.0  38.527778  22.500000  \n",
       "6    234.0    285.0  18.722222   9.777778  \n",
       "7    110.0    120.0  10.277778   5.555556  \n",
       "8    150.0    177.0  10.972222   6.444444  \n",
       "9     64.0     70.0   6.395833   2.750000  \n",
       "\n",
       "[9 rows x 1617 columns]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smileslist2 = list(liu_df[liu_df['Smiles'].isin(first_list)]['Smiles'].unique())\n",
    "full_df[full_df['Smiles'].isin(smileslist2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2420, 1877)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_liu_df = pd.merge(merged_df_with_drops, liu_df, on=overlapcols1, how='outer')\n",
    "merged_liu_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge_on_multiple = pd.concat([merged_df_with_drops, liu_df], axis=1,)\n",
    "# merge_on_multiple.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge_on_multiple.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ABC_x</th>\n",
       "      <th>ABC_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.818615</td>\n",
       "      <td>9.818615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11.877237</td>\n",
       "      <td>11.877237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.618017</td>\n",
       "      <td>9.618017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.554231</td>\n",
       "      <td>8.554231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>76.862682</td>\n",
       "      <td>76.862682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>33.790265</td>\n",
       "      <td>33.790265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>17.546021</td>\n",
       "      <td>17.546021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ABC_x      ABC_y\n",
       "0   9.818615   9.818615\n",
       "1  11.877237  11.877237\n",
       "2   9.618017   9.618017\n",
       "3   8.554231   8.554231\n",
       "4  76.862682  76.862682\n",
       "5  33.790265  33.790265\n",
       "6  17.546021  17.546021"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "innermerge = pd.merge(merged_df_with_drops, liu_df, on = 'Smiles', how='inner')\n",
    "innermerge[['ABC_x', 'ABC_y']].head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Smiles</th>\n",
       "      <th>Liver</th>\n",
       "      <th>Mol_ID</th>\n",
       "      <th>ABC</th>\n",
       "      <th>ABCGG</th>\n",
       "      <th>nAcid</th>\n",
       "      <th>nBase</th>\n",
       "      <th>SpAbs_A</th>\n",
       "      <th>SpMax_A</th>\n",
       "      <th>SpDiam_A</th>\n",
       "      <th>...</th>\n",
       "      <th>SRW10</th>\n",
       "      <th>TSRW10</th>\n",
       "      <th>MW</th>\n",
       "      <th>AMW</th>\n",
       "      <th>WPath</th>\n",
       "      <th>WPol</th>\n",
       "      <th>Zagreb1</th>\n",
       "      <th>Zagreb2</th>\n",
       "      <th>mZagreb1</th>\n",
       "      <th>mZagreb2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S=C=Nc1c2c(ccc1)cccc2</td>\n",
       "      <td>Hepatotoxicity</td>\n",
       "      <td>Mol0</td>\n",
       "      <td>9.818615</td>\n",
       "      <td>8.499990</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17.130940</td>\n",
       "      <td>2.368752</td>\n",
       "      <td>4.737504</td>\n",
       "      <td>...</td>\n",
       "      <td>9.382527</td>\n",
       "      <td>44.188427</td>\n",
       "      <td>185.029920</td>\n",
       "      <td>9.251496</td>\n",
       "      <td>236</td>\n",
       "      <td>18</td>\n",
       "      <td>64.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>3.583333</td>\n",
       "      <td>3.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c1(c(cc(cc1[N+](=O)[O-])[N+](=O)[O-])[N+](=O)[...</td>\n",
       "      <td>Hepatotoxicity</td>\n",
       "      <td>Mol1</td>\n",
       "      <td>11.877237</td>\n",
       "      <td>11.411786</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18.380381</td>\n",
       "      <td>2.411142</td>\n",
       "      <td>4.822284</td>\n",
       "      <td>...</td>\n",
       "      <td>9.637763</td>\n",
       "      <td>48.280750</td>\n",
       "      <td>227.017835</td>\n",
       "      <td>10.810373</td>\n",
       "      <td>408</td>\n",
       "      <td>25</td>\n",
       "      <td>78.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>8.277778</td>\n",
       "      <td>3.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c1(c(cc(cc1)[N+](=O)[O-])[N+](=O)[O-])O</td>\n",
       "      <td>Hepatotoxicity</td>\n",
       "      <td>Mol2</td>\n",
       "      <td>9.618017</td>\n",
       "      <td>9.150803</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.158804</td>\n",
       "      <td>2.337747</td>\n",
       "      <td>4.675495</td>\n",
       "      <td>...</td>\n",
       "      <td>9.300821</td>\n",
       "      <td>43.952649</td>\n",
       "      <td>184.012021</td>\n",
       "      <td>10.824237</td>\n",
       "      <td>240</td>\n",
       "      <td>18</td>\n",
       "      <td>62.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>6.305556</td>\n",
       "      <td>2.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O(CCO)CC</td>\n",
       "      <td>Hepatotoxicity</td>\n",
       "      <td>Mol3</td>\n",
       "      <td>3.535534</td>\n",
       "      <td>3.869735</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.987918</td>\n",
       "      <td>1.801938</td>\n",
       "      <td>3.603875</td>\n",
       "      <td>...</td>\n",
       "      <td>6.608001</td>\n",
       "      <td>28.105124</td>\n",
       "      <td>90.068080</td>\n",
       "      <td>5.629255</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oc1cc2c(cc1)cccc2</td>\n",
       "      <td>Hepatotoxicity</td>\n",
       "      <td>Mol4</td>\n",
       "      <td>8.554231</td>\n",
       "      <td>7.309128</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14.426921</td>\n",
       "      <td>2.333244</td>\n",
       "      <td>4.666488</td>\n",
       "      <td>...</td>\n",
       "      <td>9.225721</td>\n",
       "      <td>41.498380</td>\n",
       "      <td>144.057515</td>\n",
       "      <td>7.581974</td>\n",
       "      <td>144</td>\n",
       "      <td>14</td>\n",
       "      <td>56.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>3.083333</td>\n",
       "      <td>2.444444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1616 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Smiles           Liver Mol_ID  \\\n",
       "0                              S=C=Nc1c2c(ccc1)cccc2  Hepatotoxicity   Mol0   \n",
       "1  c1(c(cc(cc1[N+](=O)[O-])[N+](=O)[O-])[N+](=O)[...  Hepatotoxicity   Mol1   \n",
       "2            c1(c(cc(cc1)[N+](=O)[O-])[N+](=O)[O-])O  Hepatotoxicity   Mol2   \n",
       "3                                           O(CCO)CC  Hepatotoxicity   Mol3   \n",
       "4                                  Oc1cc2c(cc1)cccc2  Hepatotoxicity   Mol4   \n",
       "\n",
       "         ABC      ABCGG  nAcid  nBase    SpAbs_A   SpMax_A  SpDiam_A  ...  \\\n",
       "0   9.818615   8.499990      0      0  17.130940  2.368752  4.737504  ...   \n",
       "1  11.877237  11.411786      0      0  18.380381  2.411142  4.822284  ...   \n",
       "2   9.618017   9.150803      0      0  15.158804  2.337747  4.675495  ...   \n",
       "3   3.535534   3.869735      0      0   6.987918  1.801938  3.603875  ...   \n",
       "4   8.554231   7.309128      0      0  14.426921  2.333244  4.666488  ...   \n",
       "\n",
       "      SRW10     TSRW10          MW        AMW  WPath  WPol  Zagreb1  Zagreb2  \\\n",
       "0  9.382527  44.188427  185.029920   9.251496    236    18     64.0     74.0   \n",
       "1  9.637763  48.280750  227.017835  10.810373    408    25     78.0     90.0   \n",
       "2  9.300821  43.952649  184.012021  10.824237    240    18     62.0     70.0   \n",
       "3  6.608001  28.105124   90.068080   5.629255     35     3     18.0     16.0   \n",
       "4  9.225721  41.498380  144.057515   7.581974    144    14     56.0     64.0   \n",
       "\n",
       "   mZagreb1  mZagreb2  \n",
       "0  3.583333  3.055556  \n",
       "1  8.277778  3.555556  \n",
       "2  6.305556  2.916667  \n",
       "3  3.000000  1.750000  \n",
       "4  3.083333  2.444444  \n",
       "\n",
       "[5 rows x 1616 columns]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = merged_df_with_drops.drop(columns=['Unnamed: 0'])\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1241, 1542)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAXdssS</th>\n",
       "      <td>1229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MINdssS</th>\n",
       "      <td>1229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAXtCH</th>\n",
       "      <td>1230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MINtCH</th>\n",
       "      <td>1230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAXsBr</th>\n",
       "      <td>1230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MINsBr</th>\n",
       "      <td>1230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MINdS</th>\n",
       "      <td>1231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAXdS</th>\n",
       "      <td>1231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAXsSH</th>\n",
       "      <td>1236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MINsSH</th>\n",
       "      <td>1236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MINsssB</th>\n",
       "      <td>1239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAXsssB</th>\n",
       "      <td>1239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAXddC</th>\n",
       "      <td>1240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MINsssP</th>\n",
       "      <td>1240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAXsssNH</th>\n",
       "      <td>1240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MINddC</th>\n",
       "      <td>1240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAXsssP</th>\n",
       "      <td>1240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MINsssNH</th>\n",
       "      <td>1240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0\n",
       "MAXdssS   1229\n",
       "MINdssS   1229\n",
       "MAXtCH    1230\n",
       "MINtCH    1230\n",
       "MAXsBr    1230\n",
       "MINsBr    1230\n",
       "MINdS     1231\n",
       "MAXdS     1231\n",
       "MAXsSH    1236\n",
       "MINsSH    1236\n",
       "MINsssB   1239\n",
       "MAXsssB   1239\n",
       "MAXddC    1240\n",
       "MINsssP   1240\n",
       "MAXsssNH  1240\n",
       "MINddC    1240\n",
       "MAXsssP   1240\n",
       "MINsssNH  1240"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#merged_liu_df_deprec = pd.concat([merged_df_with_drops, liu_df], axis=1)\n",
    "# print(df1.isna().sum().sort_values())\n",
    "# dropping ALL NULLS\n",
    "df1 = df1.dropna(axis=1, how='all')\n",
    "print(df1.shape)\n",
    "nulldf1 = pd.DataFrame(df1.isna().sum().sort_values())\n",
    "nulldf1[nulldf1[0] > .99*1241]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AATS0Z',\n",
       " 'AATS0are',\n",
       " 'AATS0d',\n",
       " 'AATS0dv',\n",
       " 'AATS0i',\n",
       " 'AATS0m',\n",
       " 'AATS0p',\n",
       " 'AATS0pe',\n",
       " 'AATS0s',\n",
       " 'AATS0se',\n",
       " 'AATS0v',\n",
       " 'AATS1Z',\n",
       " 'AATS1are',\n",
       " 'AATS1d',\n",
       " 'AATS1dv',\n",
       " 'AATS1i',\n",
       " 'AATS1m',\n",
       " 'AATS1p',\n",
       " 'AATS1pe',\n",
       " 'AATS1s',\n",
       " 'AATS1se',\n",
       " 'AATS1v',\n",
       " 'AATS2Z',\n",
       " 'AATS2are',\n",
       " 'AATS2d',\n",
       " 'AATS2dv',\n",
       " 'AATS2i',\n",
       " 'AATS2m',\n",
       " 'AATS2p',\n",
       " 'AATS2pe',\n",
       " 'AATS2s',\n",
       " 'AATS2se',\n",
       " 'AATS2v',\n",
       " 'AATS3Z',\n",
       " 'AATS3are',\n",
       " 'AATS3d',\n",
       " 'AATS3dv',\n",
       " 'AATS3i',\n",
       " 'AATS3m',\n",
       " 'AATS3p',\n",
       " 'AATS3pe',\n",
       " 'AATS3s',\n",
       " 'AATS3se',\n",
       " 'AATS3v',\n",
       " 'AATS4Z',\n",
       " 'AATS4are',\n",
       " 'AATS4d',\n",
       " 'AATS4dv',\n",
       " 'AATS4i',\n",
       " 'AATS4m',\n",
       " 'AATS4p',\n",
       " 'AATS4pe',\n",
       " 'AATS4s',\n",
       " 'AATS4se',\n",
       " 'AATS4v',\n",
       " 'AATS5Z',\n",
       " 'AATS5are',\n",
       " 'AATS5d',\n",
       " 'AATS5dv',\n",
       " 'AATS5i',\n",
       " 'AATS5m',\n",
       " 'AATS5p',\n",
       " 'AATS5pe',\n",
       " 'AATS5s',\n",
       " 'AATS5se',\n",
       " 'AATS5v',\n",
       " 'AATS6Z',\n",
       " 'AATS6are',\n",
       " 'AATS6d',\n",
       " 'AATS6dv',\n",
       " 'AATS6i',\n",
       " 'AATS6m',\n",
       " 'AATS6p',\n",
       " 'AATS6pe',\n",
       " 'AATS6s',\n",
       " 'AATS6se',\n",
       " 'AATS6v',\n",
       " 'AATS7Z',\n",
       " 'AATS7are',\n",
       " 'AATS7d',\n",
       " 'AATS7dv',\n",
       " 'AATS7i',\n",
       " 'AATS7m',\n",
       " 'AATS7p',\n",
       " 'AATS7pe',\n",
       " 'AATS7s',\n",
       " 'AATS7se',\n",
       " 'AATS7v',\n",
       " 'AATS8Z',\n",
       " 'AATS8are',\n",
       " 'AATS8d',\n",
       " 'AATS8dv',\n",
       " 'AATS8i',\n",
       " 'AATS8m',\n",
       " 'AATS8p',\n",
       " 'AATS8pe',\n",
       " 'AATS8s',\n",
       " 'AATS8se',\n",
       " 'AATS8v',\n",
       " 'AATSC0Z',\n",
       " 'AATSC0are',\n",
       " 'AATSC0c',\n",
       " 'AATSC0d',\n",
       " 'AATSC0dv',\n",
       " 'AATSC0i',\n",
       " 'AATSC0m',\n",
       " 'AATSC0p',\n",
       " 'AATSC0pe',\n",
       " 'AATSC0s',\n",
       " 'AATSC0se',\n",
       " 'AATSC0v',\n",
       " 'AATSC1Z',\n",
       " 'AATSC1are',\n",
       " 'AATSC1c',\n",
       " 'AATSC1d',\n",
       " 'AATSC1dv',\n",
       " 'AATSC1i',\n",
       " 'AATSC1m',\n",
       " 'AATSC1p',\n",
       " 'AATSC1pe',\n",
       " 'AATSC1s',\n",
       " 'AATSC1se',\n",
       " 'AATSC1v',\n",
       " 'AATSC2Z',\n",
       " 'AATSC2are',\n",
       " 'AATSC2c',\n",
       " 'AATSC2d',\n",
       " 'AATSC2dv',\n",
       " 'AATSC2i',\n",
       " 'AATSC2m',\n",
       " 'AATSC2p',\n",
       " 'AATSC2pe',\n",
       " 'AATSC2s',\n",
       " 'AATSC2se',\n",
       " 'AATSC2v',\n",
       " 'AATSC3Z',\n",
       " 'AATSC3are',\n",
       " 'AATSC3c',\n",
       " 'AATSC3d',\n",
       " 'AATSC3dv',\n",
       " 'AATSC3i',\n",
       " 'AATSC3m',\n",
       " 'AATSC3p',\n",
       " 'AATSC3pe',\n",
       " 'AATSC3s',\n",
       " 'AATSC3se',\n",
       " 'AATSC3v',\n",
       " 'AATSC4Z',\n",
       " 'AATSC4are',\n",
       " 'AATSC4c',\n",
       " 'AATSC4d',\n",
       " 'AATSC4dv',\n",
       " 'AATSC4i',\n",
       " 'AATSC4m',\n",
       " 'AATSC4p',\n",
       " 'AATSC4pe',\n",
       " 'AATSC4s',\n",
       " 'AATSC4se',\n",
       " 'AATSC4v',\n",
       " 'AATSC5Z',\n",
       " 'AATSC5are',\n",
       " 'AATSC5c',\n",
       " 'AATSC5d',\n",
       " 'AATSC5dv',\n",
       " 'AATSC5i',\n",
       " 'AATSC5m',\n",
       " 'AATSC5p',\n",
       " 'AATSC5pe',\n",
       " 'AATSC5s',\n",
       " 'AATSC5se',\n",
       " 'AATSC5v',\n",
       " 'AATSC6Z',\n",
       " 'AATSC6are',\n",
       " 'AATSC6c',\n",
       " 'AATSC6d',\n",
       " 'AATSC6dv',\n",
       " 'AATSC6i',\n",
       " 'AATSC6m',\n",
       " 'AATSC6p',\n",
       " 'AATSC6pe',\n",
       " 'AATSC6s',\n",
       " 'AATSC6se',\n",
       " 'AATSC6v',\n",
       " 'AATSC7Z',\n",
       " 'AATSC7are',\n",
       " 'AATSC7c',\n",
       " 'AATSC7d',\n",
       " 'AATSC7dv',\n",
       " 'AATSC7i',\n",
       " 'AATSC7m',\n",
       " 'AATSC7p',\n",
       " 'AATSC7pe',\n",
       " 'AATSC7s',\n",
       " 'AATSC7se',\n",
       " 'AATSC7v',\n",
       " 'AATSC8Z',\n",
       " 'AATSC8are',\n",
       " 'AATSC8c',\n",
       " 'AATSC8d',\n",
       " 'AATSC8dv',\n",
       " 'AATSC8i',\n",
       " 'AATSC8m',\n",
       " 'AATSC8p',\n",
       " 'AATSC8pe',\n",
       " 'AATSC8s',\n",
       " 'AATSC8se',\n",
       " 'AATSC8v',\n",
       " 'ABC',\n",
       " 'ABCGG',\n",
       " 'ATS0Z',\n",
       " 'ATS0are',\n",
       " 'ATS0d',\n",
       " 'ATS0dv',\n",
       " 'ATS0i',\n",
       " 'ATS0m',\n",
       " 'ATS0p',\n",
       " 'ATS0pe',\n",
       " 'ATS0s',\n",
       " 'ATS0se',\n",
       " 'ATS0v',\n",
       " 'ATS1Z',\n",
       " 'ATS1are',\n",
       " 'ATS1d',\n",
       " 'ATS1dv',\n",
       " 'ATS1i',\n",
       " 'ATS1m',\n",
       " 'ATS1p',\n",
       " 'ATS1pe',\n",
       " 'ATS1s',\n",
       " 'ATS1se',\n",
       " 'ATS1v',\n",
       " 'ATS2Z',\n",
       " 'ATS2are',\n",
       " 'ATS2d',\n",
       " 'ATS2dv',\n",
       " 'ATS2i',\n",
       " 'ATS2m',\n",
       " 'ATS2p',\n",
       " 'ATS2pe',\n",
       " 'ATS2s',\n",
       " 'ATS2se',\n",
       " 'ATS2v',\n",
       " 'ATS3Z',\n",
       " 'ATS3are',\n",
       " 'ATS3d',\n",
       " 'ATS3dv',\n",
       " 'ATS3i',\n",
       " 'ATS3m',\n",
       " 'ATS3p',\n",
       " 'ATS3pe',\n",
       " 'ATS3s',\n",
       " 'ATS3se',\n",
       " 'ATS3v',\n",
       " 'ATS4Z',\n",
       " 'ATS4are',\n",
       " 'ATS4d',\n",
       " 'ATS4dv',\n",
       " 'ATS4i',\n",
       " 'ATS4m',\n",
       " 'ATS4p',\n",
       " 'ATS4pe',\n",
       " 'ATS4s',\n",
       " 'ATS4se',\n",
       " 'ATS4v',\n",
       " 'ATS5Z',\n",
       " 'ATS5are',\n",
       " 'ATS5d',\n",
       " 'ATS5dv',\n",
       " 'ATS5i',\n",
       " 'ATS5m',\n",
       " 'ATS5p',\n",
       " 'ATS5pe',\n",
       " 'ATS5s',\n",
       " 'ATS5se',\n",
       " 'ATS5v',\n",
       " 'ATS6Z',\n",
       " 'ATS6are',\n",
       " 'ATS6d',\n",
       " 'ATS6dv',\n",
       " 'ATS6i',\n",
       " 'ATS6m',\n",
       " 'ATS6p',\n",
       " 'ATS6pe',\n",
       " 'ATS6s',\n",
       " 'ATS6se',\n",
       " 'ATS6v',\n",
       " 'ATS7Z',\n",
       " 'ATS7are',\n",
       " 'ATS7d',\n",
       " 'ATS7dv',\n",
       " 'ATS7i',\n",
       " 'ATS7m',\n",
       " 'ATS7p',\n",
       " 'ATS7pe',\n",
       " 'ATS7s',\n",
       " 'ATS7se',\n",
       " 'ATS7v',\n",
       " 'ATS8Z',\n",
       " 'ATS8are',\n",
       " 'ATS8d',\n",
       " 'ATS8dv',\n",
       " 'ATS8i',\n",
       " 'ATS8m',\n",
       " 'ATS8p',\n",
       " 'ATS8pe',\n",
       " 'ATS8s',\n",
       " 'ATS8se',\n",
       " 'ATS8v',\n",
       " 'ATSC0Z',\n",
       " 'ATSC0are',\n",
       " 'ATSC0c',\n",
       " 'ATSC0d',\n",
       " 'ATSC0dv',\n",
       " 'ATSC0i',\n",
       " 'ATSC0m',\n",
       " 'ATSC0p',\n",
       " 'ATSC0pe',\n",
       " 'ATSC0s',\n",
       " 'ATSC0se',\n",
       " 'ATSC0v',\n",
       " 'ATSC1Z',\n",
       " 'ATSC1are',\n",
       " 'ATSC1c',\n",
       " 'ATSC1d',\n",
       " 'ATSC1dv',\n",
       " 'ATSC1i',\n",
       " 'ATSC1m',\n",
       " 'ATSC1p',\n",
       " 'ATSC1pe',\n",
       " 'ATSC1s',\n",
       " 'ATSC1se',\n",
       " 'ATSC1v',\n",
       " 'ATSC2Z',\n",
       " 'ATSC2are',\n",
       " 'ATSC2c',\n",
       " 'ATSC2d',\n",
       " 'ATSC2dv',\n",
       " 'ATSC2i',\n",
       " 'ATSC2m',\n",
       " 'ATSC2p',\n",
       " 'ATSC2pe',\n",
       " 'ATSC2s',\n",
       " 'ATSC2se',\n",
       " 'ATSC2v',\n",
       " 'ATSC3Z',\n",
       " 'ATSC3are',\n",
       " 'ATSC3c',\n",
       " 'ATSC3d',\n",
       " 'ATSC3dv',\n",
       " 'ATSC3i',\n",
       " 'ATSC3m',\n",
       " 'ATSC3p',\n",
       " 'ATSC3pe',\n",
       " 'ATSC3s',\n",
       " 'ATSC3se',\n",
       " 'ATSC3v',\n",
       " 'ATSC4Z',\n",
       " 'ATSC4are',\n",
       " 'ATSC4c',\n",
       " 'ATSC4d',\n",
       " 'ATSC4dv',\n",
       " 'ATSC4i',\n",
       " 'ATSC4m',\n",
       " 'ATSC4p',\n",
       " 'ATSC4pe',\n",
       " 'ATSC4s',\n",
       " 'ATSC4se',\n",
       " 'ATSC4v',\n",
       " 'ATSC5Z',\n",
       " 'ATSC5are',\n",
       " 'ATSC5c',\n",
       " 'ATSC5d',\n",
       " 'ATSC5dv',\n",
       " 'ATSC5i',\n",
       " 'ATSC5m',\n",
       " 'ATSC5p',\n",
       " 'ATSC5pe',\n",
       " 'ATSC5s',\n",
       " 'ATSC5se',\n",
       " 'ATSC5v',\n",
       " 'ATSC6Z',\n",
       " 'ATSC6are',\n",
       " 'ATSC6c',\n",
       " 'ATSC6d',\n",
       " 'ATSC6dv',\n",
       " 'ATSC6i',\n",
       " 'ATSC6m',\n",
       " 'ATSC6p',\n",
       " 'ATSC6pe',\n",
       " 'ATSC6s',\n",
       " 'ATSC6se',\n",
       " 'ATSC6v',\n",
       " 'ATSC7Z',\n",
       " 'ATSC7are',\n",
       " 'ATSC7c',\n",
       " 'ATSC7d',\n",
       " 'ATSC7dv',\n",
       " 'ATSC7i',\n",
       " 'ATSC7m',\n",
       " 'ATSC7p',\n",
       " 'ATSC7pe',\n",
       " 'ATSC7s',\n",
       " 'ATSC7se',\n",
       " 'ATSC7v',\n",
       " 'ATSC8Z',\n",
       " 'ATSC8are',\n",
       " 'ATSC8c',\n",
       " 'ATSC8d',\n",
       " 'ATSC8dv',\n",
       " 'ATSC8i',\n",
       " 'ATSC8m',\n",
       " 'ATSC8p',\n",
       " 'ATSC8pe',\n",
       " 'ATSC8s',\n",
       " 'ATSC8se',\n",
       " 'ATSC8v',\n",
       " 'AXp-0d',\n",
       " 'AXp-0dv',\n",
       " 'AXp-1d',\n",
       " 'AXp-1dv',\n",
       " 'AXp-2d',\n",
       " 'AXp-2dv',\n",
       " 'AXp-3d',\n",
       " 'AXp-3dv',\n",
       " 'AXp-4d',\n",
       " 'AXp-4dv',\n",
       " 'AXp-5d',\n",
       " 'AXp-5dv',\n",
       " 'AXp-6d',\n",
       " 'AXp-6dv',\n",
       " 'AXp-7d',\n",
       " 'AXp-7dv',\n",
       " 'BCUTZ-1h',\n",
       " 'BCUTZ-1l',\n",
       " 'BCUTare-1h',\n",
       " 'BCUTare-1l',\n",
       " 'BCUTc-1h',\n",
       " 'BCUTc-1l',\n",
       " 'BCUTd-1h',\n",
       " 'BCUTd-1l',\n",
       " 'BCUTdv-1h',\n",
       " 'BCUTdv-1l',\n",
       " 'BCUTi-1h',\n",
       " 'BCUTi-1l',\n",
       " 'BCUTm-1h',\n",
       " 'BCUTm-1l',\n",
       " 'BCUTp-1h',\n",
       " 'BCUTp-1l',\n",
       " 'BCUTpe-1h',\n",
       " 'BCUTpe-1l',\n",
       " 'BCUTs-1h',\n",
       " 'BCUTs-1l',\n",
       " 'BCUTse-1h',\n",
       " 'BCUTse-1l',\n",
       " 'BCUTv-1h',\n",
       " 'BCUTv-1l',\n",
       " 'BalabanJ',\n",
       " 'BertzCT',\n",
       " 'C1SP1',\n",
       " 'C1SP2',\n",
       " 'C1SP3',\n",
       " 'C2SP1',\n",
       " 'C2SP2',\n",
       " 'C2SP3',\n",
       " 'C3SP2',\n",
       " 'C3SP3',\n",
       " 'C4SP3',\n",
       " 'DetourIndex',\n",
       " 'FCSP3',\n",
       " 'GATS1Z',\n",
       " 'GATS1are',\n",
       " 'GATS1c',\n",
       " 'GATS1d',\n",
       " 'GATS1dv',\n",
       " 'GATS1i',\n",
       " 'GATS1m',\n",
       " 'GATS1p',\n",
       " 'GATS1pe',\n",
       " 'GATS1s',\n",
       " 'GATS1se',\n",
       " 'GATS1v',\n",
       " 'GATS2Z',\n",
       " 'GATS2are',\n",
       " 'GATS2c',\n",
       " 'GATS2d',\n",
       " 'GATS2dv',\n",
       " 'GATS2i',\n",
       " 'GATS2m',\n",
       " 'GATS2p',\n",
       " 'GATS2pe',\n",
       " 'GATS2s',\n",
       " 'GATS2se',\n",
       " 'GATS2v',\n",
       " 'GATS3Z',\n",
       " 'GATS3are',\n",
       " 'GATS3c',\n",
       " 'GATS3d',\n",
       " 'GATS3dv',\n",
       " 'GATS3i',\n",
       " 'GATS3m',\n",
       " 'GATS3p',\n",
       " 'GATS3pe',\n",
       " 'GATS3s',\n",
       " 'GATS3se',\n",
       " 'GATS3v',\n",
       " 'GATS4Z',\n",
       " 'GATS4are',\n",
       " 'GATS4c',\n",
       " 'GATS4d',\n",
       " 'GATS4dv',\n",
       " 'GATS4i',\n",
       " 'GATS4m',\n",
       " 'GATS4p',\n",
       " 'GATS4pe',\n",
       " 'GATS4s',\n",
       " 'GATS4se',\n",
       " 'GATS4v',\n",
       " 'GATS5Z',\n",
       " 'GATS5are',\n",
       " 'GATS5c',\n",
       " 'GATS5d',\n",
       " 'GATS5dv',\n",
       " 'GATS5i',\n",
       " 'GATS5m',\n",
       " 'GATS5p',\n",
       " 'GATS5pe',\n",
       " 'GATS5s',\n",
       " 'GATS5se',\n",
       " 'GATS5v',\n",
       " 'GATS6Z',\n",
       " 'GATS6are',\n",
       " 'GATS6c',\n",
       " 'GATS6d',\n",
       " 'GATS6dv',\n",
       " 'GATS6i',\n",
       " 'GATS6m',\n",
       " 'GATS6p',\n",
       " 'GATS6pe',\n",
       " 'GATS6s',\n",
       " 'GATS6se',\n",
       " 'GATS6v',\n",
       " 'GATS7Z',\n",
       " 'GATS7are',\n",
       " 'GATS7c',\n",
       " 'GATS7d',\n",
       " 'GATS7dv',\n",
       " 'GATS7i',\n",
       " 'GATS7m',\n",
       " 'GATS7p',\n",
       " 'GATS7pe',\n",
       " 'GATS7s',\n",
       " 'GATS7se',\n",
       " 'GATS7v',\n",
       " 'GATS8Z',\n",
       " 'GATS8are',\n",
       " 'GATS8c',\n",
       " 'GATS8d',\n",
       " 'GATS8dv',\n",
       " 'GATS8i',\n",
       " 'GATS8m',\n",
       " 'GATS8p',\n",
       " 'GATS8pe',\n",
       " 'GATS8s',\n",
       " 'GATS8se',\n",
       " 'GATS8v',\n",
       " 'HybRatio',\n",
       " 'LogEE_A',\n",
       " 'LogEE_D',\n",
       " 'LogEE_Dt',\n",
       " 'LogEE_DzZ',\n",
       " 'LogEE_Dzare',\n",
       " 'LogEE_Dzi',\n",
       " 'LogEE_Dzm',\n",
       " 'LogEE_Dzp',\n",
       " 'LogEE_Dzpe',\n",
       " 'LogEE_Dzse',\n",
       " 'LogEE_Dzv',\n",
       " 'MATS1Z',\n",
       " 'MATS1are',\n",
       " 'MATS1c',\n",
       " 'MATS1d',\n",
       " 'MATS1dv',\n",
       " 'MATS1i',\n",
       " 'MATS1m',\n",
       " 'MATS1p',\n",
       " 'MATS1pe',\n",
       " 'MATS1s',\n",
       " 'MATS1se',\n",
       " 'MATS1v',\n",
       " 'MATS2Z',\n",
       " 'MATS2are',\n",
       " 'MATS2c',\n",
       " 'MATS2d',\n",
       " 'MATS2dv',\n",
       " 'MATS2i',\n",
       " 'MATS2m',\n",
       " 'MATS2p',\n",
       " 'MATS2pe',\n",
       " 'MATS2s',\n",
       " 'MATS2se',\n",
       " 'MATS2v',\n",
       " 'MATS3Z',\n",
       " 'MATS3are',\n",
       " 'MATS3c',\n",
       " 'MATS3d',\n",
       " 'MATS3dv',\n",
       " 'MATS3i',\n",
       " 'MATS3m',\n",
       " 'MATS3p',\n",
       " 'MATS3pe',\n",
       " 'MATS3s',\n",
       " 'MATS3se',\n",
       " 'MATS3v',\n",
       " 'MATS4Z',\n",
       " 'MATS4are',\n",
       " 'MATS4c',\n",
       " 'MATS4d',\n",
       " 'MATS4dv',\n",
       " 'MATS4i',\n",
       " 'MATS4m',\n",
       " 'MATS4p',\n",
       " 'MATS4pe',\n",
       " 'MATS4s',\n",
       " 'MATS4se',\n",
       " 'MATS4v',\n",
       " 'MATS5Z',\n",
       " 'MATS5are',\n",
       " 'MATS5c',\n",
       " 'MATS5d',\n",
       " 'MATS5dv',\n",
       " 'MATS5i',\n",
       " 'MATS5m',\n",
       " 'MATS5p',\n",
       " 'MATS5pe',\n",
       " 'MATS5s',\n",
       " 'MATS5se',\n",
       " 'MATS5v',\n",
       " 'MATS6Z',\n",
       " 'MATS6are',\n",
       " 'MATS6c',\n",
       " 'MATS6d',\n",
       " 'MATS6dv',\n",
       " 'MATS6i',\n",
       " 'MATS6m',\n",
       " 'MATS6p',\n",
       " 'MATS6pe',\n",
       " 'MATS6s',\n",
       " 'MATS6se',\n",
       " 'MATS6v',\n",
       " 'MATS7Z',\n",
       " 'MATS7are',\n",
       " 'MATS7c',\n",
       " 'MATS7d',\n",
       " 'MATS7dv',\n",
       " 'MATS7i',\n",
       " 'MATS7m',\n",
       " 'MATS7p',\n",
       " 'MATS7pe',\n",
       " 'MATS7s',\n",
       " 'MATS7se',\n",
       " 'MATS7v',\n",
       " 'MATS8Z',\n",
       " 'MATS8are',\n",
       " 'MATS8c',\n",
       " 'MATS8d',\n",
       " 'MATS8dv',\n",
       " 'MATS8i',\n",
       " 'MATS8m',\n",
       " 'MATS8p',\n",
       " 'MATS8pe',\n",
       " 'MATS8s',\n",
       " 'MATS8se',\n",
       " 'MATS8v',\n",
       " 'MZ',\n",
       " 'Mare',\n",
       " 'Mi',\n",
       " 'Mm',\n",
       " 'Mp',\n",
       " 'Mpe',\n",
       " 'Mse',\n",
       " 'Mv',\n",
       " 'NaaCH',\n",
       " 'NaaN',\n",
       " 'NaaNH',\n",
       " 'NaaO',\n",
       " 'NaaS',\n",
       " 'NaaSe',\n",
       " 'NaaaC',\n",
       " 'NaasC',\n",
       " 'NaasN',\n",
       " 'NdCH2',\n",
       " 'NdNH',\n",
       " 'NdO',\n",
       " 'NdS',\n",
       " 'NdSe',\n",
       " 'NddC',\n",
       " 'NddsN',\n",
       " 'NddssS',\n",
       " 'NddssSe',\n",
       " 'NdsCH',\n",
       " 'NdsN',\n",
       " 'NdssC',\n",
       " 'NdssS',\n",
       " 'NdssSe',\n",
       " 'NdsssP',\n",
       " 'NsAsH2',\n",
       " 'NsBr',\n",
       " 'NsCH3',\n",
       " 'NsCl',\n",
       " 'NsF',\n",
       " 'NsGeH3',\n",
       " 'NsI',\n",
       " 'NsLi',\n",
       " 'NsNH2',\n",
       " 'NsNH3',\n",
       " 'NsOH',\n",
       " 'NsPH2',\n",
       " 'NsPbH3',\n",
       " 'NsSH',\n",
       " 'NsSeH',\n",
       " 'NsSiH3',\n",
       " 'NsSnH3',\n",
       " 'NssAsH',\n",
       " 'NssBH',\n",
       " 'NssBe',\n",
       " 'NssCH2',\n",
       " 'NssGeH2',\n",
       " 'NssNH',\n",
       " 'NssNH2',\n",
       " 'NssO',\n",
       " 'NssPH',\n",
       " 'NssPbH2',\n",
       " 'NssS',\n",
       " 'NssSe',\n",
       " 'NssSiH2',\n",
       " 'NssSnH2',\n",
       " 'NsssAs',\n",
       " 'NsssB',\n",
       " 'NsssCH',\n",
       " 'NsssGeH',\n",
       " 'NsssN',\n",
       " 'NsssNH',\n",
       " 'NsssP',\n",
       " 'NsssPbH',\n",
       " 'NsssSiH',\n",
       " 'NsssSnH',\n",
       " 'NsssdAs',\n",
       " 'NssssB',\n",
       " 'NssssBe',\n",
       " 'NssssC',\n",
       " 'NssssGe',\n",
       " 'NssssN',\n",
       " 'NssssPb',\n",
       " 'NssssSi',\n",
       " 'NssssSn',\n",
       " 'NsssssAs',\n",
       " 'NsssssP',\n",
       " 'NtCH',\n",
       " 'NtN',\n",
       " 'NtsC',\n",
       " 'RNCG',\n",
       " 'RPCG',\n",
       " 'SM1_Dt',\n",
       " 'SM1_DzZ',\n",
       " 'SM1_Dzare',\n",
       " 'SM1_Dzi',\n",
       " 'SM1_Dzm',\n",
       " 'SM1_Dzp',\n",
       " 'SM1_Dzpe',\n",
       " 'SM1_Dzse',\n",
       " 'SM1_Dzv',\n",
       " 'SZ',\n",
       " 'SaaCH',\n",
       " 'SaaN',\n",
       " 'SaaNH',\n",
       " 'SaaaC',\n",
       " 'SaasC',\n",
       " 'Sare',\n",
       " 'SdCH2',\n",
       " 'SdNH',\n",
       " 'SddC',\n",
       " 'SdsCH',\n",
       " 'SdsN',\n",
       " 'SdssC',\n",
       " 'Si',\n",
       " 'Sm',\n",
       " 'Sp',\n",
       " 'SpAD_A',\n",
       " 'SpAD_D',\n",
       " 'SpAD_Dt',\n",
       " 'SpAD_DzZ',\n",
       " 'SpAD_Dzare',\n",
       " 'SpAD_Dzi',\n",
       " 'SpAD_Dzm',\n",
       " 'SpAD_Dzp',\n",
       " 'SpAD_Dzpe',\n",
       " 'SpAD_Dzse',\n",
       " 'SpAD_Dzv',\n",
       " 'SpAbs_A',\n",
       " 'SpAbs_D',\n",
       " 'SpAbs_Dt',\n",
       " 'SpAbs_DzZ',\n",
       " 'SpAbs_Dzare',\n",
       " 'SpAbs_Dzi',\n",
       " 'SpAbs_Dzm',\n",
       " 'SpAbs_Dzp',\n",
       " 'SpAbs_Dzpe',\n",
       " 'SpAbs_Dzse',\n",
       " 'SpAbs_Dzv',\n",
       " 'SpDiam_A',\n",
       " 'SpDiam_D',\n",
       " 'SpDiam_Dt',\n",
       " 'SpDiam_DzZ',\n",
       " 'SpDiam_Dzare',\n",
       " 'SpDiam_Dzi',\n",
       " 'SpDiam_Dzm',\n",
       " 'SpDiam_Dzp',\n",
       " 'SpDiam_Dzpe',\n",
       " 'SpDiam_Dzse',\n",
       " 'SpDiam_Dzv',\n",
       " 'SpMAD_A',\n",
       " 'SpMAD_D',\n",
       " 'SpMAD_Dt',\n",
       " 'SpMAD_DzZ',\n",
       " 'SpMAD_Dzare',\n",
       " 'SpMAD_Dzi',\n",
       " 'SpMAD_Dzm',\n",
       " 'SpMAD_Dzp',\n",
       " 'SpMAD_Dzpe',\n",
       " 'SpMAD_Dzse',\n",
       " 'SpMAD_Dzv',\n",
       " 'SpMax_A',\n",
       " 'SpMax_D',\n",
       " 'SpMax_Dt',\n",
       " 'SpMax_DzZ',\n",
       " 'SpMax_Dzare',\n",
       " 'SpMax_Dzi',\n",
       " 'SpMax_Dzm',\n",
       " 'SpMax_Dzp',\n",
       " 'SpMax_Dzpe',\n",
       " 'SpMax_Dzse',\n",
       " 'SpMax_Dzv',\n",
       " 'Spe',\n",
       " 'SsCH3',\n",
       " 'SsLi',\n",
       " 'SsNH2',\n",
       " 'SsNH3',\n",
       " 'Sse',\n",
       " 'SssBH',\n",
       " 'SssBe',\n",
       " 'SssCH2',\n",
       " 'SssNH',\n",
       " 'SssNH2',\n",
       " 'SsssB',\n",
       " 'SsssCH',\n",
       " 'SsssN',\n",
       " 'SsssNH',\n",
       " 'SssssB',\n",
       " 'SssssBe',\n",
       " 'SssssC',\n",
       " 'StCH',\n",
       " 'StN',\n",
       " 'StsC',\n",
       " 'Sv',\n",
       " 'VE1_A',\n",
       " 'VE1_D',\n",
       " 'VE1_Dt',\n",
       " 'VE1_DzZ',\n",
       " 'VE1_Dzare',\n",
       " 'VE1_Dzi',\n",
       " 'VE1_Dzm',\n",
       " 'VE1_Dzp',\n",
       " 'VE1_Dzpe',\n",
       " 'VE1_Dzse',\n",
       " 'VE1_Dzv',\n",
       " 'VE2_A',\n",
       " 'VE2_D',\n",
       " 'VE2_Dt',\n",
       " 'VE2_DzZ',\n",
       " 'VE2_Dzare',\n",
       " 'VE2_Dzi',\n",
       " 'VE2_Dzm',\n",
       " 'VE2_Dzp',\n",
       " 'VE2_Dzpe',\n",
       " 'VE2_Dzse',\n",
       " 'VE2_Dzv',\n",
       " 'VE3_A',\n",
       " 'VE3_D',\n",
       " 'VE3_Dt',\n",
       " 'VE3_DzZ',\n",
       " 'VE3_Dzare',\n",
       " 'VE3_Dzi',\n",
       " 'VE3_Dzm',\n",
       " 'VE3_Dzp',\n",
       " 'VE3_Dzpe',\n",
       " 'VE3_Dzse',\n",
       " 'VE3_Dzv',\n",
       " 'VR1_A',\n",
       " 'VR1_D',\n",
       " 'VR1_Dt',\n",
       " 'VR1_DzZ',\n",
       " 'VR1_Dzare',\n",
       " 'VR1_Dzi',\n",
       " 'VR1_Dzm',\n",
       " 'VR1_Dzp',\n",
       " 'VR1_Dzpe',\n",
       " 'VR1_Dzse',\n",
       " 'VR1_Dzv',\n",
       " 'VR2_A',\n",
       " 'VR2_D',\n",
       " 'VR2_Dt',\n",
       " 'VR2_DzZ',\n",
       " 'VR2_Dzare',\n",
       " 'VR2_Dzi',\n",
       " 'VR2_Dzm',\n",
       " 'VR2_Dzp',\n",
       " 'VR2_Dzpe',\n",
       " 'VR2_Dzse',\n",
       " 'VR2_Dzv',\n",
       " 'VR3_A',\n",
       " 'VR3_D',\n",
       " 'VR3_Dt',\n",
       " 'VR3_DzZ',\n",
       " 'VR3_Dzare',\n",
       " 'VR3_Dzi',\n",
       " 'VR3_Dzm',\n",
       " 'VR3_Dzp',\n",
       " 'VR3_Dzpe',\n",
       " 'VR3_Dzse',\n",
       " 'VR3_Dzv',\n",
       " 'Xc-3d',\n",
       " 'Xc-3dv',\n",
       " 'Xc-4d',\n",
       " 'Xc-4dv',\n",
       " 'Xc-5d',\n",
       " 'Xc-5dv',\n",
       " 'Xc-6d',\n",
       " 'Xc-6dv',\n",
       " 'Xch-3d',\n",
       " 'Xch-3dv',\n",
       " 'Xch-4d',\n",
       " 'Xch-4dv',\n",
       " 'Xch-5d',\n",
       " 'Xch-5dv',\n",
       " 'Xch-6d',\n",
       " 'Xch-6dv',\n",
       " 'Xch-7d',\n",
       " 'Xch-7dv',\n",
       " 'Xp-0d',\n",
       " 'Xp-0dv',\n",
       " 'Xp-1d',\n",
       " 'Xp-1dv',\n",
       " 'Xp-2d',\n",
       " 'Xp-2dv',\n",
       " 'Xp-3d',\n",
       " 'Xp-3dv',\n",
       " 'Xp-4d',\n",
       " 'Xp-4dv',\n",
       " 'Xp-5d',\n",
       " 'Xp-5dv',\n",
       " 'Xp-6d',\n",
       " 'Xp-6dv',\n",
       " 'Xp-7d',\n",
       " 'Xp-7dv',\n",
       " 'Xpc-4d',\n",
       " 'Xpc-4dv',\n",
       " 'Xpc-5d',\n",
       " 'Xpc-5dv',\n",
       " 'Xpc-6d',\n",
       " 'Xpc-6dv',\n",
       " 'nAcid',\n",
       " 'nAromAtom',\n",
       " 'nAromBond',\n",
       " 'nAtom',\n",
       " 'nB',\n",
       " 'nBase',\n",
       " 'nBonds',\n",
       " 'nBondsA',\n",
       " 'nBondsD',\n",
       " 'nBondsKD',\n",
       " 'nBondsKS',\n",
       " 'nBondsM',\n",
       " 'nBondsO',\n",
       " 'nBondsS',\n",
       " 'nBondsT',\n",
       " 'nBr',\n",
       " 'nBridgehead',\n",
       " 'nC',\n",
       " 'nCl',\n",
       " 'nF',\n",
       " 'nH',\n",
       " 'nHeavyAtom',\n",
       " 'nHetero',\n",
       " 'nI',\n",
       " 'nN',\n",
       " 'nO',\n",
       " 'nP',\n",
       " 'nS',\n",
       " 'nSpiro',\n",
       " 'nX']"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overlapcols1.remove('Smiles')\n",
    "overlapcols1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1179, 261)\n"
     ]
    }
   ],
   "source": [
    "liu_df = liu_df.drop(columns=overlapcols1)\n",
    "print(liu_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Smiles</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>genescore_times_floorlog10ofpvalue</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sum of Reciprical of rank</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tbili &gt;= 100%, 2) with hyperplasia at any grade, any other path at any grade</th>\n",
       "      <td>268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hypertrophy &gt;= 1.33, 2) no other finding</th>\n",
       "      <td>281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bile duct hyperplasia, 3) any other findings; any grade for necrosis, single cell necrosis, fibrosis, hypertrophy or increased mitosis</th>\n",
       "      <td>298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hematopoeisis, 2) any other finding</th>\n",
       "      <td>354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fibrosis at any grade with any other pathology at any grade</th>\n",
       "      <td>454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Necrosis, 1) no other findings</th>\n",
       "      <td>475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trigs &lt; -60%, 1) no path findings and FC &gt;-15%</th>\n",
       "      <td>572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cholesterol &gt; 40%, 1) no path findings</th>\n",
       "      <td>662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Increased mitosis, 1) no other findings</th>\n",
       "      <td>666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Increased glycogen, 2) any other finding</th>\n",
       "      <td>675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cholesterol &lt;-30%, 1) no path findings and FC &gt;-15%</th>\n",
       "      <td>700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vacuolation, 2) allowing hypertrophy at any grade</th>\n",
       "      <td>701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2m</th>\n",
       "      <td>755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6m</th>\n",
       "      <td>780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27m</th>\n",
       "      <td>888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18m</th>\n",
       "      <td>894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46m</th>\n",
       "      <td>915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3m</th>\n",
       "      <td>920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7m</th>\n",
       "      <td>957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47m</th>\n",
       "      <td>972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120m</th>\n",
       "      <td>1172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>1172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>1172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>1173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59m</th>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>1175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>1175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>1175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121m</th>\n",
       "      <td>1175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>1175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38m</th>\n",
       "      <td>1175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>1175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>1175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183m</th>\n",
       "      <td>1176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>1176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>1176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>1176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275m</th>\n",
       "      <td>1176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>1177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>1177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124m</th>\n",
       "      <td>1177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>1177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>1177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224m</th>\n",
       "      <td>1178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>1178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>1178</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>261 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       0\n",
       "Smiles                                                 0\n",
       "genescore_times_floorlog10ofpvalue                     0\n",
       "label                                                  0\n",
       "Sum of Reciprical of rank                              0\n",
       "Tbili >= 100%, 2) with hyperplasia at any grade...   268\n",
       "Hypertrophy >= 1.33, 2) no other finding             281\n",
       "Bile duct hyperplasia, 3) any other findings; a...   298\n",
       "Hematopoeisis, 2) any other finding                  354\n",
       "Fibrosis at any grade with any other pathology ...   454\n",
       "Necrosis, 1) no other findings                       475\n",
       "Trigs < -60%, 1) no path findings and FC >-15%       572\n",
       "Cholesterol > 40%, 1) no path findings               662\n",
       "Increased mitosis, 1) no other findings              666\n",
       "Increased glycogen, 2) any other finding             675\n",
       "Cholesterol <-30%, 1) no path findings and FC >...   700\n",
       "Vacuolation, 2) allowing hypertrophy at any grade    701\n",
       "2m                                                   755\n",
       "2                                                    757\n",
       "6m                                                   780\n",
       "27m                                                  888\n",
       "18m                                                  894\n",
       "46m                                                  915\n",
       "3m                                                   920\n",
       "6                                                    931\n",
       "18                                                   949\n",
       "27                                                   950\n",
       "12                                                   952\n",
       "7m                                                   957\n",
       "31                                                   965\n",
       "47m                                                  972\n",
       "...                                                  ...\n",
       "120m                                                1172\n",
       "89                                                  1172\n",
       "177                                                 1172\n",
       "80                                                  1173\n",
       "118                                                 1173\n",
       "236                                                 1174\n",
       "233                                                 1174\n",
       "218                                                 1174\n",
       "59m                                                 1174\n",
       "137                                                 1175\n",
       "71                                                  1175\n",
       "345                                                 1175\n",
       "121m                                                1175\n",
       "287                                                 1175\n",
       "38m                                                 1175\n",
       "114                                                 1175\n",
       "58                                                  1175\n",
       "183m                                                1176\n",
       "222                                                 1176\n",
       "278                                                 1176\n",
       "340                                                 1176\n",
       "275m                                                1176\n",
       "208                                                 1177\n",
       "272                                                 1177\n",
       "124m                                                1177\n",
       "181                                                 1177\n",
       "198                                                 1177\n",
       "224m                                                1178\n",
       "288                                                 1178\n",
       "312                                                 1178\n",
       "\n",
       "[261 rows x 1 columns]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame(liu_df.isna().sum().sort_values())\n",
    "# dropping ALL NULLS\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Smiles</th>\n",
       "      <th>Liver</th>\n",
       "      <th>Mol_ID</th>\n",
       "      <th>ABC</th>\n",
       "      <th>ABCGG</th>\n",
       "      <th>nAcid</th>\n",
       "      <th>nBase</th>\n",
       "      <th>SpAbs_A</th>\n",
       "      <th>SpMax_A</th>\n",
       "      <th>SpDiam_A</th>\n",
       "      <th>...</th>\n",
       "      <th>Hypertrophy &gt;= 1.33, 2) no other finding</th>\n",
       "      <th>Increased glycogen, 2) any other finding</th>\n",
       "      <th>Increased mitosis, 1) no other findings</th>\n",
       "      <th>Necrosis, 1) no other findings</th>\n",
       "      <th>Single cell necrosis, 2) allowing hypertrophy</th>\n",
       "      <th>Tbili &gt;= 100%, 2) with hyperplasia at any grade, any other path at any grade</th>\n",
       "      <th>Trigs &lt; -60%, 1) no path findings and FC &gt;-15%</th>\n",
       "      <th>Trigs &gt; 80%, 1) no path findings</th>\n",
       "      <th>Vacuolation, 2) allowing hypertrophy at any grade</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S=C=Nc1c2c(ccc1)cccc2</td>\n",
       "      <td>Hepatotoxicity</td>\n",
       "      <td>Mol0</td>\n",
       "      <td>9.818615</td>\n",
       "      <td>8.499990</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17.130940</td>\n",
       "      <td>2.368752</td>\n",
       "      <td>4.737504</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064164</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.010211</td>\n",
       "      <td>0.010211</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001686</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c1(c(cc(cc1[N+](=O)[O-])[N+](=O)[O-])[N+](=O)[...</td>\n",
       "      <td>Hepatotoxicity</td>\n",
       "      <td>Mol1</td>\n",
       "      <td>11.877237</td>\n",
       "      <td>11.411786</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18.380381</td>\n",
       "      <td>2.411142</td>\n",
       "      <td>4.822284</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010342</td>\n",
       "      <td>0.008537</td>\n",
       "      <td>0.128815</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.056213</td>\n",
       "      <td>0.137352</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c1(c(cc(cc1)[N+](=O)[O-])[N+](=O)[O-])O</td>\n",
       "      <td>Hepatotoxicity</td>\n",
       "      <td>Mol2</td>\n",
       "      <td>9.618017</td>\n",
       "      <td>9.150803</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.158804</td>\n",
       "      <td>2.337747</td>\n",
       "      <td>4.675495</td>\n",
       "      <td>...</td>\n",
       "      <td>1.470138</td>\n",
       "      <td>0.083020</td>\n",
       "      <td>0.254973</td>\n",
       "      <td>0.010825</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.311952</td>\n",
       "      <td>0.197157</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.050395</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Oc1cc2c(cc1)cccc2</td>\n",
       "      <td>Hepatotoxicity</td>\n",
       "      <td>Mol4</td>\n",
       "      <td>8.554231</td>\n",
       "      <td>7.309128</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14.426921</td>\n",
       "      <td>2.333244</td>\n",
       "      <td>4.666488</td>\n",
       "      <td>...</td>\n",
       "      <td>0.072155</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.019041</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.069446</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Clc1ccc(C[C@@H](NC(=O)[C@H](NC(=O)C)Cc2cc3c(cc...</td>\n",
       "      <td>Hepatotoxicity</td>\n",
       "      <td>Mol5</td>\n",
       "      <td>76.862682</td>\n",
       "      <td>58.368692</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>125.113479</td>\n",
       "      <td>2.412042</td>\n",
       "      <td>4.800123</td>\n",
       "      <td>...</td>\n",
       "      <td>5.378359</td>\n",
       "      <td>0.262984</td>\n",
       "      <td>1.064018</td>\n",
       "      <td>0.428908</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.930087</td>\n",
       "      <td>0.083993</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003934</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1802 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Smiles           Liver Mol_ID  \\\n",
       "0                              S=C=Nc1c2c(ccc1)cccc2  Hepatotoxicity   Mol0   \n",
       "1  c1(c(cc(cc1[N+](=O)[O-])[N+](=O)[O-])[N+](=O)[...  Hepatotoxicity   Mol1   \n",
       "2            c1(c(cc(cc1)[N+](=O)[O-])[N+](=O)[O-])O  Hepatotoxicity   Mol2   \n",
       "3                                  Oc1cc2c(cc1)cccc2  Hepatotoxicity   Mol4   \n",
       "4  Clc1ccc(C[C@@H](NC(=O)[C@H](NC(=O)C)Cc2cc3c(cc...  Hepatotoxicity   Mol5   \n",
       "\n",
       "         ABC      ABCGG  nAcid  nBase     SpAbs_A   SpMax_A  SpDiam_A  ...  \\\n",
       "0   9.818615   8.499990      0      0   17.130940  2.368752  4.737504  ...   \n",
       "1  11.877237  11.411786      0      0   18.380381  2.411142  4.822284  ...   \n",
       "2   9.618017   9.150803      0      0   15.158804  2.337747  4.675495  ...   \n",
       "3   8.554231   7.309128      0      0   14.426921  2.333244  4.666488  ...   \n",
       "4  76.862682  58.368692      0      1  125.113479  2.412042  4.800123  ...   \n",
       "\n",
       "   Hypertrophy >= 1.33, 2) no other finding  \\\n",
       "0                                  0.064164   \n",
       "1                                  0.010342   \n",
       "2                                  1.470138   \n",
       "3                                  0.072155   \n",
       "4                                  5.378359   \n",
       "\n",
       "   Increased glycogen, 2) any other finding  \\\n",
       "0                                       NaN   \n",
       "1                                  0.008537   \n",
       "2                                  0.083020   \n",
       "3                                       NaN   \n",
       "4                                  0.262984   \n",
       "\n",
       "   Increased mitosis, 1) no other findings  Necrosis, 1) no other findings  \\\n",
       "0                                 0.010211                        0.010211   \n",
       "1                                 0.128815                             NaN   \n",
       "2                                 0.254973                        0.010825   \n",
       "3                                      NaN                        0.019041   \n",
       "4                                 1.064018                        0.428908   \n",
       "\n",
       "   Single cell necrosis, 2) allowing hypertrophy  \\\n",
       "0                                            NaN   \n",
       "1                                            NaN   \n",
       "2                                       0.000476   \n",
       "3                                            NaN   \n",
       "4                                            NaN   \n",
       "\n",
       "   Tbili >= 100%, 2) with hyperplasia at any grade, any other path at any grade  \\\n",
       "0                                           0.001686                              \n",
       "1                                           0.056213                              \n",
       "2                                           0.311952                              \n",
       "3                                           0.069446                              \n",
       "4                                           0.930087                              \n",
       "\n",
       "   Trigs < -60%, 1) no path findings and FC >-15%  \\\n",
       "0                                             NaN   \n",
       "1                                        0.137352   \n",
       "2                                        0.197157   \n",
       "3                                             NaN   \n",
       "4                                        0.083993   \n",
       "\n",
       "   Trigs > 80%, 1) no path findings  \\\n",
       "0                               NaN   \n",
       "1                               NaN   \n",
       "2                               NaN   \n",
       "3                               NaN   \n",
       "4                               NaN   \n",
       "\n",
       "   Vacuolation, 2) allowing hypertrophy at any grade  label  \n",
       "0                                                NaN      1  \n",
       "1                                                NaN      1  \n",
       "2                                           0.050395      1  \n",
       "3                                                NaN      1  \n",
       "4                                           0.003934      1  \n",
       "\n",
       "[5 rows x 1802 columns]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_liu_df = pd.merge(df1, liu_df, on='Smiles')\n",
    "merged_liu_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_liu_df =  merged_df_with_drops.join(liu_df)\n",
    "# merged_liu_df\n",
    "#pd.concat([merged_df_with_drops, liu_df], ignore_index=True, b).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# merged_liu_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ABC</th>\n",
       "      <th>ABCGG</th>\n",
       "      <th>nAcid</th>\n",
       "      <th>nBase</th>\n",
       "      <th>SpAbs_A</th>\n",
       "      <th>SpMax_A</th>\n",
       "      <th>SpDiam_A</th>\n",
       "      <th>SpAD_A</th>\n",
       "      <th>SpMAD_A</th>\n",
       "      <th>LogEE_A</th>\n",
       "      <th>...</th>\n",
       "      <th>Hypertrophy &gt;= 1.33, 2) no other finding</th>\n",
       "      <th>Increased glycogen, 2) any other finding</th>\n",
       "      <th>Increased mitosis, 1) no other findings</th>\n",
       "      <th>Necrosis, 1) no other findings</th>\n",
       "      <th>Single cell necrosis, 2) allowing hypertrophy</th>\n",
       "      <th>Tbili &gt;= 100%, 2) with hyperplasia at any grade, any other path at any grade</th>\n",
       "      <th>Trigs &lt; -60%, 1) no path findings and FC &gt;-15%</th>\n",
       "      <th>Trigs &gt; 80%, 1) no path findings</th>\n",
       "      <th>Vacuolation, 2) allowing hypertrophy at any grade</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1178.000000</td>\n",
       "      <td>1178.000000</td>\n",
       "      <td>1178.000000</td>\n",
       "      <td>1178.000000</td>\n",
       "      <td>1178.000000</td>\n",
       "      <td>1178.000000</td>\n",
       "      <td>1178.000000</td>\n",
       "      <td>1178.000000</td>\n",
       "      <td>1178.000000</td>\n",
       "      <td>1178.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1178.000000</td>\n",
       "      <td>1178.000000</td>\n",
       "      <td>1178.000000</td>\n",
       "      <td>1178.000000</td>\n",
       "      <td>1178.000000</td>\n",
       "      <td>1178.000000</td>\n",
       "      <td>1178.000000</td>\n",
       "      <td>1178.000000</td>\n",
       "      <td>1178.000000</td>\n",
       "      <td>1178.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>21.722439</td>\n",
       "      <td>17.562079</td>\n",
       "      <td>0.396435</td>\n",
       "      <td>0.731749</td>\n",
       "      <td>35.420072</td>\n",
       "      <td>2.456773</td>\n",
       "      <td>4.869460</td>\n",
       "      <td>35.420072</td>\n",
       "      <td>1.254836</td>\n",
       "      <td>4.116869</td>\n",
       "      <td>...</td>\n",
       "      <td>0.959900</td>\n",
       "      <td>0.066778</td>\n",
       "      <td>0.074643</td>\n",
       "      <td>0.201631</td>\n",
       "      <td>0.006204</td>\n",
       "      <td>0.324248</td>\n",
       "      <td>0.068306</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.150546</td>\n",
       "      <td>0.550934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>17.683959</td>\n",
       "      <td>12.786799</td>\n",
       "      <td>0.937225</td>\n",
       "      <td>1.287556</td>\n",
       "      <td>28.714815</td>\n",
       "      <td>0.136663</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>28.714815</td>\n",
       "      <td>0.058657</td>\n",
       "      <td>0.485863</td>\n",
       "      <td>...</td>\n",
       "      <td>2.865912</td>\n",
       "      <td>0.176557</td>\n",
       "      <td>0.267153</td>\n",
       "      <td>0.695039</td>\n",
       "      <td>0.029608</td>\n",
       "      <td>0.832103</td>\n",
       "      <td>0.167281</td>\n",
       "      <td>0.004980</td>\n",
       "      <td>0.355438</td>\n",
       "      <td>0.497610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.461420</td>\n",
       "      <td>4.736714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.384646</td>\n",
       "      <td>1.918986</td>\n",
       "      <td>3.837972</td>\n",
       "      <td>7.384646</td>\n",
       "      <td>0.923081</td>\n",
       "      <td>2.752227</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>14.311544</td>\n",
       "      <td>12.461206</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.377559</td>\n",
       "      <td>2.377166</td>\n",
       "      <td>4.724356</td>\n",
       "      <td>23.377559</td>\n",
       "      <td>1.225275</td>\n",
       "      <td>3.848884</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000463</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001427</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>18.535981</td>\n",
       "      <td>15.097093</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.405667</td>\n",
       "      <td>2.459334</td>\n",
       "      <td>4.859646</td>\n",
       "      <td>30.405667</td>\n",
       "      <td>1.258685</td>\n",
       "      <td>4.108444</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056168</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042012</td>\n",
       "      <td>0.001205</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>23.434202</td>\n",
       "      <td>18.903305</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>38.528487</td>\n",
       "      <td>2.536156</td>\n",
       "      <td>5.035129</td>\n",
       "      <td>38.528487</td>\n",
       "      <td>1.292439</td>\n",
       "      <td>4.335895</td>\n",
       "      <td>...</td>\n",
       "      <td>0.392531</td>\n",
       "      <td>0.016924</td>\n",
       "      <td>0.018226</td>\n",
       "      <td>0.111658</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.262313</td>\n",
       "      <td>0.038833</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051356</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>241.579838</td>\n",
       "      <td>174.358527</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>396.516849</td>\n",
       "      <td>3.187649</td>\n",
       "      <td>6.093353</td>\n",
       "      <td>396.516849</td>\n",
       "      <td>1.403982</td>\n",
       "      <td>6.652918</td>\n",
       "      <td>...</td>\n",
       "      <td>22.036753</td>\n",
       "      <td>1.519854</td>\n",
       "      <td>2.214875</td>\n",
       "      <td>6.985869</td>\n",
       "      <td>0.333843</td>\n",
       "      <td>7.669508</td>\n",
       "      <td>2.084296</td>\n",
       "      <td>0.033264</td>\n",
       "      <td>2.877466</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 1797 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               ABC        ABCGG        nAcid        nBase      SpAbs_A  \\\n",
       "count  1178.000000  1178.000000  1178.000000  1178.000000  1178.000000   \n",
       "mean     21.722439    17.562079     0.396435     0.731749    35.420072   \n",
       "std      17.683959    12.786799     0.937225     1.287556    28.714815   \n",
       "min       4.461420     4.736714     0.000000     0.000000     7.384646   \n",
       "25%      14.311544    12.461206     0.000000     0.000000    23.377559   \n",
       "50%      18.535981    15.097093     0.000000     0.000000    30.405667   \n",
       "75%      23.434202    18.903305     0.750000     1.000000    38.528487   \n",
       "max     241.579838   174.358527    10.000000    14.000000   396.516849   \n",
       "\n",
       "           SpMax_A     SpDiam_A       SpAD_A      SpMAD_A      LogEE_A  ...  \\\n",
       "count  1178.000000  1178.000000  1178.000000  1178.000000  1178.000000  ...   \n",
       "mean      2.456773     4.869460    35.420072     1.254836     4.116869  ...   \n",
       "std       0.136663     0.258065    28.714815     0.058657     0.485863  ...   \n",
       "min       1.918986     3.837972     7.384646     0.923081     2.752227  ...   \n",
       "25%       2.377166     4.724356    23.377559     1.225275     3.848884  ...   \n",
       "50%       2.459334     4.859646    30.405667     1.258685     4.108444  ...   \n",
       "75%       2.536156     5.035129    38.528487     1.292439     4.335895  ...   \n",
       "max       3.187649     6.093353   396.516849     1.403982     6.652918  ...   \n",
       "\n",
       "       Hypertrophy >= 1.33, 2) no other finding  \\\n",
       "count                               1178.000000   \n",
       "mean                                   0.959900   \n",
       "std                                    2.865912   \n",
       "min                                    0.000000   \n",
       "25%                                    0.000463   \n",
       "50%                                    0.056168   \n",
       "75%                                    0.392531   \n",
       "max                                   22.036753   \n",
       "\n",
       "       Increased glycogen, 2) any other finding  \\\n",
       "count                               1178.000000   \n",
       "mean                                   0.066778   \n",
       "std                                    0.176557   \n",
       "min                                    0.000000   \n",
       "25%                                    0.000000   \n",
       "50%                                    0.000000   \n",
       "75%                                    0.016924   \n",
       "max                                    1.519854   \n",
       "\n",
       "       Increased mitosis, 1) no other findings  \\\n",
       "count                              1178.000000   \n",
       "mean                                  0.074643   \n",
       "std                                   0.267153   \n",
       "min                                   0.000000   \n",
       "25%                                   0.000000   \n",
       "50%                                   0.000000   \n",
       "75%                                   0.018226   \n",
       "max                                   2.214875   \n",
       "\n",
       "       Necrosis, 1) no other findings  \\\n",
       "count                     1178.000000   \n",
       "mean                         0.201631   \n",
       "std                          0.695039   \n",
       "min                          0.000000   \n",
       "25%                          0.000000   \n",
       "50%                          0.007667   \n",
       "75%                          0.111658   \n",
       "max                          6.985869   \n",
       "\n",
       "       Single cell necrosis, 2) allowing hypertrophy  \\\n",
       "count                                    1178.000000   \n",
       "mean                                        0.006204   \n",
       "std                                         0.029608   \n",
       "min                                         0.000000   \n",
       "25%                                         0.000000   \n",
       "50%                                         0.000000   \n",
       "75%                                         0.000000   \n",
       "max                                         0.333843   \n",
       "\n",
       "       Tbili >= 100%, 2) with hyperplasia at any grade, any other path at any grade  \\\n",
       "count                                        1178.000000                              \n",
       "mean                                            0.324248                              \n",
       "std                                             0.832103                              \n",
       "min                                             0.000000                              \n",
       "25%                                             0.001427                              \n",
       "50%                                             0.042012                              \n",
       "75%                                             0.262313                              \n",
       "max                                             7.669508                              \n",
       "\n",
       "       Trigs < -60%, 1) no path findings and FC >-15%  \\\n",
       "count                                     1178.000000   \n",
       "mean                                         0.068306   \n",
       "std                                          0.167281   \n",
       "min                                          0.000000   \n",
       "25%                                          0.000000   \n",
       "50%                                          0.001205   \n",
       "75%                                          0.038833   \n",
       "max                                          2.084296   \n",
       "\n",
       "       Trigs > 80%, 1) no path findings  \\\n",
       "count                       1178.000000   \n",
       "mean                           0.000762   \n",
       "std                            0.004980   \n",
       "min                            0.000000   \n",
       "25%                            0.000000   \n",
       "50%                            0.000000   \n",
       "75%                            0.000000   \n",
       "max                            0.033264   \n",
       "\n",
       "       Vacuolation, 2) allowing hypertrophy at any grade        label  \n",
       "count                                        1178.000000  1178.000000  \n",
       "mean                                            0.150546     0.550934  \n",
       "std                                             0.355438     0.497610  \n",
       "min                                             0.000000     0.000000  \n",
       "25%                                             0.000000     0.000000  \n",
       "50%                                             0.000000     1.000000  \n",
       "75%                                             0.051356     1.000000  \n",
       "max                                             2.877466     1.000000  \n",
       "\n",
       "[8 rows x 1797 columns]"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FINAL_FULL_DF = merged_liu_df.fillna(0)#merged_liu_df.fillna(0)\n",
    "FINAL_FULL_DF.describe()\n",
    "\n",
    "## INCORPORATE CODY'S CODEEe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Smiles</th>\n",
       "      <th>Mol_ID</th>\n",
       "      <th>label</th>\n",
       "      <th>Liver</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S=C=Nc1c2c(ccc1)cccc2</td>\n",
       "      <td>Mol0</td>\n",
       "      <td>1</td>\n",
       "      <td>Hepatotoxicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c1(c(cc(cc1[N+](=O)[O-])[N+](=O)[O-])[N+](=O)[...</td>\n",
       "      <td>Mol1</td>\n",
       "      <td>1</td>\n",
       "      <td>Hepatotoxicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c1(c(cc(cc1)[N+](=O)[O-])[N+](=O)[O-])O</td>\n",
       "      <td>Mol2</td>\n",
       "      <td>1</td>\n",
       "      <td>Hepatotoxicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Oc1cc2c(cc1)cccc2</td>\n",
       "      <td>Mol4</td>\n",
       "      <td>1</td>\n",
       "      <td>Hepatotoxicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Clc1ccc(C[C@@H](NC(=O)[C@H](NC(=O)C)Cc2cc3c(cc...</td>\n",
       "      <td>Mol5</td>\n",
       "      <td>1</td>\n",
       "      <td>Hepatotoxicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>O([C@H]1[C@H](O)[C@@H](O)[C@H](O[C@@H]1CO)O[C@...</td>\n",
       "      <td>Mol6</td>\n",
       "      <td>1</td>\n",
       "      <td>Hepatotoxicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>O(C[C@@H](O)CNC(C)C)c1c(cc(NC(=O)CCC)cc1)C(=O)C</td>\n",
       "      <td>Mol7</td>\n",
       "      <td>1</td>\n",
       "      <td>Hepatotoxicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Clc1ccc(C(=O)n2c(c(c3c2ccc(OC)c3)CC(=O)OCC(=O)...</td>\n",
       "      <td>Mol8</td>\n",
       "      <td>1</td>\n",
       "      <td>Hepatotoxicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>S(=O)(=O)(N)c1sc(NC(=O)C)nn1</td>\n",
       "      <td>Mol9</td>\n",
       "      <td>0</td>\n",
       "      <td>NonHepatotoxicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>S1c2c(N(CCCN3CCN(CC3)CCO)c3c1cccc3)cc(cc2)C(=O)C</td>\n",
       "      <td>Mol11</td>\n",
       "      <td>1</td>\n",
       "      <td>Hepatotoxicity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Smiles Mol_ID  label  \\\n",
       "0                              S=C=Nc1c2c(ccc1)cccc2   Mol0      1   \n",
       "1  c1(c(cc(cc1[N+](=O)[O-])[N+](=O)[O-])[N+](=O)[...   Mol1      1   \n",
       "2            c1(c(cc(cc1)[N+](=O)[O-])[N+](=O)[O-])O   Mol2      1   \n",
       "3                                  Oc1cc2c(cc1)cccc2   Mol4      1   \n",
       "4  Clc1ccc(C[C@@H](NC(=O)[C@H](NC(=O)C)Cc2cc3c(cc...   Mol5      1   \n",
       "5  O([C@H]1[C@H](O)[C@@H](O)[C@H](O[C@@H]1CO)O[C@...   Mol6      1   \n",
       "6    O(C[C@@H](O)CNC(C)C)c1c(cc(NC(=O)CCC)cc1)C(=O)C   Mol7      1   \n",
       "7  Clc1ccc(C(=O)n2c(c(c3c2ccc(OC)c3)CC(=O)OCC(=O)...   Mol8      1   \n",
       "8                       S(=O)(=O)(N)c1sc(NC(=O)C)nn1   Mol9      0   \n",
       "9   S1c2c(N(CCCN3CCN(CC3)CCO)c3c1cccc3)cc(cc2)C(=O)C  Mol11      1   \n",
       "\n",
       "               Liver  \n",
       "0     Hepatotoxicity  \n",
       "1     Hepatotoxicity  \n",
       "2     Hepatotoxicity  \n",
       "3     Hepatotoxicity  \n",
       "4     Hepatotoxicity  \n",
       "5     Hepatotoxicity  \n",
       "6     Hepatotoxicity  \n",
       "7     Hepatotoxicity  \n",
       "8  NonHepatotoxicity  \n",
       "9     Hepatotoxicity  "
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ygt = FINAL_FULL_DF[['Smiles','Mol_ID', 'label', 'Liver']]\n",
    "Ygt.to_csv('FINAL_Y.csv', index= False)\n",
    "Ygt.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['S=C=Nc1c2c(ccc1)cccc2',\n",
       "       'c1(c(cc(cc1[N+](=O)[O-])[N+](=O)[O-])[N+](=O)[O-])C',\n",
       "       'c1(c(cc(cc1)[N+](=O)[O-])[N+](=O)[O-])O', 'Oc1cc2c(cc1)cccc2'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FINAL_FULL_DF['Smiles'][0:4].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1178, 1802)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FINAL_FULL_DF.shape #.head(2).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"making official train/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = FINAL_FULL_DF.label\n",
    "X = FINAL_FULL_DF.drop(columns=['Mol_ID', 'label', 'Liver'])\n",
    "Y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1178"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235.60000000000002"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1178*.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "942.4"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1178-235.6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating train and test split\n",
      "using train test split with test size at 20%\n"
     ]
    }
   ],
   "source": [
    "print('creating train and test split')\n",
    "X_traindf, X_testdf, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=42)\n",
    "print('using train test split with test size at 20%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading embeddings vectors\n",
      "done!\n",
      "(942,)\n"
     ]
    }
   ],
   "source": [
    "X_traindf.to_csv('final_official_X_filled.csv', index=False)\n",
    "X_testdf.to_csv('final_official_X_filled.csv', index=False)\n",
    "# TODO: wwork with Cody on imputation method!!\n",
    "# for now filling na with 0\n",
    "import keras.backend as K\n",
    "print('loading embeddings vectors')\n",
    "\n",
    "def get_coefs(word,*arr): \n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "embeddings_index = dict(get_coefs(*o.split(' '))\n",
    "        for o in open('/root/SMILESVecProteinRepresentation/source/utils/drug.pubchem.canon.l1.ws20.txt'))\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 14\n",
    "embed_size = 100 #embeddings dimension\n",
    "\n",
    "print(\"done!\")\n",
    "# importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import re\n",
    "# Fork of Sergei Fironov's script CNN GLOVE300 3-OOF 3 epochs\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input, Concatenate, Conv1D, Activation, TimeDistributed, Flatten, RepeatVector, Permute,multiply\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout, GRU, GlobalAveragePooling1D, MaxPooling1D, SpatialDropout1D, BatchNormalization\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "list_smiles_train = X_traindf[\"Smiles\"].fillna(\"\").values #(30053,)\n",
    "print(list_smiles_train.shape) # print(list_sentences_train.shape) # \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(236,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'s1c(C/C(=C\\\\c2n(c(nc2)CCCC)Cc2ccc(cc2)C(=O)O)/C(=O)O)ccc1'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_features = 61 #\n",
    "maxlen =  100#padding length\n",
    "num_folds = 3#2 #number of folds\n",
    "list_smiles_test = X_testdf[\"Smiles\"].fillna(\"\").values\n",
    "print(list_smiles_test.shape)\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "min_count = 1\n",
    "list_smiles_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s1c', 'C', 'C', '', 'C', 'c2n', 'c', 'nc2', 'CCCC', 'Cc2ccc', 'cc2', 'C', '', 'O', 'O', '', 'C', '', 'O', 'O', 'ccc1']\n",
      "num_words 29\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import string\n",
    "import re\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "print(re.split(string=list_smiles_train[0], pattern=regex))\n",
    "# list(list_smiles_train[0])\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(list(list_smiles_train) + list(list_smiles_test))\n",
    "num_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])\n",
    "print('num_words', num_words)\n",
    "print(MAX_NB_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padding Smiles\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = Tokenizer(char_level=True, num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(list(list_smiles_train)) # + list(list_sentences_test)\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_smiles_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_smiles_test)\n",
    "print('padding Smiles')\n",
    "X_train = {}\n",
    "X_test = {}\n",
    "X_train['smiles'] = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen, padding='post', truncating='post')\n",
    "X_test['smiles'] = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created min max scaler\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X_traindf = X_traindf.fillna(K.epsilon())\n",
    "#X_testdf = X_testdf.fillna(K.epsilon())\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(copy=True, feature_range=(0, 1))\n",
    "print('created min max scaler')\n",
    "len(X_train['smiles'][54])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make sure to check the len of the smiles arrays ^^^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K backend imported!\n"
     ]
    }
   ],
   "source": [
    "#import keras.backend as K\n",
    "from keras import backend as K\n",
    "\n",
    "def binary_accuracy(y_true, y_pred):\n",
    "    return K.mean(K.equal(y_true, K.round(y_pred)))\n",
    "\n",
    "\n",
    "def categorical_accuracy(y_true, y_pred):\n",
    "    return K.mean(K.equal(K.argmax(y_true, axis=-1),\n",
    "                          K.argmax(y_pred, axis=-1)))\n",
    "\n",
    "\n",
    "def sparse_categorical_accuracy(y_true, y_pred):\n",
    "    return K.mean(K.equal(K.max(y_true, axis=-1),\n",
    "                          K.cast(K.argmax(y_pred, axis=-1), K.floatx())))\n",
    "\n",
    "\n",
    "def top_k_categorical_accuracy(y_true, y_pred, k=5):\n",
    "    return K.mean(K.in_top_k(y_pred, K.argmax(y_true, axis=-1), k))\n",
    "\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred - y_true))\n",
    "\n",
    "\n",
    "def mean_absolute_error(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_pred - y_true))\n",
    "\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    diff = K.abs((y_true - y_pred) / K.clip(K.abs(y_true),\n",
    "                                            K.epsilon(),\n",
    "                                            None))\n",
    "    return 100. * K.mean(diff)\n",
    "\n",
    "\n",
    "def mean_squared_logarithmic_error(y_true, y_pred):\n",
    "    first_log = K.log(K.clip(y_pred, K.epsilon(), None) + 1.)\n",
    "    second_log = K.log(K.clip(y_true, K.epsilon(), None) + 1.)\n",
    "    return K.mean(K.square(first_log - second_log))\n",
    "\n",
    "\n",
    "def hinge(y_true, y_pred):\n",
    "    return K.mean(K.maximum(1. - y_true * y_pred, 0.))\n",
    "\n",
    "\n",
    "def squared_hinge(y_true, y_pred):\n",
    "    return K.mean(K.square(K.maximum(1. - y_true * y_pred, 0.)))\n",
    "\n",
    "\n",
    "def categorical_crossentropy(y_true, y_pred):\n",
    "    return K.mean(K.categorical_crossentropy(y_pred, y_true))\n",
    "\n",
    "\n",
    "def sparse_categorical_crossentropy(y_true, y_pred):\n",
    "    return K.mean(K.sparse_categorical_crossentropy(y_pred, y_true))\n",
    "\n",
    "\n",
    "def binary_crossentropy(y_true, y_pred):\n",
    "    return K.mean(K.binary_crossentropy(y_pred, y_true))\n",
    "\n",
    "\n",
    "def kullback_leibler_divergence(y_true, y_pred):\n",
    "    y_true = K.clip(y_true, K.epsilon(), 1)\n",
    "    y_pred = K.clip(y_pred, K.epsilon(), 1)\n",
    "    return K.mean(K.sum(y_true * K.log(y_true / y_pred), axis=-1))\n",
    "\n",
    "\n",
    "def poisson(y_true, y_pred):\n",
    "    return K.mean(y_pred - y_true * K.log(y_pred + K.epsilon()))\n",
    "\n",
    "\n",
    "def cosine_proximity(y_true, y_pred):\n",
    "    y_true = K.l2_normalize(y_true, axis=-1)\n",
    "    y_pred = K.l2_normalize(y_pred, axis=-1)\n",
    "    return -K.mean(y_true * y_pred)\n",
    "\n",
    "def matthews_correlation(y_true, y_pred):\n",
    "    \"\"\"Matthews correlation metric.\n",
    "    It is only computed as a batch-wise average, not globally.\n",
    "    Computes the Matthews correlation coefficient measure for quality\n",
    "    of binary classification problems.\n",
    "    \"\"\"\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    "\n",
    "    y_pos = K.round(K.clip(y_true, 0, 1))\n",
    "    y_neg = 1 - y_pos\n",
    "\n",
    "    tp = K.sum(y_pos * y_pred_pos)\n",
    "    tn = K.sum(y_neg * y_pred_neg)\n",
    "\n",
    "    fp = K.sum(y_neg * y_pred_pos)\n",
    "    fn = K.sum(y_pos * y_pred_neg)\n",
    "\n",
    "    numerator = (tp * tn - fp * fn)\n",
    "    denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "\n",
    "    return numerator / (denominator + K.epsilon())\n",
    "\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "    Only computes a batch-wise average of precision.\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "    Only computes a batch-wise average of recall.\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def fbeta_score(y_true, y_pred, beta=1):\n",
    "    \"\"\"Computes the F score.\n",
    "    The F score is the weighted harmonic mean of precision and recall.\n",
    "    Here it is only computed as a batch-wise average, not globally.\n",
    "    This is useful for multi-label classification, where input samples can be\n",
    "    classified as sets of labels. By only using accuracy (precision) a model\n",
    "    would achieve a perfect score by simply assigning every class to every\n",
    "    input. In order to avoid this, a metric should penalize incorrect class\n",
    "    assignments as well (recall). The F-beta score (ranged from 0.0 to 1.0)\n",
    "    computes this, as a weighted mean of the proportion of correct class\n",
    "    assignments vs. the proportion of incorrect class assignments.\n",
    "    With beta = 1, this is equivalent to a F-measure. With beta < 1, assigning\n",
    "    correct classes becomes more important, and with beta > 1 the metric is\n",
    "    instead weighted towards penalizing incorrect class assignments.\n",
    "    \"\"\"\n",
    "    if beta < 0:\n",
    "        raise ValueError('The lowest choosable beta is zero (only precision).')\n",
    "\n",
    "    # If there are no true positives, fix the F score at 0 like sklearn.\n",
    "    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:\n",
    "        return 0\n",
    "\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    bb = beta ** 2\n",
    "    fbeta_score = (1 + bb) * (p * r) / (bb * p + r + K.epsilon())\n",
    "    return fbeta_score\n",
    "\n",
    "\n",
    "def fmeasure(y_true, y_pred):\n",
    "    \"\"\"Computes the f-measure, the harmonic mean of precision and recall.\n",
    "    Here it is only computed as a batch-wise average, not globally.\n",
    "    \"\"\"\n",
    "    return fbeta_score(y_true, y_pred, beta=1)\n",
    "\n",
    "\n",
    "# aliases\n",
    "mse = MSE = mean_squared_error\n",
    "mae = MAE = mean_absolute_error\n",
    "mape = MAPE = mean_absolute_percentage_error\n",
    "msle = MSLE = mean_squared_logarithmic_error\n",
    "cosine = cosine_proximity\n",
    "fscore = f1score = fmeasure\n",
    "print(\"K backend imported!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below you see the average + max length of the 'tokens' for the smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean text len: 26.20063694267516\n",
      "max text len: 372\n"
     ]
    }
   ],
   "source": [
    "print('mean text len:',X_traindf[\"Smiles\"].str.count(regex).mean())\n",
    "print('max text len:',X_traindf[\"Smiles\"].str.count(regex).max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below you will see the average+max of the length of the characters in the smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean text len: 62.42675159235669\n",
      "max text len: 705\n"
     ]
    }
   ],
   "source": [
    "print('mean text len:', X_traindf[\"Smiles\"].apply(lambda x: len(list(x))).mean())\n",
    "print('max text len:',X_traindf[\"Smiles\"].apply(lambda x: len(list(x))).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences Matrix Made!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tok = Tokenizer(char_level=True, num_words=29)\n",
    "tok.fit_on_texts(X_traindf['Smiles'])\n",
    "sequences = tok.texts_to_sequences(X_traindf['Smiles'])\n",
    "sequences_matrix = sequence.pad_sequences(sequences,100)\n",
    "print(\"Sequences Matrix Made!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "1798\n"
     ]
    }
   ],
   "source": [
    "print(maxlen)\n",
    "print(MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary size:  29\n",
      "preparing embedding matrix...\n",
      "number of null word embeddings: 5\n",
      "sample words not found:  ['/' '\\\\' 'p' '@' 'p' '@' '@' '\\\\' '\\\\' 'p']\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print(\"dictionary size: \", len(word_index))\n",
    "print('preparing embedding matrix...')\n",
    "words_not_found = []\n",
    "embed_dim = 100\n",
    "nb_words = min(29, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i >= nb_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        words_not_found.append(word)\n",
    "print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "print(\"sample words not found: \", np.random.choice(words_not_found, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training CNN ...\n",
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_98 (Embedding)     (None, 100, 100)          2900      \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 100, 64)           6464      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 50, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 50, 64)            4160      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_125 (Gl (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_70 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 15,637\n",
      "Trainable params: 15,637\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 847 samples, validate on 95 samples\n",
      "Epoch 1/60\n",
      " - 6s - loss: 0.7061 - acc: 0.4911 - precision: 0.5338 - recall: 0.6614 - fmeasure: 0.5863 - val_loss: 0.6863 - val_acc: 0.5895 - val_precision: 0.5895 - val_recall: 1.0000 - val_fmeasure: 0.7417\n",
      "Epoch 2/60\n",
      " - 0s - loss: 0.6906 - acc: 0.5655 - precision: 0.5663 - recall: 0.9172 - fmeasure: 0.6992 - val_loss: 0.6862 - val_acc: 0.5895 - val_precision: 0.5895 - val_recall: 1.0000 - val_fmeasure: 0.7417\n",
      "Epoch 3/60\n",
      " - 0s - loss: 0.6923 - acc: 0.5514 - precision: 0.5589 - recall: 0.8909 - fmeasure: 0.6867 - val_loss: 0.6874 - val_acc: 0.5684 - val_precision: 0.5806 - val_recall: 0.9643 - val_fmeasure: 0.7248\n",
      "Epoch 4/60\n",
      " - 0s - loss: 0.6917 - acc: 0.5573 - precision: 0.5758 - recall: 0.7703 - fmeasure: 0.6575 - val_loss: 0.6897 - val_acc: 0.5474 - val_precision: 0.5867 - val_recall: 0.7857 - val_fmeasure: 0.6718\n",
      "Epoch 5/60\n",
      " - 0s - loss: 0.6841 - acc: 0.5608 - precision: 0.5822 - recall: 0.7400 - fmeasure: 0.6486 - val_loss: 0.6857 - val_acc: 0.5579 - val_precision: 0.5761 - val_recall: 0.9464 - val_fmeasure: 0.7162\n",
      "Epoch 6/60\n",
      " - 0s - loss: 0.6808 - acc: 0.5561 - precision: 0.5635 - recall: 0.8622 - fmeasure: 0.6815 - val_loss: 0.6825 - val_acc: 0.5895 - val_precision: 0.5895 - val_recall: 1.0000 - val_fmeasure: 0.7417\n",
      "Epoch 7/60\n",
      " - 0s - loss: 0.6785 - acc: 0.5726 - precision: 0.5692 - recall: 0.9339 - fmeasure: 0.7068 - val_loss: 0.6810 - val_acc: 0.5895 - val_precision: 0.5895 - val_recall: 1.0000 - val_fmeasure: 0.7417\n",
      "Epoch 8/60\n",
      " - 0s - loss: 0.6781 - acc: 0.5679 - precision: 0.5643 - recall: 0.9552 - fmeasure: 0.7089 - val_loss: 0.6803 - val_acc: 0.5895 - val_precision: 0.5895 - val_recall: 1.0000 - val_fmeasure: 0.7417\n",
      "Epoch 9/60\n",
      " - 0s - loss: 0.6835 - acc: 0.5514 - precision: 0.5591 - recall: 0.8913 - fmeasure: 0.6863 - val_loss: 0.6808 - val_acc: 0.5684 - val_precision: 0.5862 - val_recall: 0.9107 - val_fmeasure: 0.7133\n",
      "Epoch 10/60\n",
      " - 0s - loss: 0.6776 - acc: 0.6139 - precision: 0.6060 - recall: 0.8406 - fmeasure: 0.7040 - val_loss: 0.6832 - val_acc: 0.5789 - val_precision: 0.6176 - val_recall: 0.7500 - val_fmeasure: 0.6774\n",
      "Epoch 11/60\n",
      " - 0s - loss: 0.6789 - acc: 0.5691 - precision: 0.5911 - recall: 0.7130 - fmeasure: 0.6454 - val_loss: 0.6841 - val_acc: 0.5789 - val_precision: 0.6538 - val_recall: 0.6071 - val_fmeasure: 0.6296\n",
      "Epoch 12/60\n",
      " - 0s - loss: 0.6773 - acc: 0.5868 - precision: 0.6081 - recall: 0.7046 - fmeasure: 0.6524 - val_loss: 0.6808 - val_acc: 0.5895 - val_precision: 0.6232 - val_recall: 0.7679 - val_fmeasure: 0.6880\n",
      "Epoch 00012: early stopping\n"
     ]
    }
   ],
   "source": [
    "#CNN architecture\n",
    "num_classes = 1\n",
    "print(\"training CNN ...\")\n",
    "model = Sequential()\n",
    "num_filters = 64\n",
    "num_epochs = 60\n",
    "weight_decay = 1e-4\n",
    "batch_size = 256\n",
    "model.add(Embedding(29, embed_dim, \n",
    "          weights=[embedding_matrix], input_length=100, trainable=True))\n",
    "model.add(Conv1D(num_filters, 1, activation='relu', padding='same'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Conv1D(num_filters, 1, activation='relu', padding='same'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Dense(num_classes, activation='sigmoid'))  #multi-label (k-hot encoding)\n",
    "\n",
    "adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy', precision, recall, fmeasure])\n",
    "model.summary()\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=4, verbose=1)\n",
    "callbacks_list = [early_stopping]\n",
    "\n",
    "#model training\n",
    "hist = model.fit(sequences_matrix, Y_train, batch_size=batch_size, epochs=num_epochs, callbacks=callbacks_list, validation_split=0.1, shuffle=True, verbose=2)\n",
    "test_sequences = tok.texts_to_sequences(X_testdf['Smiles']) # keep as dataframe format\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = model.predict(test_sequences_matrix)\n",
    "model.save('cnn_embeddings_model.h5')\n",
    "\n",
    "#generate plots\n",
    "plt.figure()\n",
    "plt.plot(hist.history['loss'], lw=2.0, color='b', label='train')\n",
    "plt.plot(hist.history['val_loss'], lw=2.0, color='r', label='val')\n",
    "plt.title('CNN Tox {Pred}')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Cross-Entropy Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_60\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_99 (Embedding)     (None, 100, 100)          2900      \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 64)                42240     \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_71 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 62,037\n",
      "Trainable params: 62,037\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def RNN():\n",
    "    inputs = Input(name='inputs',shape=[maxlen])\n",
    "    layer = Embedding(29,100,input_length=maxlen)(inputs)\n",
    "    layer = LSTM(64)(layer)\n",
    "    layer = Dense(256,name='FC1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.5)(layer)\n",
    "    layer = Dense(1,name='out_layer')(layer)\n",
    "    layer = Activation('sigmoid')(layer)\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model\n",
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy', precision, recall, fmeasure])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 753 samples, validate on 189 samples\n",
      "Epoch 1/50\n",
      "753/753 [==============================] - 7s 9ms/step - loss: 0.6896 - acc: 0.5684 - precision: 0.5808 - recall: 0.8779 - fmeasure: 0.6894 - val_loss: 0.6913 - val_acc: 0.5344 - val_precision: 0.5344 - val_recall: 1.0000 - val_fmeasure: 0.6966\n",
      "Epoch 2/50\n",
      "753/753 [==============================] - 0s 594us/step - loss: 0.6853 - acc: 0.5618 - precision: 0.5618 - recall: 1.0000 - fmeasure: 0.7190 - val_loss: 0.6907 - val_acc: 0.5344 - val_precision: 0.5344 - val_recall: 1.0000 - val_fmeasure: 0.6966\n",
      "Epoch 3/50\n",
      "753/753 [==============================] - 0s 624us/step - loss: 0.6829 - acc: 0.5618 - precision: 0.5618 - recall: 1.0000 - fmeasure: 0.7190 - val_loss: 0.6968 - val_acc: 0.5344 - val_precision: 0.5344 - val_recall: 1.0000 - val_fmeasure: 0.6966\n",
      "Epoch 4/50\n",
      "753/753 [==============================] - 1s 668us/step - loss: 0.6820 - acc: 0.5618 - precision: 0.5618 - recall: 1.0000 - fmeasure: 0.7183 - val_loss: 0.6911 - val_acc: 0.5344 - val_precision: 0.5344 - val_recall: 1.0000 - val_fmeasure: 0.6966\n",
      "Epoch 5/50\n",
      "753/753 [==============================] - 0s 640us/step - loss: 0.6813 - acc: 0.5631 - precision: 0.5626 - recall: 1.0000 - fmeasure: 0.7183 - val_loss: 0.6933 - val_acc: 0.5344 - val_precision: 0.5344 - val_recall: 1.0000 - val_fmeasure: 0.6966\n",
      "Epoch 6/50\n",
      "753/753 [==============================] - 0s 615us/step - loss: 0.6785 - acc: 0.5618 - precision: 0.5618 - recall: 1.0000 - fmeasure: 0.7191 - val_loss: 0.6923 - val_acc: 0.5344 - val_precision: 0.5344 - val_recall: 1.0000 - val_fmeasure: 0.6966\n",
      "Epoch 7/50\n",
      "753/753 [==============================] - 0s 613us/step - loss: 0.6759 - acc: 0.5604 - precision: 0.5612 - recall: 0.9977 - fmeasure: 0.7183 - val_loss: 0.6944 - val_acc: 0.5344 - val_precision: 0.5344 - val_recall: 1.0000 - val_fmeasure: 0.6966\n",
      "Epoch 8/50\n",
      "753/753 [==============================] - 0s 643us/step - loss: 0.6750 - acc: 0.5657 - precision: 0.5665 - recall: 0.9796 - fmeasure: 0.7168 - val_loss: 0.7140 - val_acc: 0.5344 - val_precision: 0.5344 - val_recall: 1.0000 - val_fmeasure: 0.6966\n",
      "Epoch 9/50\n",
      "753/753 [==============================] - 0s 648us/step - loss: 0.6770 - acc: 0.5618 - precision: 0.5653 - recall: 0.9580 - fmeasure: 0.7103 - val_loss: 0.6939 - val_acc: 0.5344 - val_precision: 0.5344 - val_recall: 1.0000 - val_fmeasure: 0.6966\n",
      "Epoch 10/50\n",
      "753/753 [==============================] - 0s 636us/step - loss: 0.6712 - acc: 0.5604 - precision: 0.5646 - recall: 0.9502 - fmeasure: 0.7083 - val_loss: 0.7035 - val_acc: 0.5238 - val_precision: 0.5301 - val_recall: 0.9604 - val_fmeasure: 0.6831\n",
      "Epoch 11/50\n",
      "753/753 [==============================] - 0s 613us/step - loss: 0.6735 - acc: 0.5710 - precision: 0.5785 - recall: 0.8877 - fmeasure: 0.6980 - val_loss: 0.6951 - val_acc: 0.5397 - val_precision: 0.5417 - val_recall: 0.9010 - val_fmeasure: 0.6766\n",
      "Epoch 12/50\n",
      "753/753 [==============================] - 0s 620us/step - loss: 0.6722 - acc: 0.5896 - precision: 0.5918 - recall: 0.8877 - fmeasure: 0.7085 - val_loss: 0.6981 - val_acc: 0.5556 - val_precision: 0.5497 - val_recall: 0.9307 - val_fmeasure: 0.6912\n",
      "Epoch 13/50\n",
      "753/753 [==============================] - 0s 643us/step - loss: 0.6705 - acc: 0.5843 - precision: 0.5904 - recall: 0.8584 - fmeasure: 0.6989 - val_loss: 0.6962 - val_acc: 0.5450 - val_precision: 0.5466 - val_recall: 0.8713 - val_fmeasure: 0.6718\n",
      "Epoch 14/50\n",
      "753/753 [==============================] - 0s 619us/step - loss: 0.6772 - acc: 0.5923 - precision: 0.6168 - recall: 0.7547 - fmeasure: 0.6707 - val_loss: 0.6869 - val_acc: 0.5397 - val_precision: 0.5507 - val_recall: 0.7525 - val_fmeasure: 0.6360\n",
      "Epoch 15/50\n",
      "753/753 [==============================] - 0s 662us/step - loss: 0.6688 - acc: 0.6109 - precision: 0.6187 - recall: 0.8103 - fmeasure: 0.6999 - val_loss: 0.6888 - val_acc: 0.5450 - val_precision: 0.5460 - val_recall: 0.8812 - val_fmeasure: 0.6742\n",
      "Epoch 16/50\n",
      "753/753 [==============================] - 0s 598us/step - loss: 0.6658 - acc: 0.5963 - precision: 0.6035 - recall: 0.8175 - fmeasure: 0.6913 - val_loss: 0.6854 - val_acc: 0.5608 - val_precision: 0.5652 - val_recall: 0.7723 - val_fmeasure: 0.6527\n",
      "Epoch 17/50\n",
      "753/753 [==============================] - 0s 597us/step - loss: 0.6657 - acc: 0.6029 - precision: 0.6097 - recall: 0.8477 - fmeasure: 0.7052 - val_loss: 0.6835 - val_acc: 0.5820 - val_precision: 0.6019 - val_recall: 0.6436 - val_fmeasure: 0.6220\n",
      "Epoch 18/50\n",
      "753/753 [==============================] - 0s 591us/step - loss: 0.6634 - acc: 0.5963 - precision: 0.6239 - recall: 0.7139 - fmeasure: 0.6612 - val_loss: 0.6837 - val_acc: 0.5556 - val_precision: 0.5578 - val_recall: 0.8119 - val_fmeasure: 0.6613\n",
      "Epoch 19/50\n",
      "753/753 [==============================] - 0s 623us/step - loss: 0.6625 - acc: 0.6135 - precision: 0.6231 - recall: 0.8001 - fmeasure: 0.6973 - val_loss: 0.6851 - val_acc: 0.5661 - val_precision: 0.5655 - val_recall: 0.8119 - val_fmeasure: 0.6667\n",
      "Epoch 20/50\n",
      "753/753 [==============================] - 0s 626us/step - loss: 0.6602 - acc: 0.6175 - precision: 0.6264 - recall: 0.7965 - fmeasure: 0.7006 - val_loss: 0.6855 - val_acc: 0.5820 - val_precision: 0.5932 - val_recall: 0.6931 - val_fmeasure: 0.6393\n",
      "Epoch 21/50\n",
      "753/753 [==============================] - 0s 600us/step - loss: 0.6712 - acc: 0.5910 - precision: 0.6437 - recall: 0.6915 - fmeasure: 0.6466 - val_loss: 0.6786 - val_acc: 0.5979 - val_precision: 0.5912 - val_recall: 0.8020 - val_fmeasure: 0.6807\n",
      "Epoch 22/50\n",
      "753/753 [==============================] - 0s 601us/step - loss: 0.6624 - acc: 0.6375 - precision: 0.6448 - recall: 0.7994 - fmeasure: 0.7127 - val_loss: 0.6778 - val_acc: 0.5608 - val_precision: 0.5616 - val_recall: 0.8119 - val_fmeasure: 0.6640\n",
      "Epoch 23/50\n",
      "753/753 [==============================] - 0s 588us/step - loss: 0.6577 - acc: 0.6282 - precision: 0.6351 - recall: 0.7976 - fmeasure: 0.7057 - val_loss: 0.6807 - val_acc: 0.5608 - val_precision: 0.5600 - val_recall: 0.8317 - val_fmeasure: 0.6693\n",
      "Epoch 24/50\n",
      "753/753 [==============================] - 0s 594us/step - loss: 0.6521 - acc: 0.6454 - precision: 0.6693 - recall: 0.7612 - fmeasure: 0.7042 - val_loss: 0.7289 - val_acc: 0.5291 - val_precision: 0.5323 - val_recall: 0.9802 - val_fmeasure: 0.6899\n",
      "Epoch 25/50\n",
      "753/753 [==============================] - 0s 590us/step - loss: 0.6633 - acc: 0.6215 - precision: 0.6271 - recall: 0.8469 - fmeasure: 0.7143 - val_loss: 0.6739 - val_acc: 0.6032 - val_precision: 0.6032 - val_recall: 0.7525 - val_fmeasure: 0.6696\n",
      "Epoch 26/50\n",
      "753/753 [==============================] - 0s 601us/step - loss: 0.6606 - acc: 0.6069 - precision: 0.6424 - recall: 0.7500 - fmeasure: 0.6759 - val_loss: 0.6765 - val_acc: 0.5608 - val_precision: 0.5608 - val_recall: 0.8218 - val_fmeasure: 0.6667\n",
      "Epoch 27/50\n",
      "753/753 [==============================] - 0s 600us/step - loss: 0.6547 - acc: 0.6189 - precision: 0.6315 - recall: 0.7702 - fmeasure: 0.6912 - val_loss: 0.6744 - val_acc: 0.5767 - val_precision: 0.5802 - val_recall: 0.7525 - val_fmeasure: 0.6552\n",
      "Epoch 28/50\n",
      "753/753 [==============================] - 0s 614us/step - loss: 0.6439 - acc: 0.6375 - precision: 0.6507 - recall: 0.7707 - fmeasure: 0.7050 - val_loss: 0.6721 - val_acc: 0.5873 - val_precision: 0.6055 - val_recall: 0.6535 - val_fmeasure: 0.6286\n",
      "Epoch 29/50\n",
      "753/753 [==============================] - 0s 573us/step - loss: 0.6519 - acc: 0.6268 - precision: 0.6709 - recall: 0.6780 - fmeasure: 0.6661 - val_loss: 0.7066 - val_acc: 0.5344 - val_precision: 0.5351 - val_recall: 0.9802 - val_fmeasure: 0.6923\n",
      "Epoch 30/50\n",
      "753/753 [==============================] - 0s 622us/step - loss: 0.6545 - acc: 0.6255 - precision: 0.6302 - recall: 0.8629 - fmeasure: 0.7218 - val_loss: 0.6724 - val_acc: 0.5767 - val_precision: 0.5814 - val_recall: 0.7426 - val_fmeasure: 0.6522\n",
      "Epoch 31/50\n",
      "753/753 [==============================] - 0s 589us/step - loss: 0.6474 - acc: 0.6348 - precision: 0.6440 - recall: 0.8066 - fmeasure: 0.7119 - val_loss: 0.6743 - val_acc: 0.5926 - val_precision: 0.6364 - val_recall: 0.5545 - val_fmeasure: 0.5926\n",
      "Epoch 32/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "753/753 [==============================] - 0s 609us/step - loss: 0.6575 - acc: 0.6109 - precision: 0.6647 - recall: 0.6934 - fmeasure: 0.6610 - val_loss: 0.6744 - val_acc: 0.5767 - val_precision: 0.5778 - val_recall: 0.7723 - val_fmeasure: 0.6610\n",
      "Epoch 33/50\n",
      "753/753 [==============================] - 0s 632us/step - loss: 0.6526 - acc: 0.6295 - precision: 0.6461 - recall: 0.8197 - fmeasure: 0.7124 - val_loss: 0.6732 - val_acc: 0.5714 - val_precision: 0.5926 - val_recall: 0.6337 - val_fmeasure: 0.6124\n",
      "Epoch 34/50\n",
      "753/753 [==============================] - 0s 634us/step - loss: 0.6443 - acc: 0.6560 - precision: 0.6758 - recall: 0.7455 - fmeasure: 0.7074 - val_loss: 0.6752 - val_acc: 0.5714 - val_precision: 0.5806 - val_recall: 0.7129 - val_fmeasure: 0.6400\n",
      "Epoch 35/50\n",
      "753/753 [==============================] - 0s 581us/step - loss: 0.6503 - acc: 0.6295 - precision: 0.6722 - recall: 0.6998 - fmeasure: 0.6751 - val_loss: 0.6817 - val_acc: 0.5503 - val_precision: 0.5494 - val_recall: 0.8812 - val_fmeasure: 0.6768\n",
      "Epoch 36/50\n",
      "753/753 [==============================] - 0s 581us/step - loss: 0.6417 - acc: 0.6454 - precision: 0.6542 - recall: 0.8037 - fmeasure: 0.7185 - val_loss: 0.6847 - val_acc: 0.5714 - val_precision: 0.5685 - val_recall: 0.8218 - val_fmeasure: 0.6721\n",
      "Epoch 37/50\n",
      "753/753 [==============================] - 0s 630us/step - loss: 0.6423 - acc: 0.6321 - precision: 0.6614 - recall: 0.7162 - fmeasure: 0.6784 - val_loss: 0.6983 - val_acc: 0.5397 - val_precision: 0.5412 - val_recall: 0.9109 - val_fmeasure: 0.6790\n",
      "Epoch 38/50\n",
      "753/753 [==============================] - 0s 593us/step - loss: 0.6476 - acc: 0.6268 - precision: 0.6457 - recall: 0.8109 - fmeasure: 0.7091 - val_loss: 0.6817 - val_acc: 0.5714 - val_precision: 0.5735 - val_recall: 0.7723 - val_fmeasure: 0.6582\n",
      "Epoch 39/50\n",
      "753/753 [==============================] - 0s 588us/step - loss: 0.6448 - acc: 0.6414 - precision: 0.6713 - recall: 0.7216 - fmeasure: 0.6896 - val_loss: 0.6910 - val_acc: 0.5397 - val_precision: 0.5422 - val_recall: 0.8911 - val_fmeasure: 0.6742\n",
      "Epoch 40/50\n",
      "753/753 [==============================] - 0s 609us/step - loss: 0.6393 - acc: 0.6375 - precision: 0.6374 - recall: 0.8431 - fmeasure: 0.7231 - val_loss: 0.6818 - val_acc: 0.5608 - val_precision: 0.5726 - val_recall: 0.7030 - val_fmeasure: 0.6311\n",
      "Epoch 41/50\n",
      "753/753 [==============================] - 0s 653us/step - loss: 0.6338 - acc: 0.6481 - precision: 0.6640 - recall: 0.7655 - fmeasure: 0.7091 - val_loss: 0.6794 - val_acc: 0.5873 - val_precision: 0.6075 - val_recall: 0.6436 - val_fmeasure: 0.6250\n",
      "Epoch 42/50\n",
      "753/753 [==============================] - 0s 639us/step - loss: 0.6503 - acc: 0.6016 - precision: 0.6645 - recall: 0.6574 - fmeasure: 0.6421 - val_loss: 0.6854 - val_acc: 0.5503 - val_precision: 0.5513 - val_recall: 0.8515 - val_fmeasure: 0.6693\n",
      "Epoch 43/50\n",
      "753/753 [==============================] - 0s 639us/step - loss: 0.6383 - acc: 0.6375 - precision: 0.6485 - recall: 0.7910 - fmeasure: 0.7050 - val_loss: 0.6943 - val_acc: 0.5556 - val_precision: 0.5535 - val_recall: 0.8713 - val_fmeasure: 0.6769\n",
      "Epoch 44/50\n",
      "753/753 [==============================] - 0s 633us/step - loss: 0.6308 - acc: 0.6388 - precision: 0.6481 - recall: 0.8005 - fmeasure: 0.7131 - val_loss: 0.6814 - val_acc: 0.5608 - val_precision: 0.5703 - val_recall: 0.7228 - val_fmeasure: 0.6376\n",
      "Epoch 45/50\n",
      "753/753 [==============================] - 0s 622us/step - loss: 0.6337 - acc: 0.6560 - precision: 0.6658 - recall: 0.7882 - fmeasure: 0.7198 - val_loss: 0.6818 - val_acc: 0.5556 - val_precision: 0.5825 - val_recall: 0.5941 - val_fmeasure: 0.5882\n",
      "Epoch 46/50\n",
      "753/753 [==============================] - 0s 588us/step - loss: 0.6397 - acc: 0.6321 - precision: 0.6756 - recall: 0.6915 - fmeasure: 0.6758 - val_loss: 0.6829 - val_acc: 0.5397 - val_precision: 0.5486 - val_recall: 0.7822 - val_fmeasure: 0.6449\n",
      "Epoch 47/50\n",
      "753/753 [==============================] - 0s 589us/step - loss: 0.6370 - acc: 0.6587 - precision: 0.6760 - recall: 0.7681 - fmeasure: 0.7170 - val_loss: 0.6989 - val_acc: 0.5503 - val_precision: 0.5541 - val_recall: 0.8119 - val_fmeasure: 0.6586\n",
      "Epoch 48/50\n",
      "753/753 [==============================] - 0s 582us/step - loss: 0.6370 - acc: 0.6375 - precision: 0.6467 - recall: 0.8147 - fmeasure: 0.7139 - val_loss: 0.6817 - val_acc: 0.5661 - val_precision: 0.5941 - val_recall: 0.5941 - val_fmeasure: 0.5941\n",
      "Epoch 49/50\n",
      "753/753 [==============================] - 0s 586us/step - loss: 0.6337 - acc: 0.6547 - precision: 0.6758 - recall: 0.7587 - fmeasure: 0.7092 - val_loss: 0.6856 - val_acc: 0.5556 - val_precision: 0.5669 - val_recall: 0.7129 - val_fmeasure: 0.6316\n",
      "Epoch 50/50\n",
      "753/753 [==============================] - 0s 579us/step - loss: 0.6290 - acc: 0.6587 - precision: 0.6662 - recall: 0.7938 - fmeasure: 0.7225 - val_loss: 0.6929 - val_acc: 0.5556 - val_precision: 0.5659 - val_recall: 0.7228 - val_fmeasure: 0.6348\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(sequences_matrix,Y_train, validation_split=0.2, epochs=50, batch_size=batch_size, verbose=1) #, callbacks=[EarlyStopping(monitor='val_acc',min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236/236 [==============================] - 0s 434us/step\n",
      "Val set\n",
      "  Loss: 0.684\n",
      "  Accuracy: 0.597\n",
      "womp womp -- actually not so terriblez!\n",
      "Average precision-recall score: 0.67\n"
     ]
    }
   ],
   "source": [
    "test_sequences = tok.texts_to_sequences(X_testdf['Smiles']) # keep as dataframe format\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=maxlen)\n",
    "accr = model.evaluate(test_sequences_matrix,Y_test)\n",
    "test_prediction_scores = model.predict(test_sequences_matrix)\n",
    "print('Val set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1], accr[2], accr[3]))\n",
    "print('womp womp -- actually not so terriblez!')\n",
    "\n",
    "\n",
    "import sklearn.metrics as sklm\n",
    "\n",
    "\n",
    "# # for tox!\n",
    "# def print_metrics_binary(y_true, predictions, verbose=1):\n",
    "#     predictions = np.array(predictions)\n",
    "#     if len(predictions.shape) == 1:\n",
    "#         predictions = np.stack([1 - predictions, predictions]).transpose((1, 0))\n",
    "\n",
    "#     cf = metrics.confusion_matrix(y_true, predictions.argmax(axis=1))\n",
    "#     if verbose:\n",
    "#         print(\"confusion matrix:\")\n",
    "#         print(cf)\n",
    "#     cf = cf.astype(np.float32)\n",
    "\n",
    "#     acc = (cf[0][0] + cf[1][1]) / np.sum(cf)\n",
    "#     prec0 = cf[0][0] / (cf[0][0] + cf[1][0])\n",
    "#     prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n",
    "#     rec0 = cf[0][0] / (cf[0][0] + cf[0][1])\n",
    "#     rec1 = cf[1][1] / (cf[1][1] + cf[1][0])\n",
    "#     auroc = metrics.roc_auc_score(y_true, predictions[:, 1])\n",
    "\n",
    "#     (precisions, recalls, thresholds) = metrics.precision_recall_curve(y_true, predictions[:, 1])\n",
    "#     auprc = metrics.auc(recalls, precisions)\n",
    "#     minpse = np.max([min(x, y) for (x, y) in zip(precisions, recalls)])\n",
    "\n",
    "#     if verbose:\n",
    "#         print(\"accuracy = {}\".format(acc))\n",
    "#         print(\"precision class 0 = {}\".format(prec0))\n",
    "#         print(\"precision class 1 = {}\".format(prec1))\n",
    "#         print(\"recall class 0 = {}\".format(rec0))\n",
    "#         print(\"recall class 1 = {}\".format(rec1))\n",
    "#         print(\"AUC of ROC = {}\".format(auroc))\n",
    "#         print(\"AUC of PRC = {}\".format(auprc))\n",
    "#         print(\"min(+P, Se) = {}\".format(minpse))\n",
    "\n",
    "#     return {\"acc\": acc,\n",
    "#             \"prec0\": prec0,\n",
    "#             \"prec1\": prec1,\n",
    "#             \"rec0\": rec0,\n",
    "#             \"rec1\": rec1,\n",
    "#             \"auroc\": auroc,\n",
    "#             \"auprc\": auprc,\n",
    "#             \"minpse\": minpse}\n",
    "\n",
    "\n",
    "average_precision = sklm.average_precision_score(Y_test.values, test_prediction_scores)\n",
    "print('Average precision-recall score: {0:0.2f}'.format(\n",
    "      average_precision))\n",
    "\n",
    "## lets take a look at the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'smiles': array([[ 0,  0,  0, ...,  1,  1,  6],\n",
       "        [ 0,  0,  0, ...,  3, 19,  1],\n",
       "        [ 0,  0,  0, ...,  6,  3,  1],\n",
       "        ...,\n",
       "        [ 0,  0,  0, ...,  1,  3,  4],\n",
       "        [ 0,  0,  0, ...,  3,  1, 12],\n",
       "        [ 0,  0,  0, ...,  1,  3,  1]], dtype=int32)}"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences_matrix[0])\n",
    "X_train = {}\n",
    "X_train['smiles'] = sequences_matrix\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'smiles': array([[ 0,  0,  0, ...,  1,  6, 12],\n",
       "        [ 7,  1,  2, ...,  4,  3,  1],\n",
       "        [ 0,  0,  0, ...,  1,  1,  6],\n",
       "        ...,\n",
       "        [ 0,  0,  0, ...,  1,  1,  6],\n",
       "        [ 1,  5,  5, ...,  4,  3,  1],\n",
       "        [ 0,  0,  0, ...,  1, 10,  1]], dtype=int32)}"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = {}\n",
    "X_test['smiles'] = test_sequences_matrix\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nOydd3hUVfrHPzPphfRGEkhCySGEDoKAiKAgCqJgRde2iquuuruu23R/a1u7q2vvZa0IiGJBBaRXAekJJ4TQQkshQHqd3x9nJkySmWQmySST5HyeJ89k7j33zrkp973nLd/XYDKZ0Gg0Go2mPsb2noBGo9Fo3BNtIDQajUZjE20gNBqNRmMTbSA0Go1GYxNtIDQajUZjE20gNBqNRmMTz/aegEbTkRFCJAL7AS8pZVUTY28BbpdSnteS82g0bYU2EJougxDiABALxEop86y2bwWGAElSygPtMjmNxg3RLiZNV2M/MMvyRggxEPBvv+loNO6LXkFouhofAzcBr5jf3wx8BPzbMkAIEWzefwlQArwDPCmlrBFCeADPALcAZ4D/WJ/cfOwLwKVADfAB8LCUstqZSQohYoE3gfOAk8AzUsp3zPtGAq8DyUAp8KmU8n4hhC/wrnneHsBeYJqU8oQzn63RWNArCE1XYwMQJIRIMd/srwM+qTfmFSAY6AWMRxmUW837ZgPTgKHACOCqesd+CFQBfcxjJgO3N2Oec4BslEvsKuBJIcRE876XgJeklEFAb2CuefvN5nn3AMKBO1EGRKNpFnoFoemKWFYRK4F04Ihlh5XRGCKlLAQKhRD/AW4E3gOuAf4rpTxsHv8UcIH5+2jUyiFESlkKFAshXgTuAN5ydHJCiB7AWGCqlLIM2CaEeNc852VAJdBHCBFhjqVsMB9aiTIMfaSUO4Atzv5gNBprtIHQdEU+BlYBSSj3kjURgBdw0GrbQSDO/H0scLjePgsJ5mOPCSEs24z1xjtCLHDSbKCsP2eE+fvbgMeAPUKI/cCjUsrvzNfVA5gjhAhBrYweklJWOvn5Gg2gDYSmCyKlPGi+sV6Kutlak4d6Ek8A0szbenJ2lXEMdRPGap+Fw0A5ENHCVNWjQJgQopuVkaidg5RyLzBLCGEEZgLzhRDhUspi4FHgUXPa7CJAolY+Go3T6BiEpqtyGzDRfFOtxRxMngs8IYToJoRIAO7nbJxiLnCfECJeCBEK/N3q2GPAYuA/QoggIYRRCNFbCDHemYmZ3VfrgKeEEL5CiEHm+X4CIIT4jRAiUkpZA5wyH1YjhJgghBhodpOdQRm6Gmc+W6OxRhsITZdESrlPSrnZzu57gWIgC1gDfAa8b973DvATsB34FVhQ79ibAG/U6qMAmA90b8YUZwGJqNXEV6hMqKXmfVOA3UKIIlTA+jpzzCPG/HlnULGVlSi3k0bTLAy6YZBGo9FobKFXEBqNRqOxiTYQGo1Go7GJNhAajUajsYk2EBqNRqOxSaepg9i2bZvJx8en2ceXl5fTkuM7Kvq6uxb6ursWjlx3SUlJ3vDhwyNt7es0BsLHx4eUlJRmH5+ent6i4zsq+rq7Fvq6uxaOXPeWLVsO2tunXUwajUajsYk2EBqNRqOxiTYQGo1Go7FJp4lB2KKyspLs7GzKysocGpuent4Gs3Itvr6+xMfH4+Xl1d5T0Wg0HZxObSCys7Pp1q0biYmJGAyGRseWlpbi5+fXRjNzDSaTifz8fLKzs0lKSmrv6Wg0mg5Op3YxlZWVER4e3qRx6CwYDAbCw8MdWjFpNBpNU3RqAwF0GeNgoatdr0ajcR2d3kBoNC6jpgZ+/RiqdcM2TeekU8cg2puCggJuueUWAPLy8jAajYSFhQEwb948vL29mzzHP/7xD2bPnk2vXr1cOVVNc8jeBN/cA4HRkDy5vWej0bQ62kC4kNDQUBYuXAjAK6+8gr+/P7fdVrfDpclkwmQyYTTaXsw99dRTLp+nppmU5KvXslONj9NoOijaxdQOHDx4kEsvvZQ///nPTJ06ldzcXP7v//6PmTNnMnXqVF599dXasbNmzSI9PZ2qqipGjBjB888/z/Tp07n22mvJz89vx6vQUHZavZafad95aDQuosusIL7cks3czYft7q+pqbH7FG+Pa0b04Mrh8c2aT1ZWFs888wwDBw4E4M9//jMhISFUVVVx0003MWXKFPr06VPnmMLCQs455xweeOABnnrqKb788kvuuOOOZn2+phWwrBzKtIHQdE66jIFwN3r27FlrHAC+//575s+fT1VVFTk5OWRmZjYwEL6+vowfPx6A1NRUNm+211JZ0yaUmg2EXkFoOildxkBcOTy+0af9ti6Us/6sAwcO8NFHHzFv3jyCgoJ44IEHKC8vb3CMdXW0h4cH1dXVbTJXjR1qXUyF7TsPjcZF6BiEG1BUVERAQACBgYHk5OSwZs2a9p6SxhG0i0nTyekyKwh3JjU1ld69e3PJJZcQGxvLsGHD2ntKGkfQLiZNJ0cbiDbi3nvvrf0+ISGhNv0VVPXzc889Z/O4zz//vPZ765jD1KlTmTp1qgtmqnEY7WLSdHK0i0mjaS7axaTp5GgDodE0F+1i0nRytIHQaJqLLpTTdHK0gdBomkN1JVQWg8GoXEwmU3vPSKNpdbSB0Giag8W91C0WTNVQWdq+89FoXIA2EBpNc7C4l0J6qlftZtJ0Qlya5iqEmAK8BHgA70opn663/0VggvmtPxAlpQwRQgwB3gCCgGrgCSnlF66cq6u48cYbueOOOxg3blzttg8//JD9+/fz6KOP2jxm6NChbN26ta2mqGkOlgymkB5wCOVm6hbTrlPSaFobl60ghBAewGvAJUB/YJYQor/1GCnln6SUQ6SUQ4BXgAXmXSXATVLKVGAK8F8hRIir5upKpk2bxqJFi+psW7RoEdOmTWunGWlaBYuLKbiHetW1EJpOiCtdTCOBTClllpSyApgDXN7I+FnA5wBSygwp5V7z90eBHCDShXN1GRdffDErVqygoqICgOzsbHJyckhJSeHmm29mxowZXHbZZSxdurSdZ6pxCusVBED56fabi0bjIlzpYooDrPW1s4FRtgYKIRKAJGCZjX0jAW9gX2MfVl5eTnp6ep1tlZWVlJaq4KHHrrl47Pjc1qFg/gBnpe+qB82iesA1jY7x8fEhNTWVpUuXMmHCBBYuXMikSZMwmUw8//zzBAYGUlBQwE033cSYMWMwGAyYTKbaeTeHysrKBj8Le5SVlTk8tjPR0usOObCH7sChM9ATyN6XTmFF91abn6vQv++uRUuv212kNq4D5ksp69yjhRDdgY+Bm6WUNY2dwMfHh5SUlDrb0tPTz6qmenmD0cPu8dU11Xg0st8WHl7e4IAC7PTp01m6dCmXXnopixcv5oknnsDX15cXX3yRTZs2YTQaycnJobi4mMjISAwGQ4uUZb28vBr8LOyRnp7u8NjORIuvO9cXgJ4Dz4OVEB8RBB3g56h/310LR657y5Ytdve50kAcAXpYvY83b7PFdcDvrTcIIYKA74GHpJQbWjybIbPUlx0qXCj3feGFF/LUU0+xe/duysrKGDBgAAsWLODkyZMsWLAALy8vJk6caFPiW+OmlJ0GT18IiFDvdRaTphPiSgOxCegrhEhCGYbrgOvrDxJC9ANCgfVW27yBr4CPpJTzXTjHNiEgIIBRo0bx4IMP1grsFRYWEh4ejpeXFxs2bODIEXu2U+OWlJ0C3xDw6abe6yC1phPisiC1lLIKuAf4CUgH5kopdwshHhNCTLcaeh0wR0ppXYp6DXA+cIsQYpv5a4ir5toWTJs2jT179tQaiMsuu4xdu3Zx2WWXsXDhQnr16tXOM9Q4Rekp8A1WbkvvQC3Yp+mUuDQGIaVcBCyqt+1f9d4/YuO4T4BPXDm3tuaiiy5CSln7PiwsjC++sF3aoWsgOgBlp8DPnHnt0027mDSdEl1JrdE0h7LTysUE4BOkDYSmU6INhEbTHCwuJgDfIO1i0nRKOr2BMHUxlc2udr3tRgMXkw5SazofndpA+Pr6kp+f32VumiaTifz8fHx9fdt7Kp2bmhq1YtAuJk0nx10K5VxCfHw82dnZ5ObmNjm2srISLy+vNphVK2AygcFgc5evry/x8fFtPKEuRvkZwKRdTJpOT6c2EF5eXiQlJTk0tsNUWi57AjJ+hDtXt/dMui4WHSY/6xWEdjFpOh+d2sXUKTm4Do7vgPKi9p5J18XSC8LaxVRZDNVV7TcnjcYFaAPR0cgz11LkZ7bvPLoyFqlvi4vJUk1doVcRms6FNhAdiZKTUGyOp+Ttbd+5dGXqu5h8g8zbdRxC07nQBqIjkZdx9vt8bSDaDVsuJtCZTJpOhzYQHYlcs3vJy7+usdC0LaX1g9RasE/TOdEGoiORlwEePpAwVruY2pOyU2Awi/SBdjFpOi3aQHQk8jIgoi9EChWkrnG2B56mVSg7rQLUlloUH3OwWruYNJ0MbSA6ErkSIpLVV1UZnD7c9DGa1qfUSmYDrFxM2kBoOhfaQHQUKkvh1CG1eohIVtu0m6l9KLMS6gPtYtJ0WrSB6Cjk7QVMZ1cQoAPV7YW11Deo1qNGTx2k1nQ6tIHoKFiMQUQyBISDX5heQbQX9V1MBoMW7NN0SrSB6CjkSjAYIbyPeh+RrA1Ee1HfxQRasE/TKdEGoqOQlwEhCeBllvKO6KtdTO2BydTQxQS6J4SmU6INREchL0MFqC1EJENxDpQWtN+cuiKVpVBdUdfFBCrVtS1dTCYTpH0DVeVt95maLoc2EB2B6ipV92AJToNaQQDkadG+NqWsnlCfBZ9ubetiyt0Dc2+E9G/b7jM1XQ5tIDoCpw6qp9b6KwjQbqa2pr4OkwXfNg5Snzla91WjcQHaQHQELBpMEVYGIiQBjF7aQLQ19XWYLLR1FlNRjnotzmm7z9R0ObSB6AhYekBY3EoAHp4Q3lv3hWhrGnMxlReq2EBbYDEMRdpAaFyHNhAdgby9EBjd8KlVZzK1PY25mGqqVBC7LSjSBkLjerSB6AhYNJjqE5EMJ7OgurKJ4zPa7sm2s1PrYgqtu72te0JYGkdpA6FxIdpAuDsmU8MUVwsRyeqpteCA/eOPbIHXzoEdc102xS6FxcVkMQgWag1EG9VCFJ1QrzoGoXEh2kC4O4XH1VNphC0DYUl1bcTNtGuBet36cevPrStSegq8u6kYkDVtLdhXZF5BFOepNGiNxgVoA+HuWALUkTZcTOFNGAhLMZXBAw6shoKDrpljV6LsdMNYELS95Hdxjvq9YoKS/Lb5TE2XQxsIdyfXItJnYwXhGwSBMfY1mY5uhdOHYPxf1fsdX7hmjl0JWzpM0LYxiOoqtXKwxKUs7iaNppXxbHpI8xFCTAFeAjyAd6WUT9fb/yIwwfzWH4iSUoaY990M/NO8799Syv+5cq5uS16Gcml0i7G9P6KvfQOR/o2SoR55BxxYA9s+g/P/crYTmsZ5Sk81zGCCtnUxleQDJogZALnpOg6hcRkuW0EIITyA14BLgP7ALCFEf+sxUso/SSmHSCmHAK8AC8zHhgEPA6OAkcDDQoh6aSNdhDyp3Ev2buoRycqI1M9SMpkgbSEkjgP/MBhyPRTsh0MbXD/nzkyTLqY2CFJbDEL0APWqM5k0LsKVLqaRQKaUMktKWQHMAS5vZPws4HPz9xcDS6SUJ6WUBcASYIoL5+q+5GbYdi9ZiEhWbo/ivLrbT+xWKbD9zT/ylOngFQDbP3PdXLsC7uBishiEGG0gNK7FlS6mOMC6aXI2akXQACFEApAELGvk2LjGPqy8vJz09PRmT7asrKxFx7sCY0URoug4OaYQ8u3MLaDUl57AgS1LKI0aWrs9Yud7RBiM7PVIptp8bPe48XTb+SV7k36LyVPJhrvjdbcFzb1uUXKSgpJqcmwcKzz9KDi63+a+1iR4/zZigcyT1fTy9KPgULrDn6l/312Lll63S2MQTnAdMF9KWd3cE/j4+JCSktLsCaSnp7foeJdweBMAUf3HEdXPzty6B8JKSAwoB+v5L1sHCWNJHjr27Dbfu+B/i+jHPki5CnDT624DmnXd1ZVQVUp4XG/CbR3rF0J4gJftfa3JycUA9Bk8BtZHE+5T7fBn6t+3G1Fd1TBdupVx5Lq3bNlid58rXUxHgB5W7+PN22xxHWfdS84e23mpTXFtxMUUFAde/nUD1blSyUGnTK87NuE8CO4J2z5t/bl2BezJbFhoK8G+ohzw9APvQCXBol1MHY99y+DpHnDmWHvPpFFcaSA2AX2FEElCCG+UEfim/iAhRD8gFFhvtfknYLIQItQcnJ5s3ta1yMsAD2+l3GoPo7kNqXUtRJr5x5xyWcOxg6+FrBWNy0TvWw57Fml5jvqU2hHqs9BWXeWKcyEwUiUuBER2bAOxbxl8eg3UNNt50DHZtxwqS+DI5vaeSaO4zEBIKauAe1A39nRgrpRytxDiMSGE9aPtdcAcKaXJ6tiTwOMoI7MJeMy8rWuRmwFhvZtehkb0hXyrFUTaQugxCoK6Nxw7eBaYauzXRGz7DD6ZCXNmwUfTIWdP8+ff2bCsIGxlMUHb9aUuOqFWDqBeO3Ka6/Y5sPenrtdf/dh29Xp8V/vOowlc6gCTUi4CFtXb9q967x+xc+z7wPsum1xLqKlRN1kX+w/JkxAzsOlxEclKUqOyDM4cgRM74eInbY8N7w09zoVtn8PYP9bdt+ld+P7P0GsCiEtg+RPw5lgYdSeM/9vZXP+uSpm5vWtjLqbT2a6fR1EuhCaq7wOjVF1EdSV4eLn+s1sbS9r18Z0Q1a9959JWmExnDcQJ9zYQupK6OXz1O3j7Atdq4FSWKRG+xlJcLUT0BUxwcp8qjoOG7iVrhsxSxufor2e3rXtFGYfkS2DWHBj1O7j3VxhyA6x/DV4ZroxKTU1Lrqpj4zYuphzlYgJlIKBhmnNH4Mwx1S0R4Pj29p1LW3LqoEqXNnoqw+jGaAPhLMd2wM656il92yeu+5yT+9QqpbEAtQXr9qNpCyF2GIT0tD8+dQZ4+ip3kskEK5+Fxf+E/lfAtR+Dl0qBJSACpr8Ms5ep8319J3x2TdeNTTTpYgp2vYupplqtGALMhsHy2hHlNg6bVw9eAW5/o2xVLKuHvhebjUUbdiJ0Em0gnGXFU+pGEDsUlj8FFcWu+ZzaNqM2RPrqE9YbMEDmz0p/qf/0xsf7BkO/qbBzPlHbX1WupMGz4Mr3bLsp4obBbUuUTEfmEji+w+nL6RTUdpOz52LqBpXFrl1ZFuepBwfLysESi7D0h+hIHNqgsrH6T1cGoqs8eBzbrlYPg65R73PS2nc+jaANhDMc2QJyEYy5F6Y8DUXHYcPrrvmsvL2AQWUoNYW3P4T0UAE/aJjeaosh10PZKcLlpzDit3D5643HVIxGOPdupSC6+yuHLqHTUXoKPHzOrrDqY6mmrnChm8kSkK41EGZXU0dcQRzaAPEj1Iq3JL/xzLrOxNFtEJmirh3cevWkDYQzLH8S/MJU0LbnuSCmwpqXoNgFcst5Ut30vf0dGx+RDDWVED1QBaKbotcESDqfvJSbYeoLygA0hX8Y9LpABcS7ytOeNfZ0mCy0hWCfJaW1gYupg2UylRepG2OPUdB9kNrmTjfK8iLXnNcSoO4+WNUw+Ya4daBaGwhHObQRMpfC2D+cFWa76GHlUlj1XOt9jskEv34MGT+pm72jWFxR/RuTu7LC6AE3f0vuoLucU3dNnaH8pke3On5MZ6HMjpKrhbYQ7Cuqt4Lw9ldqvx3NQBzZDKZq6DkaolMBg/u4Lo/tgGcSlAJya3PmKJTkKQNhMCjBRTdOddUGwlGW/1sVJY2cfXZbpIChv1HpoSf3N358ycmmn7pPHVI1CN/co/6Apjzl+PxiBoLB6LiBaC4p08Do1TXdTKV2hPostIVgX30XEyg3U0erhTi0ETBAj3OUYQ3r5T4GYvvnqpXv3sWtf+5j29Rr7BD1GjNAxSDctFBQGwhH2L8a9q+C8+4H74C6+y54UAWclv3b9rEVJfDNffBsErw0SGULHdlS11jU1Cgj8/po9Y9z6fNw83cQ2kgFdX0GXgN3b7Tdea418QuF3hNg99ddz83UlIvJp41cTBaZDQsdUW7j8Aa1crAY3JiB6sm9vamphl1fqu8Prmv98x/brh7kolPV++gBqqK6qQfMxig8Aaddo0SkDURTmEwq9tCtO4y4teH+oO4w+m7YNV8Fn6w5sRvemQC/fgTDb4HIfrDhTXhn4lljsXepqlj+/s8qaHX3erVKcSQmYI2Hp+uNg4XUGapT3RH7Il+dkqZcTJYYhCtdTNYyGxY6mtxGTbUSouxhJe7cfZByXVpqTdqL/atUwD9CKDdqa2cpHtuu3MGWB02LZPuJZsZfTCbldfjxb60zv3poA9EUWcvh0DoY92fw8rM9ZuwfVPB66cPqvcmkVgTvTITSArjxK7jsJbhhHvxlr8oYshiLT69UfzTTX4Ebv3Zu1dBeiEuVRlRXczM57GI67bo5WMtsWAiM7lhZTCd2q0yvnuee3RYz6Oy+9mTnfBXTufD/lJspe1Prnv/oNug+5Oz7yBSVGdjc6z6wRgW5+05unfnVw13kvtuX0lMYqsoabjeZYNkTEBQPw26yf7xvsKoR+Okfanm6+ytI/xZ6Xwgz3jqbigjKRTP0BvVVWqB+wXHDISi29a/LVfiFqGvb/RVMetz51U5HpKZGxRYadTGZg9QudTFZyWxYCIxSq5uqcvD0cd1ntxYWeQ1bBuL4Dkgc2/CYtqCyTCkRpFwGSeOVK+jgOpW51xoUHlep8d0Hn93m5auUEJobqP7lLXVPGXh168yxHtpA1NTAfwciKophdT8VPOo+RBXCFR5V2RaXvdT0P945t8HGN2D+b1VMYtLjMPqexm+efqGNS2K4M6kzIOMH9YTV02YfqM5FRaEqUGvMxeTlp373LnUx5ajArjW1chu5EBzvus9uLQ5vgG6xEGyl6N8tWqXsNpXqajLB+leh3zQIS2rdee1drB4CBl2t3IUxg1o3DmGJsVgbCFBxiMMbnT/fqcOw53tVl2XPu9FCusCjXxMYjXD9F+Sn3KjiCRk/wQ9/gfcugrk3KantITc0fR5PH7j0P0oI77eLYex9nfvJWlyiisa6ipupKR0mUHEBV/aEqC+zYaGj1UIc2qgeKuqnVzsSqD76q4rdrX6+9ee1c576WSaer94njFUPQFXlrXN+i8RGfQHOmAFw+rDyKDjDZrOW6YjbWj43O+gVBEDCGHJLQolISVFPKGeOKF/h8Z3Qe6LjKpnJk9VXV8A3CPpOgrSvlXJsZzaG0LQOkwVXCvbVl9mwYIlJdAQDceownMmGnvc13Nd9EKx7FaoqwNPb9vGWB5L0b2Hqi/bHOUvZafVwOOLWs4oCCWNgw2sqWG3tDmsux7YpZYT6qsiWeqcTuyHxPMfOVVkGv/5PiWu6MG7Zyf+rm8ZkMvHX+dvZnWOOQRgMapmeMg0m/KNruE+aS+oMKDx2VnStM9OUDpMFV/aEsFUDAWdjXB2hFsLiSulh4/8qZqBSA8i100PZZILdC8E/XN3Qs5a33rzSv4Xq8rq+/J6j1evBta3zGZYK6vpYUl6diUPs+lKtJkfd0Tpzs0OXNxAGg4Fth0/x5IoT5Be10lKyq5B8sVKFbczNlLfXbYuAnMIRFxO41sVUX2bDQkdSdD20QdVwRA9ouC/GfPO0F4c48qtKr574f8pQ71rQevPaOQ9Ck1TCiIWAcJVl1BpxiOJ85UayZSC6xSij52iqq8mkgtOR/VQw3YV0eQMB8OK1QzhTXs3fvtyBqasVf7UEn24qvS5tYUMjcDob5twAr46Ar+/q+H0kHHYxudBAWBRb668gvHzBJ1hlOLk7FoE+W8KQYb0al/7evUBV8adeoVb4e75XrpaWUnhc1T8MvLphXCRhjIqZtPQhx1JBbZ3iasFZyY3Dv6jVyMjZzsnkNANtIIDU2GB+Ozycpek5fLzhYHtPp2OROkM9uVqesqqrlB/51ZFKfrzfNNXedPE/O3bltTu4mCwrhPoGwrLN3VcQZachZ7dK5LCF0agCtrYC1SaTehDpPVFl/6XOVJllmUtbPq/dX6nYzsCrGu5LGKM+p6VCgpYAtUWYsD4xAyF3j2NS8b+8rR5EBl3Xsjk5gDYQZq5ICeICEcm/v09nz3H3beDhdiRfDF7+6p8se7PqtLf4IRVs+/1GuPYTGHWXCvat/W97z7b5lJ5SefHWEhe2cGWQ2pbMhoXAKPfvCZG9Sd2IGwv4xgxUN+P6K84jW5SLJvUK9T7pfFWcursV3Ew75qqUVlvNuRLGqNeWupmObVcZkX6htvdHD4CqMtUorDEKj6vEkCE3gE8Tf4utgDYQZgwGA89fPZggXy/u+3wrZZWdwG/eFngHKCOx9RN49yIVOLvmY7j+C5VdYTCoLKeBV8PSR5TsSEekzFxF3VS2lsXF5IrVki2ZDQsdYQVxaKMyspY+CLaIGaSe2E8dqLt991fKvSQuVe89vFSjIfmj0jtrLvn7VOqsvUKzoFhVmNjSQPWxbWcF+mxhkdxoaqWy+QNV4W0tGupCtIGwIiLQhxeuGUzGiSKe+N5OJoWmIUN/o6Sbz70L7vlF/eNa38SMRiUv0uci+PYPkP5d+821uZSdbtq9BMrFVFMFlaWtPwdbMhsWAqLcPwZxeIN6UrZUnNvCUiNgfaM0mZQ4ZJ8L68aAUmcquf2WqK7unA8YYMCV9sckjFUriOYa/dIC1V/eVoDaQoRQBrCx3hBVFbDlA+gzybGeL62ANhD1OD85ktnjkvh4w0GWpLn5E5m70OcieOiEkie398/v6Q3XfKS6h83/LRxopdTBtqL0VNMBanBtT4ii3IYZTBYCo5QGVGsEbV1BdaVyQVpSR+0R1V9pE1nHIbI3q9qJ1Bl1xyaep34ezXUzmUwqeynxPAiOsz8uYQyUnjzbBthZLMauMQPh6a1cXI0FqtMWqoeEUb9r3jyagTYQNvjLxf0YEBfEX+dv5/hpN/2Hczcaa1dqwTtACRaGJsLn16l//NbCZFKphIc3KZ/yimfgm3ub/09dH4uLqSl8zGNckclUnFNX18uaWrkNN1zmlRIAACAASURBVK2FOL5TyVo3VVfk5Wu+UVqtIHZ/pcQhxSV1xxo9VP+TjMXN6wB3bBvk77UdnLamNg7RzIeao41kMFkTPcD+CqKmWsXxwnorHbQ2QhsIG3h7Gnn5uqGUV9Vw/bsbyDjhQm2droZ/GNy4QLlr3psEPz7Y/Kft8iLY+Ba8PQGe7gnP9VISKQtmw4onYeunKu7RGjjqYnKVYJ89mQ0L7l5NXVsg50BFsiVQDSpYnbZQ3RRtGegBM6GqFDJ+dH5O619XQf+meriHJim5/+YGqo9tV4KfARGNj4tOVYWntloYL/mXquge/9c2VS3QBsIOvSID+eCWczhTWsXlr67l662uacjRJQmOhztXqR4ZG15XKbFp3zju4y08Dj8/Bi+mwg9/BUww6Fq4+CmYNQd+/4tyeZ3/AMhFkJvR8jk76mKq7QnRygbCnsyGhQDzysIdDUTJSdVGN6Rn464cCzGDlFBmcZ4Sy7TlXrLQ41x183a2aO5EmnIvjbpDPbQ0hsGgVhHNjUPYq6Cuj73eENs+UwKFI++Awa5PbbVGG4hGGNUrnEX3ncfAuGD++MU2/vn1TsqrdHZTq+AXCtNehNuWqCrSuTfCZ9eqYJ49ctLh69/DfwfC6hcgaZwSRrxjBUx9XjVuEpcoF4WXL5wzW1V6r3+lZXM1mZxwMbnKQNiR2bBQu4Jws7hZUQ58OA3yM1WnREeoDVTvsHIvTbE91miE/ldA5pKzxYyOsOJJlS489o+OjU8Yo4xWY3+ftigvVNfeWAaTBWtNJguHNqrEjqTx6gGojdFifU0QFeTLZ7NH8dxPkrdWZbEz+zSv3TCM+FD/9p5a56DHOeoG/8tbqvfGa+eqtNnKUnWTLTuj/vHLz6gvTz8YdrPKmGoqkyMwEgbPgm2fwoR/Kknp5lBVBtUVzrmYWjtIbU9mw4JlBeFOtRCns+F/05Xb5Ia5jvdVsBiIY9vN2UsXNW6cB8xUUvt7FsGQWU2f/+hWpb00/u9Nrx4sJJh7VBxc55zM+PGdgMmxFURgpDL0lkD16Wz44jdqxX31h47F+VoZvYJwAE8PI/+4NIW3bhxOVm4xU19ew4JfszldUtneU+sceHjC6N+rFFkxRQUPi46r3gphSaooasgNMPkJuD9NrRYcTfMbfY/KoPnl7ebPz6LD5IyLqbVjEPZkNix4eqtVmbusIE5mwfuXqHnf+JVzTXf8w1SviC3/U0/t9txLFuLPUeMdzWZa9oQy9qPvdnxOEUIV5jkTh6iqUC5OcMxAgDlQvVO1Ov18lno4mTXHcUPWyjhkkoQQvYFsKWW5EOICYBDwkZSynRvIti0Xp8Yg7u3GXZ/+yv1zt2M0wIC4YMb0jmBsn3DOSQzD18ujvafZcbE8KbUmEX2g31TVAva8PzWv+rRWZsMBF5O3ZQXRygaiMZkNCwFR7hGDyNkDH12uVl03f+uYe6U+MYNAfq96jiTbcS9ZMBhUhfWGN1S8o7Gb6aENyh110SOO/T4tGI3mOEQTmUwmkwrI7/hCucdKCyBuhBLkc4SYAeo6vrpTrT6un2u7wruNcHTN8iUwQgjRB3gbWAh8Blzqqom5K4kRAXxzz1i2HjrF2sw81u3L493VWby5ch/enkaG9ghheEIoQ3qEMLRnKJHdOkALyM7OmPtgz3fK1dScHHKLb9sRF5OHpxKcc4WLyZ7MhoVANzAQR7fBJzPV6u/WRRCV0rzzxAxUBqLPRQ37J9gidSase0X9nhtrD7zs38qQjmyGTHbCGHX+M8dUczFQBqEoBwr2w94lsHMunDqkflf9pqrkid4THP+M6IHKsKZ/A5Mea/f+Mo4aiBopZZUQYgbwipTyFSHEVldOzJ3x8jAyMimMkUlh/GlSMsXlVfyy/yRrM/PYuP8kb6/KoqpGZTvEh/oxtGcoIxJCufacHnqF0R70HKX6D6x/VXXfctaX64yLCcyCfU4ETB2hMZkNC4FRyr/eXlSWwmfXKG2umxa2rNrX4pJpyr1kIXYohPdV6aDdYqHvRQ3HZK2AA6thyjOqJsdZLPUQ3/5BvZ46CAUHVZotKBmRXhNgwkPKODRWMd7YdYAS4htjo6lSG+Pof0qlEGIWcDNgaaLcZJs1IcQU4CXAA3hXSvm0jTHXAI8AJmC7lPJ68/ZngamoOMkS4A9SSreUAw3w8WRCvygm9FPL/7LKanYdOc3WQ6fYeriAzQdO8u32o3y/8xjv3DSCYD8HO9RpWo8x98EXN6gnswEznTvWUSVXCz7dXOBiyrEvs2EhMLp9VxBbP1GusFu+b7kURN/JcOV7jhsIg0Hpf31xI3x6FVzwdzjfqmbAZFKrh6B41TWuOUQPNOsyrVOv4X3UCic0UQnxxQ5p3AXoCBF9YPYy5WJzsZS3IzhqIG4F7gSekFLuF0IkAR83doAQwgN4DZgEZAObhBDfSCnTrMb0Bf4BjJVSFgghoszbxwBjUbEOgDXAeGCFoxfWnvh6eTAiMYwRiWd9oQu3HeGBedu55s31/O+3I4kJ9m3HGXZBxCWqCnXdy47fdEBJV+xaoJ4O/cMdO8YnyDUuptDExscEREJFkQpwNucJuSVUV8Lal1RdgiXjpyV4eDZd4Vyf8N5w+1L4/n5Y8ZTqmzDzHQgIJ/DYWqUme9lLqn98c+d071Z143blzdu6aVE741AWk5QyTUp5n5TycyFEKNBNSvlME4eNBDKllFlSygpgDnB5vTGzgdeklAXmz7E8/pgAX8Ab8EGtVtwkPaN5XD4kjg9uGUl2QQkzX19LZo6uzm5TjB4w5h7lgjmwxrFjys6op9G9P8GUp510MbmgDsKezIaF9qym3jlPyXGP+3P7Pvl6+8MVb8C0/yp30tvjIXszkTvfVhXRQ25o2fmNRrd4sm8rHM1iWgFMN4/fAuQIIdZKKe9v5LA44LDV+2ygvhBLsvn8a1FuqEeklD9KKdcLIZYDxwAD8KqUslF51fLyctLTm6/AWlZW1qLjHSEceHpyDP9aepwZr63h0YkxpES170qiLa7bXTD4DKWPTyili5+ibOQTjV63R1kBPVb9Cd9Tezk66hHOBJ0PDv6c4srBpzCPrNb6udZU068kn7xSI3mNnDOgoIyewIHdmyiNsK0h5pLfd001vX5+GlNIX/ZXxTv8c3IpAefiO/Et4tb+A693L8IXE0dGPcyZjMz2nlmb0tLft6MupmAp5RkhxO2o9NaHhRA22j416/P7AhcA8cAqIcRAIAJIMW8DWCKEGCelXG3vRD4+PqSkNDNjAkhPT2/R8Y6SAgxOKeGm9zfy4NLjvDprGBf1b2YBVyvQVtftNpy8m27LnyCo/Bi9h9pJwjt1GD7+DRRmw6zPiUu+GAcEIs6SEQenW/HnWpQDphoiE/sT2dg5QypgFSRG+IGdcS75facthMKDcNUHpPTv37rnbhEpMPQC+OZeivOPEHfxH4gzdq0kEUd+31u2bLG7z9FCOU8hRHfgGsBRMf8jQA+r9/HmbdZkA99IKSullPuBDJTBmAFskFIWSSmLgB+AJnSCOw49w/2Zf9cYkqO7ccfHm/l0o25z2maMuA08/UhYfjd8ebsqxjqZdVZjJ2cPvDdZSWvf+LWq6nYWn1Z2MTlSAwFnq6zb0sVkMsHq/6j4Tv/6HmQ3wD8MrvuUQxPfUG5GjVM4uoJ4DPgJWCul3CSE6AXsbeKYTUBfc0D7CHAdcH29MV8Ds4APhBARKJdTFtALmC2EeArlYhoPdOB+lQ2JCPTh89nn8vvPfuWhr3ZxIK+Yv1+Sgoex6/g324WAcJj1OcUrXyM4a6XynYPKbkkcq5rPeHirHH6LeJqz+AapRjY11a1zU2pKZsNCQARgaFsDse9nJYkx/VV9A+6EOGQgpJTzgHlW77OARlowgblu4h6UYfEA3pdS7hZCPAZsllJ+Y943WQiRBlQDf5FS5gsh5gMTAbOQCT9KKb91/vLcmwAfT969aQSPf5fGO6v3cyC/hJeuG4K/d8eSyDKZTOQWlhMV1EEys3pP4GhFDMH9+kFeBuxfpQKamUtVoHfW5xDWq/nntxbss9eD2Bmaktmw4OGlnpjbsifE6hcgKE4VhGk6HY4GqeOBV1CppwCrUXUJ2Y0dJ6VcBCyqt+1fVt+bgPvNX9ZjqoG2a5vUjnh6GHn08gEkRQTw2HdpXP3met67+Ryn0mCra0y8tyaL5XtyeeM3wwjx93bhjBvy5a9H+Ov87Xx773mkxjohX9DeGAxKxiBSqB6/FjdTS7NUrHtCtIaBcNTFBG0rt3FwvZKemPKM0oLSdDocjUF8AHwDxJq/vjVv07QSt4xN4t2bR3Agr5jLX1vDriOOVeLuPVHIzDfW8eSiPazPyuf1FftcPNO6VNeYeG15JjUmmLe50ecF96e18ttre0LUS2WuLIMf/gY/PeRcBzRHZDYstKXcxpoXVG1IY9IWmg6NowYiUkr5gZSyyvz1IdBEUrbGWSb2i2benWMwGgxc89Z63l+zn8MnS2yOrayu4bXlmUx9eQ2H8ot5edZQrhoez4frDnDkVGmbzfmn3cfZn1dMTJAvX287ovtlgJXkt1Wg+sxR+PBS2PgmrH8N3hgN+5Y7dj5HZDYsBEa1jaLrsR0qXnPu3ar2QNMpcdRA5AshfiOE8DB//Qaw0RdP01L6xwax8Pdj6RfTjce+S2Pcs8u58D8reOzbNFZl5FJWWU36sTPMeH0tz/0kmdQ/miX3j2f64Fjun5QMwItLWqGDmgOYTCZeX5FJUkQAT84cwKmSSpalu4GaaHtT25favII4vEm1Rc2VcO2ncOsPKhD+8RWqb3ZTuk2OyGxYCIxWBqU5nc+cYc0LKtZyzu2u/RxNu+JoNPS3qBjEi6ig8TrgFhfNqcsTFeTLl3eNYV9uMSszclkhc/hk40HeX7sfXy8jVdUmQvy9eOOGYVwysHvtcbEhftw6JpG3V2dx+7gk+sU4oILZAtZk5rHryBmenjmQ8clRRAf5MH9Ldp05dUmse0Js/QS++xMExaq+CNHmOoE71yg5iHWvwN6lqrueva5pjshsWAiIhMoSJbnRHLG4pigvVJpGu7+G8/7oeHW5pkPiaBbTQVQldS1CiD/SyVJP3QmDwUCfqED6RAVy23lJlFZUsyErnxUyB6PRwH0T+xIa0DAweNcFvfn8l0M8+6Pk/VvOcekcX1++j+ggH2YMi8PDaGDG0HjeWZ1FTmEZUd06SEaTK7DcmFc/D7l7VLOcqz6o26fAy0/JOfe/HBbeA59fqxQ8L32uobx1cY7qvOcI1nIb9Q3E8Z2EZH4LBUuVXpP1l0+gykSKP8e+K2vPIlj0gHKXnXO7EsPTdGpakk95P9pAtBl+3h51FGPtEeLvzd0T+vD0D3vYkJXPub0cFJhzkq2HCliflc8/p6bg46ny368aHsebK/excOtRZp/fgjTRjo4lzTV3j/LRT3rcvsR43HC4Y6UyJqueg+xf4Kr3z8o+11RDSX7TNRAWLHpNRTlnFVWLcuDnR2Hrp3THyvXk6adE/bwDoDhPNVWK7KeCzoOuUzUjoAzCD39VbTqj+sPV/3PcYGk6NC0xELqiy025ZUwiH649wNM/7OGru8dgsPFEuDP7NI98uxvvmnLu9YpkdO9wm+Ps8caKfQT7eXHdyJ612/pEdWNIjxDmb8nm9nFJTp2vU+Fl7pudMAYGX9f0eE9vmPCgWml8eTu8Owkm/1s1NyrJB1ON4zLSlhVEcY5qefnLW7DyWdWrYcw97A2bSN8BI5RRsC5sKy9UHdB+/Qh+ehCWPgL9pimX2JqXoKYSLnwYxtyr6i00XYKW9KR2y94MGiU3fv+kZLYdPsVPu4/X2WcymXhvzX5mvrGWwydLSMst4/p3N3Lxf1fx6caDlFRUNXn+zJxCFqed4OYxiQT61H3GuGp4PPJEIbuOtLKaaUfCYIDpLztmHKxJGKNiE30uhB//BnNuUKsQcNxAWFYa6d/CG2Ng8T9Vs6S7N8Dkf1MV0F25sOpXPft0UyuH25fCXeuUJEnWchVviB8Od6+Hcfdr49DFaHQFIYQoxLYhMAB+LpmRplW4cng8767J4tkfJRemROPlYeRkcQV/mbedn/fkcFFKNM9dNYhD+/eytzyYD9bu56GvdvHMD3u4bmRPbh6TSFyI7V/xGyuy8PPy4JYxiQ32XTYolse+S2P+lsMMjO9ARXPugn+YalK/4Q3VHW3fMrXdURdTQITqXbFzntJHun6u83pS0alwydOqb/PJLNU2tKuuBrs4jRoIKaUL0iA0bYGH0cBfL+7H7R9tZu7mw/SJDOQPc7ZxsriChy/rzy1jEjEYDBz3MHLV8HiuHBbHloMFfLjuAO+t2c8Ha/dzw6gE7p7Qu07A+cipUhZuO8KNoxMIsxEkD/b3YnL/aBZuP8qDVvEJjRMYDDD6buh5Lsy/FQoOQLCDerJGDxj3gFoRjPpd85vjAHj5ns260nRJOpboj8YpLkyJ4pzEUJ5atIeSiioSwgNYcPMYBsQ1fLI3GAy1XfCOnirllWWZfLzhIF9sOsytYxP53fm9Cfb34p1VWQDMHmc/CH3V8Hi+23GMZek5bpnyeii/hFNlHaCgL24Y/G415KRBSM+mx1uY+JDr5qTpUrQkBqFxcwwGA3+/JIXyqmouHxLHt/eeZ9M41Cc2xI+nZg7k5/vHM6l/NK+v2Me4Z5fx4pIM5mw6xBVD44i1434CGNc3srYmwt0wmUxc+/Z6/rOmgxT0+QaplYRG0w7oFUQnZ3hCKNsfntwshdjEiABenjWUO8f35vnFkpd+3ovBAHeObzyF1Z1rInYfPcOx02WcOIPbzU2jcTf0CqIL0FL58P6xQbx/yzl8eddo3rhhOH2img5NXTU8juoaEwu3Hm3RZ7c2KzOUdHaNCb7bfqydZ6PRuDfaQGgcZnhCGFMGxDg01romwuRqXSAnWClzSY0Nok+YN19vq9/gUKPRWKMNhMZlWGoiNh8saO+pAHC6tJIthwq4QEQyoVcgO7JPsy/XCdltjaaLoQ2ExmVcNjiWmCBfZn+02eH+Fq5kXWYe1TUmxidHMT4pEKMBFm7VqwiNxh7aQGhcRrCfF1/87lwCvD2Z9c4GtrTzSmKFzKWbryfDeoYQ7u/J2D4RfLXtiFu5wDQad0IbCI1LSQgPYO6dowkL8ObG9zayfl/7tBExmUyszMhlXN8IPD3Un/3lQ+I4fLKUXw+5hwtMo3E3tIHQuJy4ED/m/m40sSF+3PLBL7WZRG3JnuOFHD9TxgXJZyUrLk6NxtfLyNdulmml0bgL2kBo2oToIF++uONcekUGMvt/m1mS1gZtMa2wGKXx4myn3G6+XkzqH8N3O45SUVXTpvPRaDoCulBO02aEB/owZ/a53PTBL9z1yRYuHhCDr6cH3p5GvD0MeHsa8fIwMqRHCJNTHUundZQVMod+Md2IDqpbGHfFkFi+3X6UVRm5XNTfwbaeGk0XQRsITZsS7O/FJ7eN5G9f7iDt6BkqqmqoqK6hoqqGyuoayqtqqK4xcfXweB69PLXFRX4AhWWVbD5QwO029KPOT44k1N+Lr7cd0QZCo6mHNhCaNqebrxev3zDc5r6q6hpe/nkvryzPZOvhU7x2/TBETMtEhddm5lNVY+ICK/eSBS8PI5cNjuWLTYcpLKukm6/ud6DRWNAxCI1b4elh5P7Jgk9uG8Wpkkqmv7qGz3851KJU1JUZuXTz8WR4QqjN/ZcPiaO8qoYfdx23uV+j6apoA6FxS8b2ieCHP4zjnMQw/rFgJ/fN2UZhWaXT5zGZTKyUOYztE4GXh+0/92E9Q+gZ5s/CbTqbSaOxRhsIjdsS2c2Hj347kr9cLPh+x1EuemElj367m7WZeVRWO5Z1tDeniKOny2y6lywYDAauGBrH2n15nDhT1ipz/9+6A+1eGKjRtBRtIDRujdFo4PcT+jD3d6Pp3z2ITzce4oZ3NzLs8SXc89mvLNx2hNMl9lcWK6Tq+zC+EQMBKpvJZIJvWmEVcSCvmIe/2c11b69n7ubDLT6fRtNe6CC1pkMwIjGMD24dSUlFFWv25rE0/QTL9uTw3Y5jeHsY+cvFgtvOS8JorNs7eWVGLiK6G92DG2+h3isykMHxwXyx+TC3jE20645yBEvNRWpsMH+dv4PMnCL+NqUfHkbd11nTsdArCE2Hwt/bk8mpMTx71WB+efAivrp7DBP6RfLEonRu/XATeUXltWOLy6vYtL+gUfeSNb+f0IfMnCLeWrmvRXNcmZFLYrg/8+4czY3nJvD2qix+9/FmisqrWnRejaatcamBEEJMEUJIIUSmEOLvdsZcI4RIE0LsFkJ8ZrW9pxBisRAi3bw/0ZVz1XQ8jEYDQ3uG8uZvhvP4FQNYn5XPJS+tZm1mHgDr9uVTUV3TpHvJwuTUGKYN6s7LP2eScaKwWXMqq6xm/b58xidH4uVh5PErBvDY5aksl7lc9cY6sgtKmnVejaY9cJmBEEJ4AK8BlwD9gVlCiP71xvQF/gGMlVKmAn+02v0R8JyUMgUYCXSQJsKatsZgMHDjuQks/P1Ygv28+M17G3nupz38nH6CAG8PRiSEOXyuR6enEujryV/mbafKwUC4NZsPFFBaWV3HKN00OpEPbz2HI6dKueK1tWzV4oCaDoIrVxAjgUwpZZaUsgKYA1xeb8xs4DUpZQGAlCqiaDYknlLKJebtRVJK/eilaZSU7kF8c89Yrh3Rg9eW72POpsOM6ROBt6fjf+bhgT48Oj2V7dmneX/tfqfnsDIjB28PI+f2Cq+zfVzfSL66eyx+3h7c+/lWrf2k6RC40kDEAdYpHNnmbdYkA8lCiLVCiA1CiClW208JIRYIIbYKIZ4zr0g0mkbx9/bk6SsH8cqsoUR28+HKYfFOn2PaoO5M7h/NfxZnkOVkx7mVGbmMTAqzKRHSJyqQJ64YSHZBKZ//csjpeWk0bU17ZzF5An2BC4B4YJUQYqB5+zhgKHAI+AK4BXjP3onKy8tJT09v9kTKyspadHxHpbNedx9v+GhmHFBAenpDl05T131zqg/rMuHejzfy7JTuGA1NZyDlFleRcaKI83t42z13hMnEoBhfXly8h4GBxfh5tW2eSGf9fTeFvu7m4UoDcQToYfU+3rzNmmxgo5SyEtgvhMhAGYxsYJuUMgtACPE1cC6NGAgfHx9SUlKaPdn09PQWHd9R0ddtn0cI5YF529lU4MctY5OaPOc286rgmnEDSI62rx/1SGB3Zr6+jnV53twzsa9zE28h+vfdtXDkurds2WJ3nysfXzYBfYUQSUIIb+A64Jt6Y75GrR4QQkSgXEtZ5mNDhKiN9E0E0lw4V42mAVcOi+MCEckzP0oOn2w6BLZS5hIb7EvfqMBGxw3rGcqk/tG8tTKLguKK1pquRtPquMxASCmrgHuAn4B0YK6UcrcQ4jEhxHTzsJ+AfCFEGrAc+IuUMl9KWQ08APwshNgJGIB3XDVXjcYWBoOBJ2cMxMNo4G9f7mhUMLCyuoa1mXmMF5EYHHBH/eViQVFFFW+2sOaiNaioqqlTP6LRWHBpDEJKuQhYVG/bv6y+NwH3m7/qH7sEGOTK+Wk0TREb4sffL+nHP7/exfc7jzFtUKzNcVsPnaKwvIrxyY7VXCRHd2PG0Dg+XHeAW8cmERPs2/RBLuK/SzP4cN0Bfvrj+fQI82+3eWjcD11JrdE0wayRPUnpHsTTP+yhrLLa5piVGTl4GA2M6RPh8Hn/dFEyNSYTL/28t7Wm6jQmk4nvdx6jpKKah77e1SJZdU3nQxsIjaYJPIwG/m9qCtkFpXZrI1Zm5DK8ZyhBTjQc6hHmzw2jEpi7+bDT6bStxd6cIg7mlzC4RwirMnK15LmmDtpAaDQOMKZPBBelRPP68n3kFtb11+cWlrPryBmHJT2s+f2EPvh4GvnPkozWmqpTLEk7AcAbNwxjSI8QHvsujZM6cK4xow2ERuMgD17aj7LKal5YIutsX71Xqbc6Gn+wJrKbD7edl8T3O46x68jpVpmnMyxOO8HgHiHEhvjx9JUDOVNayRPfd716AY1ttIHQaBykV2QgN41O5ItNh0k/dqZ2+wqZS0SgN/27BzXrvLPP70WIvxfP/SSbHuwAp0srHdKROnGmjO2HTzG5fzQA/WKC+N34Xnz5azZr9ua1ylw0HRttIDQaJ/jDhX0J8vPi39+nYTKZqK4xsXpvLuf3jWzQi8JRgny9uGl0Iqv25pLfwnTTb7YfZdSTS/m3A6sAi3tpktlAANw7sS9JEQE8+NVOSitsB+Q1XQdtIDQaJwj29+KPF/ZlbWY+P6fnsPPIaQpKKpsVf7BmUko0JtPZZkPOUlNj4tkf93Df51sBmLPpUJNFeEvSTpAQ7l+nsM/Xy4MnZgzg0MmSds2u0rgH2kBoNE5yw7kJ9I4M4MlF6SxNO4HBoNRaW0JqbBARgT4sl84biMKySmZ/tJnXV+xj1siezL9zDGWVNXzWiCBgUXkV6/flMyklukFh35jeEVwzIp53Vmex+2jbx0U07oM2EBqNk3h5GHloagpZecW8vSqLQfEhhAV4t+icRqOBCSKSlTLHqT4U+/OKmfH6OlZm5PL45ak8OWMAA+KCOa9PBP9bd8CurPhKmUtFdU0d95I1D16aQqi/F/9YsJPqGl0b0VXRBkKjaQYTRBTj+kaojnXNyF6yec5+UZwpq2Lr4VMOjV+Vkcvlr64hv6icj28bxY2jE2tXA7eNSyKnsJzvd9qua1iSdpywAG+GJ4Ta3B/i780/p/ZnR/Zplu3Rvbq6KtpAaDTNwGAw8K9p/ekVGcC0Qd1b5Zzn9Y3A02hguQM35MycQm79cBOxIX58c895jO5dt0HR+L6R9I4M4L01+xtUR1dW17BsTw4T+0Xh6WH/FjBtUHciAn2Yt/mw3TGazo02EBpNM+kb3Y1lf76gUWlvZwjy9WJEYqhDcYjPNh7GaIBPbh9leoZpiQAAFR1JREFUUz/JaDTw2/OS2HXkDL/sP1ln3y/7T3KmrMque8mCp4eRmcPiWLYnR4v5dVG0gdBo3IgJIor0Y2c4drrU7pjyqmoWbM1mcv8YIgJ97I6bOTSeEH8v3ltTVx5kSdoJfDyNjOvbtG7UVcPjqaox8fXW+q1cNF0BbSA0GjdiQr8oQBXf2eOn3Sc4VVLJdSN72B0D4OftwQ2jerIk/QQH84sBJc63JO0E4/pG2GyLWp/k6G4Mjg9m/pbsJoX8qqpruPzVNbzYTrIhzrJmbx4XvbCSwrLK9p6K26INhEbjRvSNCiQuxK/ROMQXmw4RH+rH2N5NrwBuGp2Ip9HAB2sPAJBVUMGRU6VNupesuWpED/YcL2TXkTONjvvy12y2Z5/mkw0HqXQiE6u9WJp+gsycIjZknWx6cBdFGwiNxo0wGAxM6BfJmsw8yqsaVjIfzC9mbWY+147o4VDldnSQL9MGxTJ382FOl1ay4VAJBgNcmOK4gZg+KBZvTyPzttgPVpdXVfPyz5kE+XqSX1xRq0/lzqQdVQZvbaaWFbGHNhAajZsxQURRUlHNpv0FDfbN3ayC01eNiHf4fLedl0RJRTVfbDrE+sPFDO8Z2mjsoj7B/l5cnBrDwm1HbRotgDm/HObIqVL+e90QQv29WPCre8csampMpJn1tNbt0wbCHtpAaDRuxpjeEXh7Glku67qZqqprmLc5mwkiiu7Bfg6fb0BcMKOSwnhrZRb7TlY45V6ycPXweE6XVrI0raHrq7SimleXZzIyKYwJIorLBseyJO0EZ9zYt3+4oISi8iqSIgLIOFFETmFZe0/JLdEGQqNxM/y8PRjdK7xBHGK5zCWnsJxrz2k8OG2L285LIt+szdQcAzG2TwTdg31tupk+Wn+A3MJyHpgsMBgMzBgaR3lVDT/uPO7051g4XVLJfxZLjp92zY17t9m9dNt5SQCs35fvks/p6GgDodG4IRNEJFl5xRzIK67dNueXQ0R286nNdHKGC1OiSQz3p0ewF70iA5s+oB4eRgNXDotnVUZunZt2YVklb67cx/nJkYxMCgNgSI8QkiICWLA12+nPsfD0j3t4ZVkmV76xjsycwmafxx67j57Gw2hg5rA4gnw9WZepDYQttIHQaNyQif3UU/4Ks5vp+Okylsscrh4ej1cj1c/28DAa+PDWkfxrgvOrBwtXDY+nxkSdG//7aw5QUFLJA5OTa7dZVhEbsk5y5JT9eg577Mw+zZxNh5iSGkN5VQ1XvrGeLQdbN9Mo7egZ+kYF4u/tybm9wlmr4xA20QZCo3FDeob70ysygGXmeoh5mw9TY6JZ7iULiREBxAc3X1QwMSKAcxJDmb9Z1UScKqng3dVZXJwazaD4kDpjrxgSB+B0gV1NjYmHv9lFeIA3z149iAV3jSEswJvr39nI4t3Nd1nVZ/fRM7UNnsb2iSC7oJRD+SWtdv7OgjYQGo2bMkFEsSErn+LyKr7YfJgxvcNJCA9o1zldPbwHWXnF/HqogLdWZVFUUcX9k0SDcT3D/RmREMpXW480WWBnzVdbj/DroVP8dUo/gny96Bnuz/w7R9OvexB3frKFTzcebPE15BaWk1NYTv9YZSDGmHWs9CqiIdpAaDRuysR+UVRU1fDcT5LsgtIWrR5ai0sHdcfPy4M3V2bx4doDTB8ci4ixrUU1Y1gcmTlFtQHhpigsq+TpH/cwpEcIVw07m8YbHujD57NHMT45koe+2sULi6VTRqc+lh4XqbHBAPSJCiSqm4+uh7CBNhAajZsyIjGUAG8PPlx3gBBzLUJ7E+jjyaUDu7Mk7QQV1TX88aJku2OnDYzF28PocE3EK8syyS0s59HpqQ2KAP29PXnnphFcMyKel5dlNtCXcgZL/YNlBWEwGBjTO5z1+/JbZHg6I9pAaDRuio+nB2P7KDmNGUPj8PXyaOcZKa42F+ldNSyepAj7Lq9gfy8m9ovim+1Hm2yClJlTxPtr9nPNiHgG9wixOcbTw8gzVw5iYr8o/rt0L7mFzVOY3X30DPGhfgT7edVuG9MngvziCuSJ1s+Y6shoA6HRuDFTBsTgYTQwa2TP9p5KLaOSwnjx2sE8eGlKk2NnDIsjr6ic1Y24b0wmE49+uxs/bw/+OqVfo+czGAw8NDWFsspqXmimKGDa0TOkmlcPFiyGeK1Od62DNhAajRszY2gca/42odV6TrQGKo01nmB/rybHThBRhPh78VUjbqYlaSdYvTePP12U7JAESO/IQG4ancgXmw7V6ik5SlF5FQfyi2vjDxbiQvxIDPdnnY5D1EEbCI3GjTEYDE7Jargb3p5Gpg7szuK04xSVVzXYX1ZZzePfp9E3KpAbRyc4fN4/XNiXID8vHv8uzam4wZ5jZzCZqE1xtWZ07wg27j/pVE/wzo42EBqNxqXMHBZHWWUN3+84ijxeyIJfs3ns2zSufWs95zyxlMMnS3l0eqpTBYDB/l7cPymZ9Vn5LEk74fBxloyq1LiGBmJsn3CKyqvYnn3a4fN1dpruGKLRaDQtYFjPUBLC/fnblztrt/l6GekXE8T0wbGcnxzJmD5N97aoz/Uje/Lx+oM8sSid8SISH8+mg/i7j54mLMCbmCDfBvtG91L1EOsy8xieEOr0fDojLjUQQogpwEuAB/CulPJpG2OuAR4BTMB2KeX1VvuCgDTgaynlPa6cq0ajcQ0Gg4HHLh/A2sw8Urp3IzU2mF4RAXg2QzLEGk8PI/+c1p+b3/+Fj9YdZPb5vZo8Ju2YClAbDA17aYQH+pDSPYh1+/K598K+LZpbZ8FlLiYhhAfwGnAJ0B+YJYToX29MX+AfwFgpZSrwx3qneRxY5ao5ajSatmF8ciQPXprCjKHxJEd3a7FxsD7vBBHJyz/vJb+o8bTXyur/b+/eo6sqzzyOfyEX7iTcwi0YkMRHAQEBMYhLAZ1ZVBmdVR0ognYYV1XEGWY6dup0dNq6lmv1MpQqY6sWRaUMlFGxtOMoXnDscNUoHYTwQECkIJdyC9ckhGT+2DsxTXYgt8ORc36ftbLOvp1z3pds8uz9vu9+3gq27jsR2f9QZezAbhTsOkLJmeh5L5JNLPsgRgNF7r7D3cuAJcBttY75BvCUux8BcP8iAb6ZjQR6AitiWEYRucj9yy2DONWAYa/b9p+g7GxF9QNyUcbmdqesvIIPd9adrCkZxbKJqS9QM3n8buCaWsdcBmBmqwiaob7n7m+YWWtgDjAduKkhX1ZaWkphYWGTC1tSUtKs91+sVO/kkqj1nmSdWLx+F2N7VjCgS92EhCUlJfyuaAsA7UoOUlgYPTy285kKUlrB8nVOt/LmT5u6bHMxK3ec4NvXZ9G38/mHBbe05v6+491JnQrkAeOAbOB9M7uSIDC87u67zeomAovSpk0brrji/A/u1KewsLBZ779Yqd7JJVHr/f2cMt778Xss2nyaX94ztE4fQ2FhIUdIp13aYW4cPZSUc8znPXxVMX60otn/TgdPlPLLxSs5VXaWh97cxzPTR3JN2BF+oTTk911QUFDvvlg2Me0BamYXyw631bQbWO7uZ9z9U2ArQcAYAzxoZjuBfwPuNrM6HdwiIgCZ7dP5h5vyWFV0iLcL606LCsEQ18t7dzpncICgH2LjnmKKTzdvytSfv7edkjNnWTDjarp1SGf6c+t49aOmT6IUD7EMEB8AeWY2wMzSga8By2sd8xrB3QNm1p2gyWmHu09z90vcvT/wEPCSuz8cw7KKyEVuWn4OuVkdefy/NlNa/qedzBWVlRRGpNiIMja3OxWVX0zW1BSfHz3NwrWfcfuIbMZbFq/OHMvV/bvyzaW/b3Y22gspZgHC3cuBB4E3gUJgqbtvMrPHzOzW8LA3gUNmthlYCXzL3ZUMRUQaLS2lNY/ccgU7D53ixdU7/2Tf/hPlHC8tr5NiI8qo/l3p17Udi9btanJZ5r27jcrKSmbfFAyXzWifxgszRldno529ZMNFMVIqpn0Q7v468Hqtbf9aY7kS+Gb4U99nvAC8EJsSikgiGWdZjLcezHuniK+OyK7O7bT9cDAE9lxDXKuktG7FtGty+MF/b8H3Ha93vov67Dx4kqUf7uau/Byyu7Sv3p6eGmSj7d+9Az96w9l24AS5WR2r7yaq7inapLbmgXG55GY1fu7wlqZUGyKSUB6ZNIjTZ84yZ4VXb9t+qIyU1q0a/Md+8qh+pKe2ZuHanY3+/rlvbyUtpRUPjB9YZ1+rVq14YFwuP5s2goqKSjbtKWbz58fYvPcYhXuPsWXvMVZs2s+dv1j7pZgCNd6jmEREWlRVttcFqz9len4Og/tksONIGbk9OjZ4To2uHdL5i6F9WPbRHr498XI6tW3YENUt+46x/Pefc/8NA8nqVDedR5Wbr+zNzVf2jtzn+44z5dk13Dl/LS/ffy29Mur/nFjTHYSIJJzZN+aRWSPb6/ZDpQ3qoK7prjE5nCw7y7KPGzYjHsCcFVvpmJ7KfQ1I+1Ef69WJl/5mNEdPnWHa/LUcPM8T4rGkACEiCacq2+vaHYdZtG4Xh06fPecT1FGG98tkaHYGC9d81qBRRx/vOsJbm/dz7/WXktm+7sN6jTE0O5Pn//pq9hw9zd3PrW/2kNumUoAQkYQ0dfQlWM9OPPabzQCNDhAA0/Nz2HbgBOs+PXzeY+es2Eq3DunMuG5Ao78nyugBXXnmrlFsO3CcGQvWczJiPo1YU4AQkYSUmtKaRycNoiycAGhw7/MPca3t1mF9yGiXxsI1n53zuNVFB/nfooPMHDeQjm1armv3hst6MG/qVWz4w1HuXfjhBR8aqwAhIgnrurzuTBzci5zMtAZNkVpb27QUJo/K5s1N+9h/rCTymP3HSnj015/QO6Mt0/MbPiteQ00c0psf3zGMVUWHePS1T1r8889FAUJEEtqTU6/iJzf3bfL7p+fnUF5RyeL1dR+c23XoFH/19Br2FZcwd8rwBo+SaqzbR2Yza/xA/rNgN+9uafgMes2lACEiCS09tTXt05r+py6nWwduuKwHi9fv4kyN+ap933HueHo1x0rOsOgb+eTHOBHf392Yh/XsxMOvbKT41IXptFaAEBE5j7vH5LD/WGn1/Ncf7zrC5GfWALD0vjEM75cZ8zK0SU1hzuRhHDpZxvd/uynm3wcKECIi5zXOssju0o6Faz5jVdFBps1fR0a7NF6ZeS2X9WxcKo7mGNI3g1njBvLqR3uqg1UsKUCIiJxHVX6mNTsOMWPBB/Tr0p6X7x9Dv67tz//mFvbghDwu79WJ7yzbyJGTZTH9LgUIEZEGmHJ1PzqkpzCoT2d+dV8+WZ3jkwIjPbU1cyYP48jJMr73m9g2NSkXk4hIA3TtkM7Kh8aR2T6d9NT4XlsP7pPBgxNy+enb2/jKkN5MHNIrJt+jOwgRkQbK6tw27sGhyqzxuQzq3ZlHXtvI4Rg1NX05aioiIo2SlhI0NRWfPsPct7bG5DvUxCQicpG6ondn5k29ivKK2ExhqgAhInIRmzgkel6JlqAmJhERiaQAISIikRQgREQkkgKEiIhEUoAQEZFIChAiIhJJAUJERCIpQIiISKRWlZWxeQLvQisoKPgjcO6ZxUVEpLackSNH9ojakTABQkREWpaamEREJJIChIiIRFKAEBGRSAoQIiISSQFCREQiKUCIiEikpJ8wyMwmAk8AKcB8d/9BnIsUM2b2PDAJOODuQ8JtXYFfAf2BncBkdz8SrzK2NDPrB7wE9AQqgWfd/YkkqHdb4H2gDcH/85fd/btmNgBYAnQDCoC73D02ExrHkZmlAB8Ce9x9UhLVeydwHDgLlLv7qOac60l9BxGeRE8BXwEGAVPNbFB8SxVTLwATa217GHjH3fOAd8L1RFIO/KO7DwLygVnh7zjR610KTHD3YcBwYKKZ5QM/BOa6ey5wBLgnjmWMpdlAYY31ZKk3wHh3H+7uo8L1Jp/rSR0ggNFAkbvvCK8mlgC3xblMMePu7wOHa22+DXgxXH4R+MsLWqgYc/e97v5RuHyc4I9GXxK/3pXufiJcTQt/KoEJwMvh9oSrN4CZZQO3APPD9VYkQb3PocnnerIHiL7AH2qs7w63JZOe7r43XN5H0BSTkMysP3AVsI4kqLeZpZjZBuAA8BawHTjq7uXhIYl6vv8U+CegIlzvRnLUG4KLgBVmVmBm94bbmnyuJ3uAkBrcvZLgBEs4ZtYReAX4e3c/VnNfotbb3c+6+3Agm+Bu+fI4FynmzKyqj60g3mWJk+vcfQRBs/ksM7u+5s7GnuvJHiD2AP1qrGeH25LJfjPrDRC+HohzeVqcmaURBIdF7v5quDnh613F3Y8CK4ExQKaZVQ1OScTzfSxwa9hZu4SgaekJEr/eALj7nvD1ALCM4MKgyed6sgeID4A8MxtgZunA14DlcS7ThbYc+Hq4/HXg13EsS4sL25+fAwrd/Sc1diV6vXuYWWa43A74M4L+l5XAHeFhCVdvd/9nd8929/4E/5/fdfdpJHi9Acysg5l1qloG/hz4hGac60mfzdXMbiZos0wBnnf3x+NcpJgxs8XAOKA7sB/4LvAasBS4hCBd+mR3r92RfdEys+uA3wEb+aJN+jsE/RCJXO+hBB2SKQQXgkvd/TEzu5Tgyror8DEw3d1L41fS2DGzccBD4TDXhK93WMdl4Woq8B/u/riZdaOJ53rSBwgREYmW7E1MIiJSDwUIERGJpAAhIiKRFCBERCSSAoSIiERK+myuIo1hZmcJhsxWWdJSGYDDVCC/rcq0KxJvChAijXM6TF8hkvAUIERaQJjaYSlBDpzTwJ3uXhTeFTxP8HDiH4EZ7r7LzHoCTwOXhh8xE/gcSDGzXwDXEqSDuM3dT1/AqohUUx+ESOO0M7MNNX6m1NhX7O5XAv9O8HQ+wDzgRXcfCiwCngy3Pwn8TzhfwwhgU7g9D3jK3QcDR4HbY1wfkXrpDkKkcc7VxLS4xuvccHkM8NVweSHwo3B5AnA3BFlXgWIz6wJ86u4bwmMKCGYBE4kL3UGItJzKepYbo2Z+oLPoIk7iSAFCpOVMqfG6JlxeTZBVFGAaQeJACKZ+nAnVE/tkXKhCijSUrk5EGqddOEtblTfcvWqO3y5m9n8EdwFTw21/Cywws28RdlKH22cDz5rZPQR3CjOBvYh8iSibq0gLCEcxjXL3g3EuikiLUROTiIhE0h2EiIhE0h2EiIhEUoAQEZFIChAiIhJJAUJERCIpQIiISKT/B3pn+wLVr38QAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# let's plot anyways\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, '2-class Precision-Recall curve: AP=0.67')"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3de5wcZZ3v8U9nEpJALlwiikmACOFHAggYBDnqiooseBBc8UK4CCvictZ4OSh7dL0Q2FXBCx5eKygvQUFFsoFVNyvBeAXUAxoSQDeMv5AwiQmguRIymUwmmfT546lK13S6q2s6U909M9/365VXprurqp9+pqe+9TxP1VOFYrGIiIhINSOaXQAREWltCgoREUmloBARkVQKChERSaWgEBGRVAoKERFJpaAYxMzscjP7TbPLMdDMbJmZnVFjmcPNrNPM2hpUrNyZ2SozOzP6ea6Zfa/ZZRIBGNnsAgw3ZjYauBU4EzgYWAl80t0faGrBMjCzVcBLgV5gG/AAMMfdOwfyfdz9uAzL/BkYN5DvGzOzucCngB3ALuAp4GPu/kge7zfUmdmdwCXAVHd/PvH8XAagns3sIuALwCTgZ8D73H1TlWXbgOuA9wHjgRXAG939BTP7RlTO2Cigx93H96c8Q5FaFI03ElgDvAGYCHwamG9mRzazUP3wNncfB7wKOIVQ/j7MrGBmg/279e/R55wE/Aq4t8nlGXBmlvuBopkdAFwAbKHvTjgW1/NLgN8APzCzQj+2fxxwG3Ap4SCmi3AgVs11wP8ATgcmROt1A7j7Ve4+Lv4H3MMQ/L3XQy2KBnP3bcDcxFM/NrMOYBawqtI6ZjYVuBl4PSHc73H3ORWWuxl4ByGAngY+6u6/jl47lfAHdAywHbjb3a82szHA7cA5QFu03rnu/tcan+NZM3sAOD7a/oPAb4EzCCFygpmtB24C3grsBr4NXOvuvdE6VwJXA1MI4XmJuy+NWi7vd/efp5T7SKADGOXuu8zs5cA3gNcBm4Ab3f2b0fvMBWYSdgh/B/wZuMzdH0v7jNHn3GVmdwP/bGYvcff10TbPBf4VOJJwJHyVu/8heq3i78vMjgK+CZwIFIFFwAfd/YVa5ShnZucTdnqvANZH2/lJsu4Sn/1od78kUWfvB64FVpnZNuB+d/9aYttPAte5+w/M7Fjg3wjfz/XAZ9x9fj+KegHwAvBl4ErgS5UWcvedZnYXcA1wCLAh4/YvBv7L3R+Oyv4ZoN3Mxrv71uSCZnYQ8FHgRHdfHT3935U2mgi4czOWY0gb7Ed9g56ZvZSwE1xW5fU24MfAasJOaTIwr8rmFgMnEbq0vg/cGwUBhB3Xze4+ATgKiP/YLyMEy1TCH+hVhB1yrXJPJQTA44mnLwU+QGjSrwbuJHQpHA2cDJxF2ElhZu8iBOZ7CUd25wEbK7xVtXKXmwesBV4OvBP4vJm9KfH6edEyBwILgK/ttYXKn3O/qIwbgc3RcycD3wL+gVBntwELzGx0jd9XgdBF8nJgBqHO52YpR1mZTgW+Q9ipHgj8DVUOMqp4Q/T+f0s4ap6d2PZM4Ajg/mhn+TPCd+lQ4ELg1mgZzOwiM/tDjfe6LHqPecCxZjarymcaDVwOrHH3DWb2OjN7IeXf66JVjwOejLfj7iuBHsLfVLkTCN/Hd5rZX8xsuZl9sEq5LyAE48M1Pt+woBZFE5nZKOBu4C53/1OVxU4l7Fiucfdd0XMVB7DdPTn4+RUz+zRghD+kncDRZjbJ3TcAj0bL7STs7I6OjoiX1Cj2j8xsF6Er4X7g84nX7nT3ZdFneykhSA509+3ANjP7KiFIbiMExhfdfXG07ooq71et3HtEofVa4H+6ezfwhJndTtjB/zJa7DfuvjBa/ruEI8s0745aDeMJR8QXJOr/A8Bt7v676PFdZvbPwGsIO6mKvy93X5H4nOvN7CbCkX1/XQF8y91/Fj1+tp/rz41atpjZD4Gvm9kR0VH2xcAP3H2Hmb0dWOXu347We9zM/gN4F6HF8X1CiFRkZocDbySMO/zVzH5B+J0kv2NxPfcQju7/DsDdf0MIwVrGEb6LSVsIv7dyUwgHRccA04DpwC/MbHmiLmOXAd9xd02Gh4KiaaI+/O8S/kDmJJ5/gNBlAeGIdSewOrHTSdvmxwk7kZcTujYmEPrYiZ6/HvhT1NV1nbv/OCrDVGCemR0IfA/4lLvvrPI2b4+7NSpYk/j5CMJg4PNmFj83IrHMVMJAfi3Vyp30cmBTWVfDasIYSuwviZ+7gDFRH/17CMEF8Gt3Pyf6eX7UXTMJ+A9C18uDic92mZl9KLHN/aJy9FLl9xWFZ9wlNZ5QH5trfP5KpgIL61gvtuf35O5bzex+QmvhRkLr4sro5SOA08ws2TU2kvCdyeJSoN3dn4ge3004gPl44vs1390rjV1k1Un4nidNALZWWDZuKV8fHbz8wczmEQ5o9gRFFHBnUKqHYU9B0QTRYN0dhMG3tyZ3yokdVbzs6cDhZjYyLSzM7PXAPwFvBpa5+24z20zo7sDdnwZmRwH1DuA+MzskOrK8Drgu6sNeCHhUvv5KHn2tIZzNMqlKudcQupJSVSt32WLPAQeX9UsfToYjbXe/m7ADq/b6BjP7APCYmX0/OmtnDfA5d/9c+fI1fl+fJ9TRCe6+KTpiz9QFViat7rYB+ycev6zCMuVHyfcA15rZw8AYwuB9/D4Puftb6igjhNbD4WYWh/RIQuv1rcB/pq0YfZ/TzgQ8Jxp/W0YY84nXewUwGlheYZ24myz5+Su1GC4Ffuvuz6SVcThRUDTH1wl9xGdGRzZpfg88D9xgZtcSjlhnuftvy5YbT+h/XQ+MNLNPkDjSMrNLgEXuvj5xhLjbzN5IGDh8CniR0ILZvU+fDnD3583sp4QjyM8QjvymAVPc/SHCAPpNFq4DWUrY8e1MDDKmlrvsvdaY2f8DvhC1qo4htEQu3tfPEW3fzWwRIYj/N2FA+odm9nPC72d/whHow6T/vsYTukW2mNlkwhhDPe4AfmpmPybs1A8Dxkfdl08AF0Yt0xMJ4zU/qbG9hYQxl+sJZyHF9fvj6HNcSmmc5SSg093b0zYYBeZRhLGp9YmXvkIIkNSgiEIgy+nPdwOPRMGyNPoMPygfyI62udLMfg18ysw+TDgR4EISYzSR9xJaVxLRYHaDmdkRhC6lk4C/WLhorNPMKu7UojOE3kYYEP4zYcD2PRUWXUTYISwndLt007cr6GxgmZl1Ero/LoxC6mXAfYSQaAceInvXQi3vJXTJPEXoYrmPsFPD3e8FPkfo494K/IgwCF+uWrnLzSYMHj8H/JBwdlW1LrJ6fAn4gJkdGp0tdSWhNbCZMO5wOdT8fV1HOCMsHt/5QT0FcfffA38PfDXa1kOEbiKAzxB20Juj96s6hpDY3o6oLGcml492tmcRdqbPEbrvbiQcsWNmF5tZxZMwCH38/+nuf3T3v8T/CL/Dc82s0u+636IxsasIgbGOEMb/GL9uZg9E40ex2YS62kj4HXzG3X+RWP50wliGTotNKOjGRSIikkYtChERSaWgEBGRVAoKERFJpaAQEZFUg+702KVLlxbHjh3b7GK0hB07djB69OhmF6MlqC5KVBclqouSrq6uDbNmzXpJPesOuqAoFArMmDGj2cVoCe3t7aqLiOqiRHVRorooWbJkyeraS1WmricREUmloBARkVQKChERSaWgEBGRVAoKERFJpaAQEZFUuZ0ea2bfItxvdp27H1/h9QJhJsm3Em4kc7m7L82rPCIiUp88WxR3EqaIruYcwq0IpxNuLfn1HMsiIiJ1yq1F4e4PR3dMq+Z8SvekfdTMDjSzw6I7iFW1Y0eB5ZXuXVXBwQfDpEm1lxMRkeqaeWX2ZPreWGdt9FxqUBR7i6x8vPatlru7RzDmgN0ceVzPPhWylXV3d9PennqjsWFDdVGiuihRXQyMQTeFx5j94E1nTqm53NMrCvT29DJjxtCdF0rTE5SoLkpUFyWqi5IlS5bUvW4zz3p6FpiaeDwlek5ERFpIM1sUC4A5ZjYPOA3YUmt8QkREGi/P02PvAc4AJpnZWuBaYBSAu38DWEg4NXYF4fTYv8+rLCIiUr88z3qaXeP1IvDBvN5fREQGhq7MFhGRVAoKERFJpaAQEZFUCgoREUmloBARkVQKChERSaWgEBGRVAoKERFJpaAQEZFUCgoREUmloBARkVQKChERSaWgEBGRVIPuDnd52LABNm0KP+s+2yIifSkoCCGxaW03xV1FujcWmDRpTLOLJCLSMhQUkbGjirQd0EZvT2+ziyIi0lKGbVAku5u6uqDQ3OKIiLSsYRsUcXfT2FFFCsDEcb109rTVtS2NcYjIUDZsgwJCd9OxM+MTv0bw9Ir6tqMxDhEZyoZVUAxkd1P5tjTGISJD1bAKikrdTf25lCQZDlu3QvembsaN2feuKxGRVjasggL27m7qj2TQjAQmT+rlsMNH7dlWra6rZNCAxjNEZHAY8kExEN1N8TbiLqb+Bk28frIV0rUdjWeIyKAw5INiX7ubyrexr+vHrZCnVxRSxzPU+hCRVjHkgwL2rbtp7230rxUB5S2RbOsnw0WtDxFppmERFPWop8tqayfs7oHlyysPdtcKiWrhUqv1ISKSJwVFFfV2We3uLbL9r9srDnZXU2kMo95uMhGRgTZkgyI+ui+Ozz6AnWwR1DtwXewtcuzM+DTZ6utUa31kDRcRkUYZskEB4ei+sHV7v47M4xZBPUf0E8ZDD8V+lW/v1ofCQURay5AOitLRffadb9YWQSVHH1UERtVcbiDeS0SkUYZ0ULSy/rY+RESaRUGR0Midd39bHyIizZJrUJjZ2cDNQBtwu7vfUPb64cBdwIHRMp9w94V5limNdt4iInvLrWPczNqAW4BzgJnAbDObWbbYp4H57n4ycCFwa17lERGR+uQ5gnoqsMLdn3H3HmAecH7ZMkVgQvTzROC5HMszaG3thM7OcCrthg3NLo2IDDd5dj1NBtYkHq8FTitbZi7wUzP7EHAAcGatjRaLRTo6Omq++ZYtoyh0F+jo6Mlc4Fb13HOjoafAunVFxhywmyOPC5+pu7ub9vb2JpeuNaguSlQXJaqLgdHswezZwJ3u/hUzOx34rpkd7+67q61QKBSYNm1azQ1nWGTQ2LylwM7OnRz8slH09vQyY8ZYANrb25kxY0aTS9caVBclqosS1UXJkiVL6l43z66nZ4GpicdToueSrgDmA7j7I8AYQHOklpkwHsaP06m0ItIcebYoFgPTzWwaISAuBC4qW+bPwJuBO81sBiEo1udYpkEpPhur3nt6i4jsi9xaFO6+C5gDLALaCWc3LTOz683svGixjwFXmtmTwD3A5e6uQ2cRkRaS6xhFdE3EwrLnPpv4+SngtXmWQURE9o0mGBIRkVQKChERSdXs02OlH5L3sADYvLktfQURkQGgoBhk4ntYdG2HLS+GoEjeQvXgg2GSTjAWkQGkrqdBZMJ4GDd2N8fOHMH4iW0Q3UY7vm3rxlXbee7p7uYWUkSGHLUoBpHk7LZbO6Gra0Sf27a2HdBGb09vU8uo1o3I0KOgGMx2F/rctvX5TW19xjDKd9TJnXil1+uV3G58/+8RxSLdGwtMmjRm399ARJpKQTGYFeHYmXHv4Qie39R3DCPeUcc78ngnPm5Msc/r9agUDuPGFPfc/7uzZ7+mt25EZGAoKAapCePhxf33njsxvg/30ysKe3bU8RjG2FFFJk/q5bDDR/V5vR7JbcbhcNjh8U2fRmi6EZEhREExSB19VJG2EX2nUE/eyjV5Km08hhFaH/Wfv5BsRfTdJvu0XRFpbQqKIaT8Vq5xN1Q8hlHvzrxS19W+blNEBg8FxRAVWhe7OXZmqTuoP6qNQZR3MYnI0KegGKLKWxdZ1BqgDgGhcBAZbhQUUrFrqdIAtYgMTwoKqXhWVKBwEBEFxbCVx1lR1bYPukpbZDBTUAxjA3VWVK3td3bC+tUFNh0RLu5TaIgMLgqKYWpfz4rqz/aXPlFg1/Zde10xLiKDg4JimKrnrKh6t58MjX29IlxEGk9BIbnLO5REJF86rUUaamsndHaGQe4NG5pdGhHJQi0KabjdvUXWr9xecYBb97MQaT0KCmmoeLyim/32DHAnz4pq9v0sFFQie1NQSEPF4xUrVlY+K6qe+1ls2ACrVu1HW7iFeL938Lrxkkg6BYU0RbWzooLa97Mo37l3rRvB9nGVT7+tdGc/0I2XRLJSUEjT9eesqLR5qY6dOWLP6bfVJjiMu7nGHDwmdV4r3XhJpERBIS0vy5TnHR27wuvR1CGrV1ee4DDu5hrZtT11XqtqU5BoDEOGIwWFtKTkjrq/U57v7i1WDYK+3VzpZ4dXmoKknjGM8q6vPZ9xa/h//HiFjrQ2BYW0rHhH3Z8pz2sFQdZuruR2NmyEdc9WH2yvFATJEEgGXVJ3Z/i/Z6OmNZHWpqCQllTvXFQDdRV4cjuTDoFJhyT/VEaw9InKLZ5YHAIju6gQdMGKlQV6tvYwalz6wHmlwfhkN1gylAA2b27ba71KLRZ1o0lWCgppSYNh2o/qLZ5SCKQFXfwZk6EDlYOgfDA+2Q3GrhBQI7vCVe9/WTeK5ZPo83pyHejb0tGpwFJL5qAws8nAEcl13P3hPAol0upqtXj6G3Rp4yHJgftK3WDbdu63J5Q2bIStL/QNsFEHjNqzTqWWjk4FlloyBYWZ3Qi8B3gKiL9RRSA1KMzsbOBmoA243d1vqLDMu4G50faedPeLshZepFkGssVTazwkOXBfqRss/OmUusmOnN7DtGkjEq+X1qnU0nl6BXRtb8wZXtUG9tX11dqytijeDpi778i6YTNrA24B3gKsBRab2QJ3fyqxzHTgk8Br3X2zmR2avegiQ0Ot8ZC83it20EFFdnbtrtqiqdZ1Bek7+GqD/OXjObpHSevLGhTPEL5dmYMCOBVY4e7PAJjZPOB8QqskdiVwi7tvBnD3df3YvogMgGQ4VZpOpVrXVTJUKrVCKoVCpfEc3aOk9WUNii7gCTP7BYmwcPcPp6wzGViTeLwWOK1smWMAzOy3hO6pue7+k4xlEpEBVm3spVLXVRwqyZmAq1/zktT/VpLO0GqurEGxIPqXx/tPB84ApgAPm9kJ7v5CtRWKxSIdHR05FGXw2bFjh+oioroo2Ze6aBsBYydCtdWTr2/ZMopCd4EJh/ayZX0bL24KLYeDxhUZfVC4Ur67t/q2Yr58NOyC5zp3V11m27YRFLYBu4ERsP+hey87cWIvBx3Ut2XS3d1Ne3t7egGkpkxB4e53mdl+RC2A8JTvrLHas8DUxOMp0XNJa4HfRdvqMLPlhOBYXG2jhUKBadOmZSn2kNfR0aG6iKguShpVFwP1Fpu3hJbJhHGF6guNg4njSt1gI0b2bZV0bYcDCgVmzOg7ztHe3s6MGTMGpqCD3JIlS+peN+tZT2cAdwGrgAIw1cwuq3F67GJguplNIwTEhUD5GU0/AmYD3zazSYQgeqY/H0BEBre9u7uq6dsNlqRxjnxl7Xr6CnCWuzuAmR0D3APMqraCu+8ysznAIsL4w7fcfZmZXQ885u4LotfOMrP4tNtr3H1j/R9HRAabwXBx5XCXNShGxSEB4O7Lzazmb9bdFwILy577bOLnInB19E9ERFpQ1qB4zMxuB74XPb4YeCyfIomI9F980aDmtRp4WYPifwEfBOLTYX8N3JpLiURE+im+aDB5qi6UbpFb6eJBUGhklfWspx3ATdE/EZGWEl80mLxgEIAXqXrxYPKCwWoUJEFqUJjZfHd/t5n9kTChTB/u/srcSiYi0k/lZ1CNHruz6rxXe4VKmfIgKb/XermhHCq1WhQfif4/N++CiIjsq/6cQVXrtNzkBI2V7rWeNNTnq0oNCnd/PvpxA7Dd3XdHp8YeCzyQd+FERPJSK1SSc2DFoTFir1vsBk+vKLB1Sy/xNW3x/T7in2FwtziyDmY/DLzezA4Cfkq4mO49hLOfRESGtL6z+u49V1U8mN61LnRjjezqe5fDeloc5RMsQvPur541KAru3mVmVwC3uvsXzeyJPAsmIjJYxEGSnDSxfALFrS/09jl9t9q9OWLJCRbj0Olet/e4SSNCI3NQmNnphBbEFdFzbfkUSURkcEp2Z5V3be3uLVadabeS5Oy7cegcOrV/Z20NlKxB8VHCDYZ+GE3D8QrgV/kVS0Rk6IgHzpM7+urTsCeFbq6+N7fqO25S7aytvYzmgHrLn/U6ioeAhxKPn6F08Z2IiKSotKMP6r+D4d53Q0y3bGWh7l6gWtdR/F93/6iZ/ReVr6M4r943FhGRwaFWHH03+v/LeRdERERaU63rKOI7XTxGdB0FgJm1AaNzLpuIiLSArB1kvwD2TzweC/x84IsjIiKtJmtQjHH3zvhB9PP+KcuLiMgQkTUotpnZq+IHZjYLyHhOloiIDGb9uY7iXjN7jnDP7JcRpvAQEZEhLut1FIvN7FjASk/5zvyKJSIirSJT15OZ7Q/8H+Aj7v7fwJFmpqnHRUSGgaxjFN8GeoDTo8fPAv+aS4lERKSlZA2Ko9z9i8BOAHfvIoxViIjIEJc1KHrMbCzRNB5mdhSwI7dSiYhIy8h61tO1wE+AqWZ2N/Ba4PK8CiUiIq2jZlCYWQH4E/AO4DWELqePuPuGnMsmIiItoGZQuHvRzBa6+wnA/Q0ok4iItJCsYxRLzezVuZZERERaUtYxitOAS8xsFbCN0P1UdPdX5lQuERFpEVmD4m9zLYWIiLSsWne4GwNcBRwN/BG4w913NaJgIiLSGmqNUdwFnEIIiXOAr+ReIhERaSm1up5mRmc7YWZ3AL/Pv0giItJKarUo9swQqy4nEZHhqVaL4kQzezH6uQCMjR7HZz1NSFvZzM4GbgbagNvd/YYqy10A3Ae82t0f688HEBGRfKUGhbu31bthM2sDbgHeAqwFFpvZAnd/qmy58cBHgN/V+14iIpKfrBfc1eNUYIW7P+PuPcA84PwKy/0LcCPQnWNZRESkTlmvo6jHZGBN4vFawoV7e0T34Z7q7veb2TVZNlosFuno6Bi4Ug5iO3bsUF1EVBclqosS1cXAyDMoUpnZCOAm+jkLbaFQYNq0abmUabDp6OhQXURUFyWqixLVRcmylcvqXjfPrqdngamJx1Oi52LjgeOBB6OpQV4DLDCzU3Isk4iI9FOeLYrFwHQzm0YIiAuBi+IX3X0LMCl+bGYPAh/XWU8iIq0ltxZFdN3FHGAR0A7Md/dlZna9mZ2X1/uKiMjAynWMwt0XAgvLnvtslWXPyLMsIiJSnzzHKEREZAhQUIiISCoFhYiIpFJQiIhIKgWFiIikUlCIiEgqBYWIiKRSUIiISCoFhYiIpFJQiIhIKgWFiIikUlCIiEgqBYWIiKRSUIiISCoFhYiIpFJQiIhIKgWFiIikUlCIiEgqBYWIiKRSUIiISCoFhYiIpFJQiIhIKgWFiIikUlCIiEgqBYWIiKRSUIiISCoFhYiIpFJQiIhIKgWFiIikUlCIiEgqBYWIiKRSUIiISKqReW7czM4GbgbagNvd/Yay168G3g/sAtYD73P31XmWSURE+ie3FoWZtQG3AOcAM4HZZjazbLHHgVPc/ZXAfcAX8yqPiIjUJ88WxanACnd/BsDM5gHnA0/FC7j7rxLLPwpckmN5RESkDnkGxWRgTeLxWuC0lOWvAB6otdFisUhHR8c+Fm1o2LFjh+oiorooUV2UqC4GRq5jFFmZ2SXAKcAbai1bKBSYNm1a/oUaBDo6OlQXEdVFieqiRHVRsmzlsrrXzTMongWmJh5PiZ7rw8zOBD4FvMHdd+RYHhERqUOeQbEYmG5m0wgBcSFwUXIBMzsZuA04293X5VgWERGpU25nPbn7LmAOsAhoB+a7+zIzu97MzosW+xIwDrjXzJ4wswV5lUdEROqT6xiFuy8EFpY999nEz2fm+f4iIrLvdGW2iIikUlCIiEgqBYWIiKRSUIiISCoFhYiIpFJQiIhIKgWFiIikUlCIiEgqBYWIiKRSUIiISCoFhYiIpFJQiIhIKgWFiIikUlCIiEgqBYWIiKRSUIiISCoFhYiIpFJQiIhIKgWFiIikUlCIiEgqBYWIiKRSUIiISCoFhYiIpFJQiIhIKgWFiIikUlCIiEgqBYWIiKRSUIiISCoFhYiIpFJQiIhIKgWFiIikUlCIiEgqBYWIiKQamefGzexs4GagDbjd3W8oe3008B1gFrAReI+7r8qzTCIi0j+5tSjMrA24BTgHmAnMNrOZZYtdAWx296OBrwI35lUeERGpT55dT6cCK9z9GXfvAeYB55ctcz5wV/TzfcCbzayQY5lERKSf8ux6mgysSTxeC5xWbRl332VmW4BDgA3VNrq7sHvLspXL1g1wWQetZSuXNbsILUN1UaK6KFFdBMURxUPrXTfXMYo8zDpl1oHNLoOIyHCSZ9fTs8DUxOMp0XMVlzGzkcBEwqC2iIi0iDxbFIuB6WY2jRAIFwIXlS2zALgMeAR4J/BLdy/mWCYREemn3FoU7r4LmAMsAtqB+e6+zMyuN7PzosXuAA4xsxXA1cAn8iqPiIjUp1As6gBeRESq05XZIiKSSkEhIiKpWvb0WE3/UZKhLq4G3g/sAtYD73P31Q0vaAPUqovEchcQLuJ8tbs/1sAiNkyWujCzdwNzgSLwpLuXn1AyJGT4GzmccHHvgdEyn3D3hQ0vaM7M7FvAucA6dz++wusFQj29FegCLnf3pbW225ItCk3/UZKxLh4HTnH3VxJ2jl9sbCkbI2NdYGbjgY8Av2tsCRsnS12Y2XTgk8Br3f044KMNL2gDZPxefJpwQs3JhDMwb21sKRvmTuDslNfPAaZH/z4AfD3LRlsyKND0H0k168Ldf+XuXdHDRwnXrAxFWb4XAP9COHDobmThGixLXVwJ3OLumwHcfajOaJClLorAhOjnicBzDSxfw7j7w8CmlEXOB77j7kV3fxQ40MwOq7XdVg2KStN/TK62THQqbjz9x1CTpS6SrgAeyLVEzVOzLszsVcBUd7+/kQVrgizfi2OAY8zst2b2aNQ9MxRlqYu5wCVmthZYCHyoMUVrOf3dnwCtG+n++mUAAAMJSURBVBRSBzO7BDgF+FKzy9IMZjYCuAn4WLPL0iJGEroYzgBmA980s+E6Bc5s4E53n0Lon/9u9H2RDFq1ojT9R0mWusDMzgQ+BZzn7jsaVLZGq1UX44HjgQfNbBXwGmCBmZ3SqAI2UJbvxVpggbvvdPcOYDkhOIaaLHVxBTAfwN0fAcYAkxpSutaSaX9SrlXPetL0HyU168LMTgZuA84ewv3QUKMu3H0LiT9+M3sQ+PgQPespy9/IjwhH0t82s0mErqhnGlrKxshSF38G3gzcaWYzCEGxvqGlbA0LgDlmNo8wm/cWd3++1kot2aLQ9B8lGeviS8A44F4ze8LMFjSpuLnKWBfDQsa6WARsNLOngF8B17j7kGt1Z6yLjwFXmtmTwD2E00KH3IGlmd1DOHg2M1trZleY2VVmdlW0yELCwcIK4JvAP2bZrqbwEBGRVC3ZohARkdahoBARkVQKChERSaWgEBGRVAoKERFJ1arXUYg0jZn1An8k/H10AJe6+wsDuP3LCZM4zjGzuUCnu395oLYvMtDUohDZ23Z3PymapnkT8MFmF0ikmdSiEEn3CPDK+IGZXQO8GxgN/NDdr42efy/wccIspX9w90vN7G2E6a33I0wvc7G7/7XB5RfZZ2pRiFQR3efgzYRpDzCzswhzJZ0KnATMMrO/MbPjCIHwJnc/kXAvDIDfAK+J7oEwD/inBn8EkQGhFoXI3saa2ROE6ZfbgZ9Fz58V/Xs8ejyOEBwnAve6+wYAd4/vBzAF+Pdovv/9COMdIoOOWhQie9vu7icBRwAFSmMUBeAL0fjFSe5+tLvfkbKdfwO+5u4nAP9AmIhOZNBRUIhUEd018MPAx6Kp7BcB7zOzcQBmNtnMDgV+CbzLzA6Jnj842sRESlM4X9bQwosMIAWFSAp3fxz4AzDb3X8KfB94xMz+SLgF73h3XwZ8Dngomp30pmj1uYQZfZcAGxpeeJEBotljRUQklVoUIiKSSkEhIiKpFBQiIpJKQSEiIqkUFCIikkpBISIiqRQUIiKS6v8DUHqD+RYr2JAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "precision, recall, thresholds = metrics.precision_recall_curve(Y_test.values, test_prediction_scores)\n",
    "import matplotlib.pyplot as plt\n",
    "from inspect import signature\n",
    "step_kwargs = ({'step': 'post'}\n",
    "               if 'step' in signature(plt.fill_between).parameters\n",
    "               else {})\n",
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, alpha=0.2, color='plum', **step_kwargs)\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('Tox Liver Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "          average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Tox Liver Precision-Recall curve: AP=0.67')"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "precision, recall, thresholds = sklm.precision_recall_curve(Y_test.values, test_prediction_scores)\n",
    "import matplotlib.pyplot as plt\n",
    "from inspect import signature\n",
    "step_kwargs = ({'step': 'post'}\n",
    "               if 'step' in signature(plt.fill_between).parameters\n",
    "               else {})\n",
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, alpha=0.2, color='plum', **step_kwargs)\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('Tox Liver Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "          average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving keras model!\n",
    "model.save('my_dweeby_model.h5')  # creates a HDF5 file \n",
    "del model  # deletes the existing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train['smiles'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'smiles': array([[ 0,  0,  0, ...,  1,  6, 12],\n",
       "        [ 7,  1,  2, ...,  4,  3,  1],\n",
       "        [ 0,  0,  0, ...,  1,  1,  6],\n",
       "        ...,\n",
       "        [ 0,  0,  0, ...,  1,  1,  6],\n",
       "        [ 1,  5,  5, ...,  4,  3,  1],\n",
       "        [ 0,  0,  0, ...,  1, 10,  1]], dtype=int32)}"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1799\n",
      "1798\n",
      "mean text len: 26.20063694267516\n",
      "max text len: 372\n",
      "num_words 29\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# smi tokenizer >\n",
    "final_list_except_wo_SMILES = list(X_traindf.columns)\n",
    "print(len(final_list_except_wo_SMILES))\n",
    "final_list_except_wo_SMILES.remove('Smiles')\n",
    "print(len(final_list_except_wo_SMILES))\n",
    "\n",
    "X_train['num_vars'] = scaler.fit_transform(X_traindf[final_list_except_wo_SMILES])\n",
    "X_test['num_vars'] = scaler.transform(X_testdf[final_list_except_wo_SMILES])\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "print('mean text len:',X_traindf[\"Smiles\"].str.count(regex).mean())\n",
    "print('max text len:',X_traindf[\"Smiles\"].str.count(regex).max())\n",
    "min_count = 1\n",
    "tokenizer = Tokenizer(char_level=True, num_words=29)\n",
    "tokenizer.fit_on_texts(list(list_smiles_train) + list(list_smiles_test))\n",
    "num_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])\n",
    "print('num_words',num_words)\n",
    "maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smi_tokenizer(smi):\n",
    "    \"\"\"\n",
    "    Tokenize a SMILES molecule or reaction\n",
    "    \"\"\"\n",
    "    import re\n",
    "    pattern =  \"(\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9])\"\n",
    "    regex = re.compile(pattern)\n",
    "    tokens = [token for token in regex.findall(smi)]\n",
    "    assert smi == ''.join(tokens)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# list_sentences_train = train[\"Smiles\"].fillna(\"\").values #.apply(lambda x: smi_tokenizer(x)).values\n",
    "# print(list_sentences_train)#fillna(\"\").values #(30053,)\n",
    "# print(list_sentences_train.shape) # print(list_sentences_train.shape) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = Y_train.values\n",
    "# max_features = 29 #\n",
    "# maxlen = 100 #padding length\n",
    "# num_folds = 3#2 #number of folds\n",
    "# test = X_testdf\n",
    "# train = X_traindf\n",
    "# # TO DO --- fix SMILES emveddigsall\n",
    "# list_sentences_test = test[\"Smiles\"].fillna(\"\").values#.apply(lambda x: smi_tokenizer(x)).values\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "# print('mean text len:',train[\"Smiles\"].str.count('\\S+').mean())\n",
    "# print('max text len:',train[\"Smiles\"].str.count('\\S+').max())\n",
    "# min_count =2\n",
    "# tokenizer = Tokenizer(char_level=True)\n",
    "# tokenizer.fit_on_texts(list(list_sentences_train) + list(list_sentences_test))\n",
    "# num_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])\n",
    "# print('num_words',num_words)\n",
    "# tokenizer = Tokenizer(char_level=True)#, num_words=num_words)\n",
    "# tokenizer.fit_on_texts(list(list_sentences_train)) # + list(list_sentences_test)\n",
    "# list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "# list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "# print('padding sequences')\n",
    "# X_train = {}\n",
    "# X_test = {}\n",
    "# X_train['sentence'] = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen, padding='post', truncating='post')\n",
    "# X_test['sentence'] = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Brc1c(NC2=NCCN2)ccc2nccnc12'"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_smiles_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_sequences_matrix[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'smiles': array([[ 0,  0,  0, ...,  1,  6, 12],\n",
       "        [ 7,  1,  2, ...,  4,  3,  1],\n",
       "        [ 0,  0,  0, ...,  1,  1,  6],\n",
       "        ...,\n",
       "        [ 0,  0,  0, ...,  1,  1,  6],\n",
       "        [ 1,  5,  5, ...,  4,  3,  1],\n",
       "        [ 0,  0,  0, ...,  1, 10,  1]], dtype=int32),\n",
       " 'num_vars': array([[0.04266347, 0.04125442, 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [1.12883944, 1.08106115, 0.7       , ..., 0.05049383, 0.        ,\n",
       "         0.00466838],\n",
       "        [0.08117071, 0.08097098, 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        ...,\n",
       "        [0.0368367 , 0.03908137, 0.        , ..., 0.28635605, 0.        ,\n",
       "         0.29306099],\n",
       "        [0.14463684, 0.1294301 , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.03719085, 0.0462475 , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ]])}"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_size\n",
    "len(X_test['smiles'][0])\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1798\n",
      "Found 29 unique tokens.\n",
      "Found 60 word vectors.\n",
      "3000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1798]"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train[\"num_vars\"].shape[1])\n",
    "\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "embeddings_index = {}\n",
    "\n",
    "EMBEDDING_PATH = \"/root/SMILESVecProteinRepresentation/source/utils/drug.chembl.canon.l1.ws20.txt\"\n",
    "EMBEDDING_DIM = 100\n",
    "# dimension and path\n",
    "\n",
    "f = open(EMBEDDING_PATH)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 1798\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "print(embedding_matrix.size)\n",
    "[X_train[\"num_vars\"].shape[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/ToxEnv/lib/python3.6/site-packages/keras/engine/training.py:378: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if metric in ('accuracy', 'acc', 'crossentropy', 'ce'):\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "('Could not interpret metric function identifier:', array([0.53191489, 0.52991453, 0.53218884, 0.53017241, 0.53246753,\n       0.53043478, 0.52838428, 0.53070175, 0.53303965, 0.53539823,\n       0.53333333, 0.53571429, 0.53811659, 0.53603604, 0.53846154,\n       0.53636364, 0.53424658, 0.53669725, 0.53456221, 0.53240741,\n       0.53023256, 0.53271028, 0.53051643, 0.53301887, 0.53554502,\n       0.53809524, 0.54066986, 0.54326923, 0.5410628 , 0.54368932,\n       0.54146341, 0.54411765, 0.54679803, 0.54455446, 0.54726368,\n       0.55      , 0.55276382, 0.55555556, 0.55329949, 0.55102041,\n       0.54871795, 0.55154639, 0.55440415, 0.55208333, 0.55497382,\n       0.55789474, 0.56084656, 0.55851064, 0.56149733, 0.55913978,\n       0.55675676, 0.55978261, 0.56284153, 0.56593407, 0.56353591,\n       0.56666667, 0.56424581, 0.56179775, 0.55932203, 0.5625    ,\n       0.56571429, 0.56896552, 0.56647399, 0.56976744, 0.57309942,\n       0.57647059, 0.57988166, 0.58333333, 0.58682635, 0.59036145,\n       0.59393939, 0.59146341, 0.59509202, 0.59259259, 0.59627329,\n       0.59375   , 0.59119497, 0.59493671, 0.59235669, 0.58974359,\n       0.59354839, 0.5974026 , 0.60130719, 0.60526316, 0.60927152,\n       0.61333333, 0.61073826, 0.60810811, 0.60544218, 0.60273973,\n       0.6       , 0.59722222, 0.6013986 , 0.59859155, 0.60283688,\n       0.60714286, 0.60431655, 0.60869565, 0.60583942, 0.61029412,\n       0.60740741, 0.60447761, 0.60902256, 0.61363636, 0.61068702,\n       0.61538462, 0.62015504, 0.625     , 0.62204724, 0.61904762,\n       0.624     , 0.62096774, 0.62601626, 0.62295082, 0.61983471,\n       0.625     , 0.62184874, 0.62711864, 0.62393162, 0.62068966,\n       0.62608696, 0.62280702, 0.61946903, 0.61607143, 0.62162162,\n       0.61818182, 0.62385321, 0.62962963, 0.63551402, 0.63207547,\n       0.63809524, 0.63461538, 0.63106796, 0.62745098, 0.63366337,\n       0.63      , 0.62626263, 0.62244898, 0.62886598, 0.63541667,\n       0.64210526, 0.63829787, 0.6344086 , 0.64130435, 0.63736264,\n       0.64444444, 0.65168539, 0.65909091, 0.65517241, 0.65116279,\n       0.64705882, 0.64285714, 0.65060241, 0.64634146, 0.64197531,\n       0.6375    , 0.64556962, 0.65384615, 0.64935065, 0.64473684,\n       0.64      , 0.63513514, 0.63013699, 0.63888889, 0.64788732,\n       0.65714286, 0.66666667, 0.66176471, 0.65671642, 0.66666667,\n       0.66153846, 0.671875  , 0.66666667, 0.67741935, 0.68852459,\n       0.68333333, 0.69491525, 0.68965517, 0.70175439, 0.71428571,\n       0.72727273, 0.74074074, 0.73584906, 0.73076923, 0.7254902 ,\n       0.72      , 0.71428571, 0.70833333, 0.70212766, 0.69565217,\n       0.68888889, 0.70454545, 0.72093023, 0.73809524, 0.73170732,\n       0.725     , 0.71794872, 0.73684211, 0.72972973, 0.75      ,\n       0.74285714, 0.73529412, 0.75757576, 0.75      , 0.77419355,\n       0.8       , 0.79310345, 0.82142857, 0.85185185, 0.84615385,\n       0.84      , 0.83333333, 0.82608696, 0.86363636, 0.85714286,\n       0.85      , 0.84210526, 0.83333333, 0.88235294, 0.875     ,\n       0.86666667, 0.85714286, 0.84615385, 0.83333333, 0.81818182,\n       0.9       , 0.88888889, 0.875     , 0.85714286, 0.83333333,\n       0.8       , 1.        , 1.        , 1.        , 1.        ,\n       1.        ]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-375-11cd6e3a8ddc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-375-11cd6e3a8ddc>\u001b[0m in \u001b[0;36mget_model_cnn\u001b[0;34m(X_train)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sigmoid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_vars\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary_crossentropy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmeasure\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ToxEnv/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    452\u001b[0m                 \u001b[0moutput_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnested_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                 \u001b[0moutput_weighted_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnested_weighted_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m                 \u001b[0mhandle_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m                 \u001b[0mhandle_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_weighted_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ToxEnv/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mhandle_metrics\u001b[0;34m(metrics, weights)\u001b[0m\n\u001b[1;32m    409\u001b[0m                     \u001b[0mmetric_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric_name_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m                     \u001b[0mmetric_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m                     \u001b[0mweighted_metric_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweighted_masked_objective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m                     \u001b[0;31m# Get metric name as string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ToxEnv/lib/python3.6/site-packages/keras/metrics.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(identifier)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         raise ValueError('Could not interpret '\n\u001b[0;32m---> 86\u001b[0;31m                          'metric function identifier:', identifier)\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: ('Could not interpret metric function identifier:', array([0.53191489, 0.52991453, 0.53218884, 0.53017241, 0.53246753,\n       0.53043478, 0.52838428, 0.53070175, 0.53303965, 0.53539823,\n       0.53333333, 0.53571429, 0.53811659, 0.53603604, 0.53846154,\n       0.53636364, 0.53424658, 0.53669725, 0.53456221, 0.53240741,\n       0.53023256, 0.53271028, 0.53051643, 0.53301887, 0.53554502,\n       0.53809524, 0.54066986, 0.54326923, 0.5410628 , 0.54368932,\n       0.54146341, 0.54411765, 0.54679803, 0.54455446, 0.54726368,\n       0.55      , 0.55276382, 0.55555556, 0.55329949, 0.55102041,\n       0.54871795, 0.55154639, 0.55440415, 0.55208333, 0.55497382,\n       0.55789474, 0.56084656, 0.55851064, 0.56149733, 0.55913978,\n       0.55675676, 0.55978261, 0.56284153, 0.56593407, 0.56353591,\n       0.56666667, 0.56424581, 0.56179775, 0.55932203, 0.5625    ,\n       0.56571429, 0.56896552, 0.56647399, 0.56976744, 0.57309942,\n       0.57647059, 0.57988166, 0.58333333, 0.58682635, 0.59036145,\n       0.59393939, 0.59146341, 0.59509202, 0.59259259, 0.59627329,\n       0.59375   , 0.59119497, 0.59493671, 0.59235669, 0.58974359,\n       0.59354839, 0.5974026 , 0.60130719, 0.60526316, 0.60927152,\n       0.61333333, 0.61073826, 0.60810811, 0.60544218, 0.60273973,\n       0.6       , 0.59722222, 0.6013986 , 0.59859155, 0.60283688,\n       0.60714286, 0.60431655, 0.60869565, 0.60583942, 0.61029412,\n       0.60740741, 0.60447761, 0.60902256, 0.61363636, 0.61068702,\n       0.61538462, 0.62015504, 0.625     , 0.62204724, 0.61904762,\n       0.624     , 0.62096774, 0.62601626, 0.62295082, 0.61983471,\n       0.625     , 0.62184874, 0.62711864, 0.62393162, 0.62068966,\n       0.62608696, 0.62280702, 0.61946903, 0.61607143, 0.62162162,\n       0.61818182, 0.62385321, 0.62962963, 0.63551402, 0.63207547,\n       0.63809524, 0.63461538, 0.63106796, 0.62745098, 0.63366337,\n       0.63      , 0.62626263, 0.62244898, 0.62886598, 0.63541667,\n       0.64210526, 0.63829787, 0.6344086 , 0.64130435, 0.63736264,\n       0.64444444, 0.65168539, 0.65909091, 0.65517241, 0.65116279,\n       0.64705882, 0.64285714, 0.65060241, 0.64634146, 0.64197531,\n       0.6375    , 0.64556962, 0.65384615, 0.64935065, 0.64473684,\n       0.64      , 0.63513514, 0.63013699, 0.63888889, 0.64788732,\n       0.65714286, 0.66666667, 0.66176471, 0.65671642, 0.66666667,\n       0.66153846, 0.671875  , 0.66666667, 0.67741935, 0.68852459,\n       0.68333333, 0.69491525, 0.68965517, 0.70175439, 0.71428571,\n       0.72727273, 0.74074074, 0.73584906, 0.73076923, 0.7254902 ,\n       0.72      , 0.71428571, 0.70833333, 0.70212766, 0.69565217,\n       0.68888889, 0.70454545, 0.72093023, 0.73809524, 0.73170732,\n       0.725     , 0.71794872, 0.73684211, 0.72972973, 0.75      ,\n       0.74285714, 0.73529412, 0.75757576, 0.75      , 0.77419355,\n       0.8       , 0.79310345, 0.82142857, 0.85185185, 0.84615385,\n       0.84      , 0.83333333, 0.82608696, 0.86363636, 0.85714286,\n       0.85      , 0.84210526, 0.83333333, 0.88235294, 0.875     ,\n       0.86666667, 0.85714286, 0.84615385, 0.83333333, 0.81818182,\n       0.9       , 0.88888889, 0.875     , 0.85714286, 0.83333333,\n       0.8       , 1.        , 1.        , 1.        , 1.        ,\n       1.        ]))"
     ]
    }
   ],
   "source": [
    "max_features = 30\n",
    "maxlen = 100\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import adam\n",
    "\n",
    "opt = adam(lr=0.01, decay=1e-6)\n",
    "\n",
    "def get_model_cnn(X_train):\n",
    "    \n",
    "    global embed_size\n",
    "    inp = Input(shape=(maxlen, ), name=\"smiles\")\n",
    "    num_vars = Input(shape=[X_train[\"num_vars\"].shape[1]], name=\"num_vars\")\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    z = GlobalMaxPool1D()(x)\n",
    "    x = GlobalMaxPool1D()(Conv1D(embed_size, 4, activation=\"relu\")(x))\n",
    "    x = Concatenate()([x,z,num_vars])\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=[inp,num_vars], outputs=x)\n",
    "    model.compile(loss=binary_crossentropy, optimizer=opt, metrics=['accuracy', precision, recall, fmeasure])\n",
    "    \n",
    "    return model   \n",
    "\n",
    "model = get_model_cnn(X_train)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K backend imported!\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "\n",
    "\n",
    "def binary_accuracy(y_true, y_pred):\n",
    "    return K.mean(K.equal(y_true, K.round(y_pred)))\n",
    "\n",
    "\n",
    "def categorical_accuracy(y_true, y_pred):\n",
    "    return K.mean(K.equal(K.argmax(y_true, axis=-1),\n",
    "                          K.argmax(y_pred, axis=-1)))\n",
    "\n",
    "\n",
    "def sparse_categorical_accuracy(y_true, y_pred):\n",
    "    return K.mean(K.equal(K.max(y_true, axis=-1),\n",
    "                          K.cast(K.argmax(y_pred, axis=-1), K.floatx())))\n",
    "\n",
    "\n",
    "def top_k_categorical_accuracy(y_true, y_pred, k=5):\n",
    "    return K.mean(K.in_top_k(y_pred, K.argmax(y_true, axis=-1), k))\n",
    "\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred - y_true))\n",
    "\n",
    "\n",
    "def mean_absolute_error(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_pred - y_true))\n",
    "\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    diff = K.abs((y_true - y_pred) / K.clip(K.abs(y_true),\n",
    "                                            K.epsilon(),\n",
    "                                            None))\n",
    "    return 100. * K.mean(diff)\n",
    "\n",
    "\n",
    "def mean_squared_logarithmic_error(y_true, y_pred):\n",
    "    first_log = K.log(K.clip(y_pred, K.epsilon(), None) + 1.)\n",
    "    second_log = K.log(K.clip(y_true, K.epsilon(), None) + 1.)\n",
    "    return K.mean(K.square(first_log - second_log))\n",
    "\n",
    "\n",
    "def hinge(y_true, y_pred):\n",
    "    return K.mean(K.maximum(1. - y_true * y_pred, 0.))\n",
    "\n",
    "\n",
    "def squared_hinge(y_true, y_pred):\n",
    "    return K.mean(K.square(K.maximum(1. - y_true * y_pred, 0.)))\n",
    "\n",
    "\n",
    "def categorical_crossentropy(y_true, y_pred):\n",
    "    return K.mean(K.categorical_crossentropy(y_pred, y_true))\n",
    "\n",
    "\n",
    "def sparse_categorical_crossentropy(y_true, y_pred):\n",
    "    return K.mean(K.sparse_categorical_crossentropy(y_pred, y_true))\n",
    "\n",
    "\n",
    "def binary_crossentropy(y_true, y_pred):\n",
    "    return K.mean(K.binary_crossentropy(y_pred, y_true))\n",
    "\n",
    "\n",
    "def kullback_leibler_divergence(y_true, y_pred):\n",
    "    y_true = K.clip(y_true, K.epsilon(), 1)\n",
    "    y_pred = K.clip(y_pred, K.epsilon(), 1)\n",
    "    return K.mean(K.sum(y_true * K.log(y_true / y_pred), axis=-1))\n",
    "\n",
    "\n",
    "def poisson(y_true, y_pred):\n",
    "    return K.mean(y_pred - y_true * K.log(y_pred + K.epsilon()))\n",
    "\n",
    "\n",
    "def cosine_proximity(y_true, y_pred):\n",
    "    y_true = K.l2_normalize(y_true, axis=-1)\n",
    "    y_pred = K.l2_normalize(y_pred, axis=-1)\n",
    "    return -K.mean(y_true * y_pred)\n",
    "\n",
    "def matthews_correlation(y_true, y_pred):\n",
    "    \"\"\"Matthews correlation metric.\n",
    "    It is only computed as a batch-wise average, not globally.\n",
    "    Computes the Matthews correlation coefficient measure for quality\n",
    "    of binary classification problems.\n",
    "    \"\"\"\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    "\n",
    "    y_pos = K.round(K.clip(y_true, 0, 1))\n",
    "    y_neg = 1 - y_pos\n",
    "\n",
    "    tp = K.sum(y_pos * y_pred_pos)\n",
    "    tn = K.sum(y_neg * y_pred_neg)\n",
    "\n",
    "    fp = K.sum(y_neg * y_pred_pos)\n",
    "    fn = K.sum(y_pos * y_pred_neg)\n",
    "\n",
    "    numerator = (tp * tn - fp * fn)\n",
    "    denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "\n",
    "    return numerator / (denominator + K.epsilon())\n",
    "\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "    Only computes a batch-wise average of precision.\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "    Only computes a batch-wise average of recall.\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def fbeta_score(y_true, y_pred, beta=1):\n",
    "    \"\"\"Computes the F score.\n",
    "    The F score is the weighted harmonic mean of precision and recall.\n",
    "    Here it is only computed as a batch-wise average, not globally.\n",
    "    This is useful for multi-label classification, where input samples can be\n",
    "    classified as sets of labels. By only using accuracy (precision) a model\n",
    "    would achieve a perfect score by simply assigning every class to every\n",
    "    input. In order to avoid this, a metric should penalize incorrect class\n",
    "    assignments as well (recall). The F-beta score (ranged from 0.0 to 1.0)\n",
    "    computes this, as a weighted mean of the proportion of correct class\n",
    "    assignments vs. the proportion of incorrect class assignments.\n",
    "    With beta = 1, this is equivalent to a F-measure. With beta < 1, assigning\n",
    "    correct classes becomes more important, and with beta > 1 the metric is\n",
    "    instead weighted towards penalizing incorrect class assignments.\n",
    "    \"\"\"\n",
    "    if beta < 0:\n",
    "        raise ValueError('The lowest choosable beta is zero (only precision).')\n",
    "\n",
    "    # If there are no true positives, fix the F score at 0 like sklearn.\n",
    "    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:\n",
    "        return 0\n",
    "\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    bb = beta ** 2\n",
    "    fbeta_score = (1 + bb) * (p * r) / (bb * p + r + K.epsilon())\n",
    "    return fbeta_score\n",
    "\n",
    "\n",
    "def fmeasure(y_true, y_pred):\n",
    "    \"\"\"Computes the f-measure, the harmonic mean of precision and recall.\n",
    "    Here it is only computed as a batch-wise average, not globally.\n",
    "    \"\"\"\n",
    "    return fbeta_score(y_true, y_pred, beta=1)\n",
    "\n",
    "\n",
    "# aliases\n",
    "mse = MSE = mean_squared_error\n",
    "mae = MAE = mean_absolute_error\n",
    "mape = MAPE = mean_absolute_percentage_error\n",
    "msle = MSLE = mean_squared_logarithmic_error\n",
    "cosine = cosine_proximity\n",
    "fscore = f1score = fmeasure\n",
    "print(\"K backend imported!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "328     0\n",
       "358     1\n",
       "43      0\n",
       "1055    0\n",
       "628     0\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "Y_train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y_trainf64 = Y_train.astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
       "       0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1.,\n",
       "       1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "       0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0.,\n",
       "       0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
       "       1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0.,\n",
       "       1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0.])"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_testf64 = Y_test.astype(np.float64).values\n",
    "Y_testf64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "le = LabelBinarizer()\n",
    "Yl_train = le.fit_transform(Y_train)\n",
    "# Yl_train\n",
    "import sklearn.metrics as sklm\n",
    "from keras.callbacks import Callback\n",
    "# making class Metrics\n",
    "class Metrics(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.confusion = []\n",
    "        self.precision = []\n",
    "        self.recall = []\n",
    "        self.f1s = []\n",
    "        self.kappa = []\n",
    "        self.auc = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        score = np.asarray(self.model.predict(self.validation_data[0]))\n",
    "        predict = np.round(np.asarray(self.model.predict(self.validation_data[0])))\n",
    "        targ = self.validation_data[1]\n",
    "\n",
    "        self.auc.append(sklm.roc_auc_score(targ, score))\n",
    "        self.confusion.append(sklm.confusion_matrix(targ, predict))\n",
    "        self.precision.append(sklm.precision_score(targ, predict))\n",
    "        auc = sklm.roc_auc_score(targ, score)\n",
    "        prec = sklm.precision_score(targ, predict)\n",
    "        self.recall.append(sklm.recall_score(targ, predict))\n",
    "        rec = sklm.recall_score(targ, predict)\n",
    "        self.f1s.append(sklm.f1_score(targ, predict, pos_label=1))\n",
    "        f = sklm.f1_score(targ, predict, pos_label=1)\n",
    "        self.kappa.append(sklm.cohen_kappa_score(targ, predict))\n",
    "\n",
    "        print(\"\\nepoch: %d == AUC: %.6f == Prec: %.6f == Rec: %.6f == F1: %.6f \" %\n",
    "              (epoch + 1, auc, prec, rec, f))\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "kmetrics = Metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'smiles': array([[ 0,  0,  0, ...,  1,  1,  6],\n",
       "        [ 0,  0,  0, ...,  3, 19,  1],\n",
       "        [ 0,  0,  0, ...,  6,  3,  1],\n",
       "        ...,\n",
       "        [ 0,  0,  0, ...,  1,  3,  4],\n",
       "        [ 0,  0,  0, ...,  3,  1, 12],\n",
       "        [ 0,  0,  0, ...,  1,  3,  1]], dtype=int32),\n",
       " 'num_vars': array([[0.08779506, 0.09206065, 0.2       , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.08338062, 0.08002749, 0.        , ..., 0.        , 0.        ,\n",
       "         0.00108198],\n",
       "        [0.01346517, 0.01304467, 0.        , ..., 0.06651872, 0.        ,\n",
       "         0.        ],\n",
       "        ...,\n",
       "        [0.06694081, 0.06430076, 0.        , ..., 0.00844209, 0.        ,\n",
       "         0.        ],\n",
       "        [0.08300219, 0.07681889, 0.        , ..., 0.02531853, 0.        ,\n",
       "         0.01988009],\n",
       "        [0.09008543, 0.09121451, 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ]])}"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_71\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "smiles (InputLayer)             (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_113 (Embedding)       (None, 30, 100)      3000        smiles[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_68 (SpatialDr (None, 30, 100)      0           embedding_113[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_90 (Conv1D)              (None, 27, 100)      40100       spatial_dropout1d_68[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_147 (Globa (None, 100)          0           conv1d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_146 (Globa (None, 100)          0           spatial_dropout1d_68[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "num_vars (InputLayer)           (None, 1798)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_68 (Concatenate)    (None, 1998)         0           global_max_pooling1d_147[0][0]   \n",
      "                                                                 global_max_pooling1d_146[0][0]   \n",
      "                                                                 num_vars[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_82 (Dropout)            (None, 1998)         0           concatenate_68[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_88 (Dense)                (None, 1)            1999        dropout_82[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 45,099\n",
      "Trainable params: 45,099\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix.size\n",
    "X_test\n",
    "\n",
    "def get_model_cnn(X_train):\n",
    "    \n",
    "    global embed_size\n",
    "    inp = Input(shape=(len(word_index)+1, ), name=\"smiles\")\n",
    "    num_vars = Input(shape=[X_train[\"num_vars\"].shape[1]], name=\"num_vars\")\n",
    "    x = Embedding(len(word_index)+1, 100, weights=[embedding_matrix])(inp)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    z = GlobalMaxPool1D()(x)\n",
    "    x = GlobalMaxPool1D()(Conv1D(embed_size, 4, activation=\"relu\")(x))\n",
    "    x = Concatenate()([x,z,num_vars])\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=[inp,num_vars], outputs=x)\n",
    "    model.compile(loss=binary_crossentropy, optimizer='adam', metrics=[precision, recall, fmeasure])\n",
    "    \n",
    "    return model   \n",
    "\n",
    "model = get_model_cnn(X_train)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'smiles': array([[ 0,  0,  0, ...,  1,  1,  6],\n",
       "        [ 0,  0,  0, ...,  3, 19,  1],\n",
       "        [ 0,  0,  0, ...,  6,  3,  1],\n",
       "        ...,\n",
       "        [ 0,  0,  0, ...,  1,  3,  4],\n",
       "        [ 0,  0,  0, ...,  3,  1, 12],\n",
       "        [ 0,  0,  0, ...,  1,  3,  1]], dtype=int32),\n",
       " 'num_vars': array([[0.08779506, 0.09206065, 0.2       , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.08338062, 0.08002749, 0.        , ..., 0.        , 0.        ,\n",
       "         0.00108198],\n",
       "        [0.01346517, 0.01304467, 0.        , ..., 0.06651872, 0.        ,\n",
       "         0.        ],\n",
       "        ...,\n",
       "        [0.06694081, 0.06430076, 0.        , ..., 0.00844209, 0.        ,\n",
       "         0.        ],\n",
       "        [0.08300219, 0.07681889, 0.        , ..., 0.02531853, 0.        ,\n",
       "         0.01988009],\n",
       "        [0.09008543, 0.09121451, 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ]])}"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-20\n",
      "100 Dimension Embeddings Used: sliding window of 8 characters of SMILES \n",
      "\n",
      "start modeling\n",
      "Model: \"model_86\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "smiles (InputLayer)             (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_130 (Embedding)       (None, 100, 100)     3000        smiles[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_83 (SpatialDr (None, 100, 100)     0           embedding_130[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_105 (Conv1D)             (None, 91, 100)      100100      spatial_dropout1d_83[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_177 (Globa (None, 100)          0           conv1d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_176 (Globa (None, 100)          0           spatial_dropout1d_83[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "num_vars (InputLayer)           (None, 1798)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_83 (Concatenate)    (None, 1998)         0           global_max_pooling1d_177[0][0]   \n",
      "                                                                 global_max_pooling1d_176[0][0]   \n",
      "                                                                 num_vars[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_97 (Dropout)            (None, 1998)         0           concatenate_83[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_103 (Dense)               (None, 1)            1999        dropout_97[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 105,099\n",
      "Trainable params: 105,099\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/120\n",
      " - 9s - loss: 7.8765 - acc: 0.4968 - precision: 0.5403 - recall: 0.7237 - fmeasure: 0.5723\n",
      "Epoch 2/120\n",
      " - 0s - loss: 7.3404 - acc: 0.5446 - precision: 0.5446 - recall: 1.0000 - fmeasure: 0.7050\n",
      "Epoch 3/120\n",
      " - 0s - loss: 7.3404 - acc: 0.5446 - precision: 0.5446 - recall: 1.0000 - fmeasure: 0.7049\n",
      "Epoch 4/120\n",
      " - 0s - loss: 7.3404 - acc: 0.5446 - precision: 0.5446 - recall: 1.0000 - fmeasure: 0.7050\n",
      "Epoch 5/120\n",
      " - 0s - loss: 7.3404 - acc: 0.5446 - precision: 0.5446 - recall: 1.0000 - fmeasure: 0.7049\n",
      "Epoch 6/120\n",
      " - 0s - loss: 7.3404 - acc: 0.5446 - precision: 0.5446 - recall: 1.0000 - fmeasure: 0.7050\n",
      "Epoch 7/120\n",
      " - 0s - loss: 7.3404 - acc: 0.5446 - precision: 0.5446 - recall: 1.0000 - fmeasure: 0.7043\n",
      "Epoch 8/120\n",
      " - 0s - loss: 7.3404 - acc: 0.5446 - precision: 0.5446 - recall: 1.0000 - fmeasure: 0.7051\n",
      "Epoch 9/120\n",
      " - 0s - loss: 7.3404 - acc: 0.5446 - precision: 0.5446 - recall: 1.0000 - fmeasure: 0.7047\n",
      "Epoch 10/120\n",
      " - 0s - loss: 7.3404 - acc: 0.5446 - precision: 0.5446 - recall: 1.0000 - fmeasure: 0.7051\n",
      "Epoch 11/120\n",
      " - 0s - loss: 7.3404 - acc: 0.5446 - precision: 0.5446 - recall: 1.0000 - fmeasure: 0.7047\n",
      "Epoch 12/120\n",
      " - 0s - loss: 7.3404 - acc: 0.5446 - precision: 0.5446 - recall: 1.0000 - fmeasure: 0.7051\n",
      "Epoch 13/120\n",
      " - 0s - loss: 7.3404 - acc: 0.5446 - precision: 0.5446 - recall: 1.0000 - fmeasure: 0.7050\n",
      "Epoch 14/120\n",
      " - 0s - loss: 7.3404 - acc: 0.5446 - precision: 0.5446 - recall: 1.0000 - fmeasure: 0.7049\n",
      "Epoch 15/120\n",
      " - 0s - loss: 7.3404 - acc: 0.5446 - precision: 0.5446 - recall: 1.0000 - fmeasure: 0.7049\n",
      "Epoch 16/120\n",
      " - 0s - loss: 7.3404 - acc: 0.5446 - precision: 0.5446 - recall: 1.0000 - fmeasure: 0.7050\n",
      "Epoch 17/120\n",
      " - 0s - loss: 7.3404 - acc: 0.5446 - precision: 0.5446 - recall: 1.0000 - fmeasure: 0.7042\n",
      "Epoch 18/120\n",
      " - 0s - loss: 7.3404 - acc: 0.5446 - precision: 0.5446 - recall: 1.0000 - fmeasure: 0.7051\n",
      "Epoch 19/120\n",
      " - 0s - loss: 7.3404 - acc: 0.5446 - precision: 0.5446 - recall: 1.0000 - fmeasure: 0.7027\n",
      "Epoch 20/120\n",
      " - 0s - loss: 7.3404 - acc: 0.5446 - precision: 0.5446 - recall: 1.0000 - fmeasure: 0.7045\n",
      "Epoch 21/120\n",
      " - 0s - loss: 7.3404 - acc: 0.5446 - precision: 0.5446 - recall: 1.0000 - fmeasure: 0.7049\n",
      "Epoch 22/120\n",
      " - 0s - loss: 7.3404 - acc: 0.5446 - precision: 0.5446 - recall: 1.0000 - fmeasure: 0.7038\n",
      "Epoch 23/120\n",
      " - 0s - loss: 7.3404 - acc: 0.5446 - precision: 0.5446 - recall: 1.0000 - fmeasure: 0.7052\n",
      "Epoch 24/120\n",
      " - 0s - loss: 7.3404 - acc: 0.5446 - precision: 0.5446 - recall: 1.0000 - fmeasure: 0.7043\n",
      "Epoch 25/120\n",
      " - 0s - loss: 7.3404 - acc: 0.5446 - precision: 0.5446 - recall: 1.0000 - fmeasure: 0.7049\n",
      "Epoch 26/120\n",
      " - 0s - loss: 7.3404 - acc: 0.5446 - precision: 0.5446 - recall: 1.0000 - fmeasure: 0.7046\n",
      "Epoch 27/120\n",
      " - 0s - loss: 7.3404 - acc: 0.5446 - precision: 0.5446 - recall: 1.0000 - fmeasure: 0.7050\n",
      "Epoch 28/120\n",
      " - 0s - loss: 7.3404 - acc: 0.5446 - precision: 0.5446 - recall: 1.0000 - fmeasure: 0.7049\n",
      "Epoch 29/120\n",
      " - 0s - loss: 7.3404 - acc: 0.5446 - precision: 0.5446 - recall: 1.0000 - fmeasure: 0.7049\n",
      "Epoch 30/120\n",
      " - 0s - loss: 7.3404 - acc: 0.5446 - precision: 0.5446 - recall: 1.0000 - fmeasure: 0.7048\n",
      "Epoch 31/120\n",
      " - 0s - loss: 7.3404 - acc: 0.5446 - precision: 0.5446 - recall: 1.0000 - fmeasure: 0.7050\n",
      "Epoch 32/120\n",
      " - 0s - loss: 7.3404 - acc: 0.5446 - precision: 0.5446 - recall: 1.0000 - fmeasure: 0.7044\n",
      "Epoch 33/120\n",
      " - 0s - loss: 7.3404 - acc: 0.5446 - precision: 0.5446 - recall: 1.0000 - fmeasure: 0.7046\n",
      "Epoch 34/120\n",
      " - 0s - loss: 7.3404 - acc: 0.5446 - precision: 0.5446 - recall: 1.0000 - fmeasure: 0.7051\n",
      "Epoch 35/120\n",
      " - 0s - loss: 7.3404 - acc: 0.5446 - precision: 0.5446 - recall: 1.0000 - fmeasure: 0.7048\n",
      "Epoch 36/120\n",
      " - 0s - loss: 7.3404 - acc: 0.5446 - precision: 0.5446 - recall: 1.0000 - fmeasure: 0.7048\n",
      "Epoch 37/120\n",
      " - 0s - loss: 7.3404 - acc: 0.5446 - precision: 0.5446 - recall: 1.0000 - fmeasure: 0.7051\n",
      "Epoch 38/120\n",
      " - 0s - loss: 7.3404 - acc: 0.5446 - precision: 0.5446 - recall: 1.0000 - fmeasure: 0.7049\n",
      "Epoch 39/120\n",
      " - 0s - loss: 7.3404 - acc: 0.5446 - precision: 0.5446 - recall: 1.0000 - fmeasure: 0.7050\n",
      "Epoch 40/120\n",
      " - 0s - loss: 7.3404 - acc: 0.5446 - precision: 0.5446 - recall: 1.0000 - fmeasure: 0.7050\n",
      "Epoch 41/120\n",
      " - 0s - loss: 7.3404 - acc: 0.5446 - precision: 0.5446 - recall: 1.0000 - fmeasure: 0.7047\n",
      "Epoch 42/120\n",
      " - 0s - loss: 7.3404 - acc: 0.5446 - precision: 0.5446 - recall: 1.0000 - fmeasure: 0.7038\n",
      "Epoch 43/120\n",
      " - 0s - loss: 7.3404 - acc: 0.5446 - precision: 0.5446 - recall: 1.0000 - fmeasure: 0.7047\n",
      "Epoch 44/120\n",
      " - 0s - loss: 7.3404 - acc: 0.5446 - precision: 0.5446 - recall: 1.0000 - fmeasure: 0.7050\n",
      "Epoch 45/120\n",
      " - 0s - loss: 7.3404 - acc: 0.5446 - precision: 0.5446 - recall: 1.0000 - fmeasure: 0.7041\n",
      "Epoch 46/120\n",
      " - 0s - loss: 7.3404 - acc: 0.5446 - precision: 0.5446 - recall: 1.0000 - fmeasure: 0.7050\n",
      "Epoch 47/120\n",
      " - 0s - loss: 7.3404 - acc: 0.5446 - precision: 0.5446 - recall: 1.0000 - fmeasure: 0.7051\n",
      "Epoch 48/120\n",
      " - 0s - loss: 7.3404 - acc: 0.5446 - precision: 0.5446 - recall: 1.0000 - fmeasure: 0.7048\n",
      "Epoch 49/120\n",
      " - 0s - loss: 7.3404 - acc: 0.5446 - precision: 0.5446 - recall: 1.0000 - fmeasure: 0.7048\n",
      "Epoch 50/120\n",
      " - 0s - loss: 7.3404 - acc: 0.5446 - precision: 0.5446 - recall: 1.0000 - fmeasure: 0.7050\n",
      "Epoch 51/120\n",
      " - 0s - loss: 7.3404 - acc: 0.5446 - precision: 0.5446 - recall: 1.0000 - fmeasure: 0.7051\n",
      "Epoch 52/120\n",
      " - 0s - loss: 7.3404 - acc: 0.5446 - precision: 0.5446 - recall: 1.0000 - fmeasure: 0.7047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "628/628 [==============================] - 4s 6ms/step\n",
      "cv score:  0.5\n",
      "Epoch 1/120\n",
      " - 0s - loss: 7.2891 - acc: 0.5478 - precision: 0.5478 - recall: 1.0000 - fmeasure: 0.7070\n",
      "Epoch 2/120\n",
      " - 0s - loss: 7.2891 - acc: 0.5478 - precision: 0.5478 - recall: 1.0000 - fmeasure: 0.7076\n",
      "Epoch 3/120\n",
      " - 0s - loss: 7.2891 - acc: 0.5478 - precision: 0.5478 - recall: 1.0000 - fmeasure: 0.7077\n",
      "Epoch 4/120\n",
      " - 0s - loss: 7.2891 - acc: 0.5478 - precision: 0.5478 - recall: 1.0000 - fmeasure: 0.7074\n",
      "Epoch 5/120\n",
      " - 0s - loss: 7.2891 - acc: 0.5478 - precision: 0.5478 - recall: 1.0000 - fmeasure: 0.7076\n",
      "Epoch 6/120\n",
      " - 0s - loss: 7.2891 - acc: 0.5478 - precision: 0.5478 - recall: 1.0000 - fmeasure: 0.7070\n",
      "Epoch 7/120\n",
      " - 0s - loss: 7.2891 - acc: 0.5478 - precision: 0.5478 - recall: 1.0000 - fmeasure: 0.7077\n",
      "Epoch 8/120\n",
      " - 0s - loss: 7.2891 - acc: 0.5478 - precision: 0.5478 - recall: 1.0000 - fmeasure: 0.7067\n",
      "Epoch 9/120\n",
      " - 0s - loss: 7.2891 - acc: 0.5478 - precision: 0.5478 - recall: 1.0000 - fmeasure: 0.7072\n",
      "Epoch 10/120\n",
      " - 0s - loss: 7.2891 - acc: 0.5478 - precision: 0.5478 - recall: 1.0000 - fmeasure: 0.7075\n",
      "Epoch 11/120\n",
      " - 0s - loss: 7.2891 - acc: 0.5478 - precision: 0.5478 - recall: 1.0000 - fmeasure: 0.7076\n",
      "Epoch 12/120\n",
      " - 0s - loss: 7.2891 - acc: 0.5478 - precision: 0.5478 - recall: 1.0000 - fmeasure: 0.7077\n",
      "Epoch 13/120\n",
      " - 0s - loss: 7.2891 - acc: 0.5478 - precision: 0.5478 - recall: 1.0000 - fmeasure: 0.7075\n",
      "Epoch 14/120\n",
      " - 0s - loss: 7.2891 - acc: 0.5478 - precision: 0.5478 - recall: 1.0000 - fmeasure: 0.7076\n",
      "Epoch 15/120\n",
      " - 0s - loss: 7.2891 - acc: 0.5478 - precision: 0.5478 - recall: 1.0000 - fmeasure: 0.7077\n",
      "Epoch 16/120\n",
      " - 0s - loss: 7.2891 - acc: 0.5478 - precision: 0.5478 - recall: 1.0000 - fmeasure: 0.7074\n",
      "Epoch 17/120\n",
      " - 0s - loss: 7.2891 - acc: 0.5478 - precision: 0.5478 - recall: 1.0000 - fmeasure: 0.7073\n",
      "Epoch 18/120\n",
      " - 0s - loss: 7.2891 - acc: 0.5478 - precision: 0.5478 - recall: 1.0000 - fmeasure: 0.7078\n",
      "Epoch 19/120\n",
      " - 0s - loss: 7.2891 - acc: 0.5478 - precision: 0.5478 - recall: 1.0000 - fmeasure: 0.7076\n",
      "Epoch 20/120\n",
      " - 0s - loss: 7.2891 - acc: 0.5478 - precision: 0.5478 - recall: 1.0000 - fmeasure: 0.7071\n",
      "Epoch 21/120\n",
      " - 0s - loss: 7.2891 - acc: 0.5478 - precision: 0.5478 - recall: 1.0000 - fmeasure: 0.7078\n",
      "Epoch 22/120\n",
      " - 0s - loss: 7.2891 - acc: 0.5478 - precision: 0.5478 - recall: 1.0000 - fmeasure: 0.7072\n",
      "Epoch 23/120\n",
      " - 0s - loss: 7.2891 - acc: 0.5478 - precision: 0.5478 - recall: 1.0000 - fmeasure: 0.7069\n",
      "Epoch 24/120\n",
      " - 0s - loss: 7.2891 - acc: 0.5478 - precision: 0.5478 - recall: 1.0000 - fmeasure: 0.7077\n",
      "Epoch 25/120\n",
      " - 0s - loss: 7.2891 - acc: 0.5478 - precision: 0.5478 - recall: 1.0000 - fmeasure: 0.7074\n",
      "Epoch 26/120\n",
      " - 0s - loss: 7.2891 - acc: 0.5478 - precision: 0.5478 - recall: 1.0000 - fmeasure: 0.7074\n",
      "Epoch 27/120\n",
      " - 0s - loss: 7.2891 - acc: 0.5478 - precision: 0.5478 - recall: 1.0000 - fmeasure: 0.7066\n",
      "Epoch 28/120\n",
      " - 0s - loss: 7.2891 - acc: 0.5478 - precision: 0.5478 - recall: 1.0000 - fmeasure: 0.7074\n",
      "Epoch 29/120\n",
      " - 0s - loss: 7.2891 - acc: 0.5478 - precision: 0.5478 - recall: 1.0000 - fmeasure: 0.7075\n",
      "Epoch 30/120\n",
      " - 0s - loss: 7.2891 - acc: 0.5478 - precision: 0.5478 - recall: 1.0000 - fmeasure: 0.7076\n",
      "Epoch 31/120\n",
      " - 0s - loss: 7.2891 - acc: 0.5478 - precision: 0.5478 - recall: 1.0000 - fmeasure: 0.7078\n",
      "Epoch 32/120\n",
      " - 0s - loss: 7.2891 - acc: 0.5478 - precision: 0.5478 - recall: 1.0000 - fmeasure: 0.7074\n",
      "Epoch 33/120\n",
      " - 0s - loss: 7.2891 - acc: 0.5478 - precision: 0.5478 - recall: 1.0000 - fmeasure: 0.7077\n",
      "Epoch 34/120\n",
      " - 0s - loss: 7.2891 - acc: 0.5478 - precision: 0.5478 - recall: 1.0000 - fmeasure: 0.7078\n",
      "Epoch 35/120\n",
      " - 0s - loss: 7.2891 - acc: 0.5478 - precision: 0.5478 - recall: 1.0000 - fmeasure: 0.7076\n",
      "Epoch 36/120\n",
      " - 0s - loss: 7.2891 - acc: 0.5478 - precision: 0.5478 - recall: 1.0000 - fmeasure: 0.7078\n",
      "Epoch 37/120\n",
      " - 0s - loss: 7.2891 - acc: 0.5478 - precision: 0.5478 - recall: 1.0000 - fmeasure: 0.7074\n",
      "Epoch 38/120\n",
      " - 0s - loss: 7.2891 - acc: 0.5478 - precision: 0.5478 - recall: 1.0000 - fmeasure: 0.7076\n",
      "Epoch 39/120\n",
      " - 0s - loss: 7.2891 - acc: 0.5478 - precision: 0.5478 - recall: 1.0000 - fmeasure: 0.7076\n",
      "Epoch 40/120\n",
      " - 0s - loss: 7.2891 - acc: 0.5478 - precision: 0.5478 - recall: 1.0000 - fmeasure: 0.7077\n",
      "Epoch 41/120\n",
      " - 0s - loss: 7.2891 - acc: 0.5478 - precision: 0.5478 - recall: 1.0000 - fmeasure: 0.7076\n",
      "Epoch 42/120\n",
      " - 0s - loss: 7.2891 - acc: 0.5478 - precision: 0.5478 - recall: 1.0000 - fmeasure: 0.7075\n",
      "Epoch 43/120\n",
      " - 0s - loss: 7.2891 - acc: 0.5478 - precision: 0.5478 - recall: 1.0000 - fmeasure: 0.7069\n",
      "Epoch 44/120\n",
      " - 0s - loss: 7.2891 - acc: 0.5478 - precision: 0.5478 - recall: 1.0000 - fmeasure: 0.7077\n",
      "Epoch 45/120\n",
      " - 0s - loss: 7.2891 - acc: 0.5478 - precision: 0.5478 - recall: 1.0000 - fmeasure: 0.7075\n",
      "Epoch 46/120\n",
      " - 0s - loss: 7.2891 - acc: 0.5478 - precision: 0.5478 - recall: 1.0000 - fmeasure: 0.7078\n",
      "Epoch 47/120\n",
      " - 0s - loss: 7.2891 - acc: 0.5478 - precision: 0.5478 - recall: 1.0000 - fmeasure: 0.7077\n",
      "Epoch 48/120\n",
      " - 0s - loss: 7.2891 - acc: 0.5478 - precision: 0.5478 - recall: 1.0000 - fmeasure: 0.7077\n",
      "Epoch 49/120\n",
      " - 0s - loss: 7.2891 - acc: 0.5478 - precision: 0.5478 - recall: 1.0000 - fmeasure: 0.7078\n",
      "Epoch 50/120\n",
      " - 0s - loss: 7.2891 - acc: 0.5478 - precision: 0.5478 - recall: 1.0000 - fmeasure: 0.7078\n",
      "Epoch 51/120\n",
      " - 0s - loss: 7.2891 - acc: 0.5478 - precision: 0.5478 - recall: 1.0000 - fmeasure: 0.7069\n",
      "628/628 [==============================] - 0s 181us/step\n",
      "cv score:  0.5\n",
      "Epoch 1/120\n",
      " - 0s - loss: 6.8271 - acc: 0.5764 - precision: 0.5764 - recall: 1.0000 - fmeasure: 0.7311\n",
      "Epoch 2/120\n",
      " - 0s - loss: 6.8271 - acc: 0.5764 - precision: 0.5764 - recall: 1.0000 - fmeasure: 0.7312\n",
      "Epoch 3/120\n",
      " - 0s - loss: 6.8271 - acc: 0.5764 - precision: 0.5764 - recall: 1.0000 - fmeasure: 0.7312\n",
      "Epoch 4/120\n",
      " - 0s - loss: 6.8271 - acc: 0.5764 - precision: 0.5764 - recall: 1.0000 - fmeasure: 0.7303\n",
      "Epoch 5/120\n",
      " - 0s - loss: 6.8271 - acc: 0.5764 - precision: 0.5764 - recall: 1.0000 - fmeasure: 0.7311\n",
      "Epoch 6/120\n",
      " - 0s - loss: 6.8271 - acc: 0.5764 - precision: 0.5764 - recall: 1.0000 - fmeasure: 0.7311\n",
      "Epoch 7/120\n",
      " - 0s - loss: 6.8271 - acc: 0.5764 - precision: 0.5764 - recall: 1.0000 - fmeasure: 0.7313\n",
      "Epoch 8/120\n",
      " - 0s - loss: 6.8271 - acc: 0.5764 - precision: 0.5764 - recall: 1.0000 - fmeasure: 0.7308\n",
      "Epoch 9/120\n",
      " - 0s - loss: 6.8271 - acc: 0.5764 - precision: 0.5764 - recall: 1.0000 - fmeasure: 0.7312\n",
      "Epoch 10/120\n",
      " - 0s - loss: 6.8271 - acc: 0.5764 - precision: 0.5764 - recall: 1.0000 - fmeasure: 0.7311\n",
      "Epoch 11/120\n",
      " - 0s - loss: 6.8271 - acc: 0.5764 - precision: 0.5764 - recall: 1.0000 - fmeasure: 0.7312\n",
      "Epoch 12/120\n",
      " - 0s - loss: 6.8271 - acc: 0.5764 - precision: 0.5764 - recall: 1.0000 - fmeasure: 0.7311\n",
      "Epoch 13/120\n",
      " - 0s - loss: 6.8271 - acc: 0.5764 - precision: 0.5764 - recall: 1.0000 - fmeasure: 0.7313\n",
      "Epoch 14/120\n",
      " - 0s - loss: 6.8271 - acc: 0.5764 - precision: 0.5764 - recall: 1.0000 - fmeasure: 0.7308\n",
      "Epoch 15/120\n",
      " - 0s - loss: 6.8271 - acc: 0.5764 - precision: 0.5764 - recall: 1.0000 - fmeasure: 0.7311\n",
      "Epoch 16/120\n",
      " - 0s - loss: 6.8271 - acc: 0.5764 - precision: 0.5764 - recall: 1.0000 - fmeasure: 0.7312\n",
      "Epoch 17/120\n",
      " - 0s - loss: 6.8271 - acc: 0.5764 - precision: 0.5764 - recall: 1.0000 - fmeasure: 0.7308\n",
      "Epoch 18/120\n",
      " - 0s - loss: 6.8271 - acc: 0.5764 - precision: 0.5764 - recall: 1.0000 - fmeasure: 0.7311\n",
      "Epoch 19/120\n",
      " - 0s - loss: 6.8271 - acc: 0.5764 - precision: 0.5764 - recall: 1.0000 - fmeasure: 0.7310\n",
      "Epoch 20/120\n",
      " - 0s - loss: 6.8271 - acc: 0.5764 - precision: 0.5764 - recall: 1.0000 - fmeasure: 0.7313\n",
      "Epoch 21/120\n",
      " - 0s - loss: 6.8271 - acc: 0.5764 - precision: 0.5764 - recall: 1.0000 - fmeasure: 0.7311\n",
      "Epoch 22/120\n",
      " - 0s - loss: 6.8271 - acc: 0.5764 - precision: 0.5764 - recall: 1.0000 - fmeasure: 0.7311\n",
      "Epoch 23/120\n",
      " - 0s - loss: 6.8271 - acc: 0.5764 - precision: 0.5764 - recall: 1.0000 - fmeasure: 0.7309\n",
      "Epoch 24/120\n",
      " - 0s - loss: 6.8271 - acc: 0.5764 - precision: 0.5764 - recall: 1.0000 - fmeasure: 0.7312\n",
      "Epoch 25/120\n",
      " - 0s - loss: 6.8271 - acc: 0.5764 - precision: 0.5764 - recall: 1.0000 - fmeasure: 0.7311\n",
      "Epoch 26/120\n",
      " - 0s - loss: 6.8271 - acc: 0.5764 - precision: 0.5764 - recall: 1.0000 - fmeasure: 0.7306\n",
      "Epoch 27/120\n",
      " - 0s - loss: 6.8271 - acc: 0.5764 - precision: 0.5764 - recall: 1.0000 - fmeasure: 0.7313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/120\n",
      " - 0s - loss: 6.8271 - acc: 0.5764 - precision: 0.5764 - recall: 1.0000 - fmeasure: 0.7308\n",
      "Epoch 29/120\n",
      " - 0s - loss: 6.8271 - acc: 0.5764 - precision: 0.5764 - recall: 1.0000 - fmeasure: 0.7296\n",
      "Epoch 30/120\n",
      " - 0s - loss: 6.8271 - acc: 0.5764 - precision: 0.5764 - recall: 1.0000 - fmeasure: 0.7302\n",
      "Epoch 31/120\n",
      " - 0s - loss: 6.8271 - acc: 0.5764 - precision: 0.5764 - recall: 1.0000 - fmeasure: 0.7298\n",
      "Epoch 32/120\n",
      " - 0s - loss: 6.8271 - acc: 0.5764 - precision: 0.5764 - recall: 1.0000 - fmeasure: 0.7312\n",
      "Epoch 33/120\n",
      " - 0s - loss: 6.8271 - acc: 0.5764 - precision: 0.5764 - recall: 1.0000 - fmeasure: 0.7304\n",
      "Epoch 34/120\n",
      " - 0s - loss: 6.8271 - acc: 0.5764 - precision: 0.5764 - recall: 1.0000 - fmeasure: 0.7312\n",
      "Epoch 35/120\n",
      " - 0s - loss: 6.8271 - acc: 0.5764 - precision: 0.5764 - recall: 1.0000 - fmeasure: 0.7292\n",
      "Epoch 36/120\n",
      " - 0s - loss: 6.8271 - acc: 0.5764 - precision: 0.5764 - recall: 1.0000 - fmeasure: 0.7312\n",
      "Epoch 37/120\n",
      " - 0s - loss: 6.8271 - acc: 0.5764 - precision: 0.5764 - recall: 1.0000 - fmeasure: 0.7312\n",
      "Epoch 38/120\n",
      " - 0s - loss: 6.8271 - acc: 0.5764 - precision: 0.5764 - recall: 1.0000 - fmeasure: 0.7308\n",
      "Epoch 39/120\n",
      " - 0s - loss: 6.8271 - acc: 0.5764 - precision: 0.5764 - recall: 1.0000 - fmeasure: 0.7305\n",
      "Epoch 40/120\n",
      " - 0s - loss: 6.8271 - acc: 0.5764 - precision: 0.5764 - recall: 1.0000 - fmeasure: 0.7300\n",
      "Epoch 41/120\n",
      " - 0s - loss: 6.8271 - acc: 0.5764 - precision: 0.5764 - recall: 1.0000 - fmeasure: 0.7311\n",
      "Epoch 42/120\n",
      " - 0s - loss: 6.8271 - acc: 0.5764 - precision: 0.5764 - recall: 1.0000 - fmeasure: 0.7313\n",
      "Epoch 43/120\n",
      " - 0s - loss: 6.8271 - acc: 0.5764 - precision: 0.5764 - recall: 1.0000 - fmeasure: 0.7304\n",
      "Epoch 44/120\n",
      " - 0s - loss: 6.8271 - acc: 0.5764 - precision: 0.5764 - recall: 1.0000 - fmeasure: 0.7308\n",
      "Epoch 45/120\n",
      " - 0s - loss: 6.8271 - acc: 0.5764 - precision: 0.5764 - recall: 1.0000 - fmeasure: 0.7311\n",
      "Epoch 46/120\n",
      " - 0s - loss: 6.8271 - acc: 0.5764 - precision: 0.5764 - recall: 1.0000 - fmeasure: 0.7312\n",
      "Epoch 47/120\n",
      " - 0s - loss: 6.8271 - acc: 0.5764 - precision: 0.5764 - recall: 1.0000 - fmeasure: 0.7308\n",
      "Epoch 48/120\n",
      " - 0s - loss: 6.8271 - acc: 0.5764 - precision: 0.5764 - recall: 1.0000 - fmeasure: 0.7307\n",
      "Epoch 49/120\n",
      " - 0s - loss: 6.8271 - acc: 0.5764 - precision: 0.5764 - recall: 1.0000 - fmeasure: 0.7312\n",
      "Epoch 50/120\n",
      " - 0s - loss: 6.8271 - acc: 0.5764 - precision: 0.5764 - recall: 1.0000 - fmeasure: 0.7308\n",
      "Epoch 51/120\n",
      " - 0s - loss: 6.8271 - acc: 0.5764 - precision: 0.5764 - recall: 1.0000 - fmeasure: 0.7313\n",
      "628/628 [==============================] - 0s 186us/step\n",
      "cv score:  0.5\n",
      "Total CV score is 0.5\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import CSVLogger\n",
    "# from keras.callbacks import History\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "print(today)\n",
    "max_features = 30\n",
    "maxlen = 100\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='acc', factor=0.5,\n",
    "                              patience=50, min_lr=0.001)\n",
    "csv_logger = CSVLogger('training_logs/' + str(today) + '_' + 'embedcharacter' + '_GRUPOOL.training.log',\n",
    "                       append=False)\n",
    "\n",
    "early = EarlyStopping(monitor='acc', min_delta=0.0001, patience=50)\n",
    "\n",
    "\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "print(\"100 Dimension Embeddings Used: sliding window of 8 characters of SMILES \\n\")\n",
    "print('start modeling')\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import adam\n",
    "\n",
    "opt = adam(lr=0.01, decay=1e-6)\n",
    "\n",
    "def get_model_cnn(X_train):\n",
    "    \n",
    "    global embed_dim\n",
    "    inp = Input(shape=(maxlen, ), name=\"smiles\")\n",
    "    num_vars = Input(shape=[X_train[\"num_vars\"].shape[1]], name=\"num_vars\")\n",
    "    x = Embedding(30, embed_size, weights=[embedding_matrix])(inp)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    z = GlobalMaxPool1D()(x)\n",
    "    x = GlobalMaxPool1D()(Conv1D(embed_dim, 10, activation=\"relu\")(x))\n",
    "    x = Concatenate()([x,z,num_vars])\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=[inp,num_vars], outputs=x)\n",
    "    model.compile(loss=binary_crossentropy, optimizer=opt, metrics=['accuracy', precision, recall, fmeasure])\n",
    "    return model   \n",
    "\n",
    "model = get_model_cnn(X_train)\n",
    "model.summary()\n",
    "\n",
    "scores = []\n",
    "predict = np.zeros((X_testdf.shape[0],1))\n",
    "oof_predict = np.zeros((X_traindf.shape[0],1))\n",
    "\n",
    "acc_scores = []\n",
    "num_folds = 3\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=239)\n",
    "for train_index, test_index in kf.split(X_train['num_vars']):\n",
    "    kfold_X_train = {}\n",
    "    kfold_X_valid = {}\n",
    "    cvy_train,cvy_test = Yl_train[train_index], Yl_train[test_index]\n",
    "    for c in ['smiles','num_vars']:\n",
    "        kfold_X_train[c] = X_train[c][train_index]\n",
    "        kfold_X_valid[c] = X_train[c][test_index]\n",
    "\n",
    " \n",
    "    hist = model.fit(kfold_X_train, cvy_train, batch_size=256,epochs=120, callbacks=[early, reduce_lr, csv_logger], verbose=2)\n",
    "    model.evaluate(kfold_X_train, cvy_train) #, batch_size=256, epochs=120, callbacks=[early, reduce_lr, csv_logger])\n",
    "    predict += model.predict(X_test, batch_size=1000) / num_folds\n",
    "    oof_predict[test_index] = model.predict(kfold_X_valid, batch_size=1000)\n",
    "    \n",
    "    cv_score = sklm.roc_auc_score(cvy_test, oof_predict[test_index])\n",
    "   \n",
    "    sklm.average_precision_score(cvy_test, oof_predict[test_index])\n",
    "#     print(model.metrics_names)#, model.metrics_tensors)\n",
    "    scores.append(cv_score)\n",
    "\n",
    "    print('cv score: ', cv_score)\n",
    "\n",
    "    \n",
    "\n",
    "print('Total CV score is {}'.format(np.mean(scores)))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('loss', [6.827091444829467, 6.827091347639728, 6.8270915329076685, 6.82709128993332, 6.82709142964357, 6.827091138074352, 6.82709114718589, 6.82709147823844, 6.8270912990448585, 6.827091235264092, 6.82709147823844, 6.827091493424336, 6.82709147823844, 6.827091624023049, 6.827091396234597, 6.82709124437563, 6.827091444829467, 6.82709142964357, 6.827091396234597, 6.82709147823844, 6.827091265635885, 6.827091590614076, 6.82709142964357, 6.8270912838589615, 6.827091639208946, 6.827091235264092, 6.827091444829467, 6.8270912838589615, 6.827091250449988, 6.827091201855119, 6.827091326379472, 6.827091332453831, 6.827091347639728, 6.827091435717929, 6.82709152683331, 6.827091611874331, 6.827091493424336, 6.827091721212788, 6.8270913810487, 6.827091396234597, 6.8270913810487, 6.827091235264092, 6.827091362825625, 6.827091396234597, 6.82709124437563, 6.827091311193575, 6.827091590614076, 6.827091542019207, 6.827091174520505, 6.827091198817939, 6.827091341565369]), ('acc', [0.5764331191208711, 0.5764331217784031, 0.5764331217784031, 0.5764331217784031, 0.5764331206394608, 0.5764331206394608, 0.576433122537698, 0.5764331206394608, 0.5764331217784031, 0.5764331232969928, 0.5764331229173454, 0.5764331195005186, 0.5764331206394608, 0.5764331229173454, 0.5764331191208711, 0.5764331202598134, 0.5764331217784031, 0.5764331232969928, 0.5764331195005186, 0.5764331229173454, 0.5764331210191083, 0.5764331217784031, 0.5764331229173454, 0.5764331206394608, 0.5764331213987557, 0.5764331229173454, 0.5764331217784031, 0.5764331232969928, 0.5764331195005186, 0.5764331217784031, 0.5764331187412237, 0.5764331206394608, 0.5764331213987557, 0.576433119880166, 0.5764331206394608, 0.576433119880166, 0.5764331195005186, 0.5764331229173454, 0.5764331202598134, 0.5764331213987557, 0.5764331202598134, 0.5764331202598134, 0.5764331210191083, 0.5764331195005186, 0.576433122537698, 0.5764331221580505, 0.5764331191208711, 0.5764331217784031, 0.5764331202598134, 0.5764331215885794, 0.576433122537698]), ('precision', [0.5764331191208711, 0.5764331217784031, 0.5764331217784031, 0.5764331217784031, 0.5764331206394608, 0.5764331206394608, 0.576433122537698, 0.5764331206394608, 0.5764331217784031, 0.5764331232969928, 0.5764331229173454, 0.5764331195005186, 0.5764331206394608, 0.5764331229173454, 0.5764331191208711, 0.5764331202598134, 0.5764331217784031, 0.5764331232969928, 0.5764331195005186, 0.5764331229173454, 0.5764331210191083, 0.5764331217784031, 0.5764331229173454, 0.5764331206394608, 0.5764331213987557, 0.5764331229173454, 0.5764331217784031, 0.5764331232969928, 0.5764331195005186, 0.5764331217784031, 0.5764331187412237, 0.5764331206394608, 0.5764331213987557, 0.576433119880166, 0.5764331206394608, 0.576433119880166, 0.5764331195005186, 0.5764331229173454, 0.5764331202598134, 0.5764331213987557, 0.5764331202598134, 0.5764331202598134, 0.5764331210191083, 0.5764331195005186, 0.576433122537698, 0.5764331221580505, 0.5764331191208711, 0.5764331217784031, 0.5764331202598134, 0.5764331215885794, 0.576433122537698]), ('recall', [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]), ('fmeasure', [0.7289796687994793, 0.7300135941262458, 0.7296923941867367, 0.7303222444406741, 0.7299881828059057, 0.7306750530649901, 0.7300596343483895, 0.7304693206100706, 0.7284155943591124, 0.7292775709158296, 0.7306100578065131, 0.7297038219536945, 0.7308955279884825, 0.7286129787469365, 0.729969090716854, 0.7289776365468457, 0.7308206273491975, 0.7283600268849901, 0.7302300254250788, 0.729735550607086, 0.729918037250543, 0.729672166572255, 0.730024331694196, 0.7300774941019191, 0.7288043984941616, 0.7292846752579805, 0.730078595838729, 0.7285800097854274, 0.7303195311005708, 0.7303327348581545, 0.7259637208501245, 0.7302431661611909, 0.7289254300913234, 0.7296602570327224, 0.7297946172914688, 0.7291608657806542, 0.7297603013409171, 0.7302353852873396, 0.7293202546751423, 0.7300021526919809, 0.7296993979223215, 0.7294584159638472, 0.7299638561382416, 0.7301253217041113, 0.7282696726975167, 0.7284098852212262, 0.7300988390187549, 0.7300266600718164, 0.7279989051211412, 0.7281126243293665, 0.7286606973903195]), ('lr', [0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025])])\n"
     ]
    }
   ],
   "source": [
    "#, #class_weight={0: 1, 1: 90})\n",
    "# val_weights={0:1, 1:90}\n",
    "import pprint\n",
    "pprint.pprint(history.history.items())\n",
    "# print(pd.DataFrame(kmetrics))\n",
    "# print('confusion\\n', kmetrics.confusion, 'precision\\n', kmetrics.precision, 'recall\\n',\n",
    "# kmetrics.recall, 'f1\\n', kmetrics.f1s, 'kappa\\n', kmetrics.kappa, 'auc\\n', kmetrics.auc)\n",
    "# print(\"\\nmodel predicting.. with batch size 1024 ( large batch size )\")\n",
    "predictions = model.predict(X_test, batch_size=1024)\n",
    "# predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'val_acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-403-5c4dd5583789>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# print(\"type predictions:\", type(prediction_classes))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'val_acc'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# predictions_binary = [round(value) for value in predictions]\n",
    "# print(\"predictions binary\\n\", predictions_binary)\n",
    "# print(type(predictions_binary))\n",
    "\n",
    "# print(\"type predictions:\", type(prediction_classes))\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig('pngs/gru_acc_' + str(today) + '_' + embed_name + '.png')\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig('pngs/gru_loss_' + str(today) + '_' + embed_name + '.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D \n",
    "from keras.utils import plot_model\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "import os, re, csv, math, codecs\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "MAX_NB_WORDS = 100000\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "#load embeddings\n",
    "print('loading word embeddings...')\n",
    "embeddings_index = {}\n",
    "f = codecs.open('/root/SMILESVecProteinRepresentation/source/utils/drug.pubchem.canon.l8.ws20.txt', encoding='utf-8')\n",
    "for line in tqdm(f):\n",
    "    values = line.rstrip().rsplit(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('found %s word vectors' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([X_traindf, Y_train], axis=1)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['doc_len'] = train_df['Smiles'].apply(lambda words: len(words.split(\" \")))\n",
    "max_seq_len = np.round(train_df['doc_len'].mean() + train_df['doc_len'].std()).astype(int)\n",
    "sns.distplot(train_df['doc_len'], hist=True, kde=True, color='b', label='doc len')\n",
    "plt.axvline(x=max_seq_len, color='k', linestyle='--', label='max len')\n",
    "plt.title('Smiles Length'); plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## THIRD MODEL\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)\n",
    "print(\"importing important packages!\")\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import CSVLogger\n",
    "# from keras.callbacks import History\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "import tensorflow as tf\n",
    "\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "from keras import backend as K\n",
    "import datetime\n",
    "\n",
    "today = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "# printing out date\n",
    "print(today)\n",
    "\n",
    "\n",
    "\n",
    "# making weighted binary cross entropy fxn\n",
    "def weighted_binary_crossentropy(y_true, y_pred, weight=1.):\n",
    "    y_true = K.clip(y_true, K.epsilon(), 1)\n",
    "    y_pred = K.clip(y_pred, K.epsilon(), 1)\n",
    "    logloss = -(y_true * K.log(y_pred) * weight + (1 - y_true) * K.log(1 - y_pred))\n",
    "    return K.mean(logloss, axis=-1)\n",
    "\n",
    "\n",
    "print(\"making environment!!\")\n",
    "embed_name = 'chembl8'  # 'WikiPubMedQNotes300DIM'\n",
    "print(\"using the following pre-trained embeddings:\", embed_name)\n",
    "#.r/PycharmProjects/fastText-0.2.0/result/dim300WikiNotesLRpoint01.vec\n",
    "EMBEDDING_FILE = '/root/SMILESVecProteinRepresentation/source/utils/drug.l8.chembl23.canon.ws20.txt'#'~/PycharmProjects/fastText-0.2.0/result/' + embed_name + '.vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = X_traindf\n",
    "print(\"submission df and training and test made!\")\n",
    "\n",
    "X_train = train[\"Smiles\"].fillna(\"fill na\").values\n",
    "print('type train[\"text\"].fillna(\"fill na\").values', type(X_train))\n",
    "# print(\"type(train[['text','label']].values)\", type(train[['text','label']].values))\n",
    "y_train = Y_train\n",
    "print(\"train['label'] type\", type(X_train.shape))\n",
    "test = X_testdf\n",
    "X_test = test[\"Smiles\"].fillna(\"fill na\").values\n",
    "y_test = Y_test\n",
    "print(\"made X_train and y_train and X_test and y_test\")\n",
    "max_features = 29  # thinking about max features -- may change this\n",
    "maxlen =   100 # using max length as 2500\n",
    "embed_size = 100\n",
    "print(\"tokenizing text...\")\n",
    "tokenizer = text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(list(X_train) + list(X_test))\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "x_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "print(\"making embedding matrix...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "\n",
    "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print(\"length of imbedding matrix:\", len(embedding_matrix))\n",
    "from keras.callbacks import Callback\n",
    "import sklearn.metrics as sklm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making class Metrics\n",
    "class Metrics(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.confusion = []|\n",
    "        self.precision = []\n",
    "        self.recall = []\n",
    "        self.f1s = []\n",
    "        self.kappa = []\n",
    "        self.auc = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        score = np.asarray(self.model.predict(self.validation_data[0]))\n",
    "        predict = np.round(np.asarray(self.model.predict(self.validation_data[0])))\n",
    "        targ = self.validation_data[1]\n",
    "\n",
    "        self.auc.append(sklm.roc_auc_score(targ, score))\n",
    "        self.confusion.append(sklm.confusion_matrix(targ, predict))\n",
    "        self.precision.append(sklm.precision_score(targ, predict))\n",
    "        auc = sklm.roc_auc_score(targ, score)\n",
    "        prec = sklm.precision_score(targ, predict)\n",
    "        self.recall.append(sklm.recall_score(targ, predict))\n",
    "        rec = sklm.recall_score(targ, predict)\n",
    "        self.f1s.append(sklm.f1_score(targ, predict, pos_label=1))\n",
    "        f = sklm.f1_score(targ, predict, pos_label=1)\n",
    "        self.kappa.append(sklm.cohen_kappa_score(targ, predict))\n",
    "\n",
    "        print(\"\\nepoch: %d == AUC: %.6f == Prec: %.6f == Rec: %.6f == F1: %.6f \" %\n",
    "              (epoch + 1, auc, prec, rec, f))\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "kmetrics = Metrics()\n",
    "embed_name = \"pubcheml1\"\n",
    "csv_logger = CSVLogger('training_logs/' + str(today) + '_' + embed_name + '_GRUPOOL.training.log',\n",
    "                       append=False)\n",
    "early = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=50, verbose=1)\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    inp = Input(shape=(maxlen,))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = Bidirectional(GRU(80, return_sequences=True))(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    outp = Dense(1, activation=\"sigmoid\")(conc)\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss=weighted_binary_crossentropy,\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc', precision, recall, fmeasure])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = get_model()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32  # trying 128 again because may be large enough to have class weight\n",
    "epochs = 60\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "                              patience=50, min_lr=0.001)\n",
    "\n",
    "print(\"final shape of x_train:\", x_train.shape)\n",
    "print(\"final type of x_train:\", type(x_train))\n",
    "print(\"final shape of x_test:\", x_test.shape)\n",
    "print(\"final type of x_test:\", type(x_test))\n",
    "print(\"final type of y_train:\", type(y_train))\n",
    "print(\"\\nfitting model..\")\n",
    "\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test),\n",
    "                    callbacks=[reduce_lr, csv_logger, early, kmetrics], verbose=2)#, #class_weight={0: 1, 1: 90})\n",
    "# val_weights={0:1, 1:90}\n",
    "print(history.history.keys())\n",
    "print(pd.DataFrame(kmetrics))\n",
    "print('confusion\\n', kmetrics.confusion, 'precision\\n', kmetrics.precision, 'recall\\n',\n",
    "kmetrics.recall, 'f1\\n', kmetrics.f1s, 'kappa\\n', kmetrics.kappa, 'auc\\n', kmetrics.auc)\n",
    "print(\"\\nmodel predicting.. with batch size 1024 ( large batch size )\")\n",
    "predictions = model.predict(x_test, batch_size=1024)\n",
    "print(predictions)\n",
    "print(type(predictions))\n",
    "predictions_binary = [round(value) for value in predictions]\n",
    "print(\"predictions binary\\n\", predictions_binary)\n",
    "print(type(predictions_binary))\n",
    "prediction_classes = np.round(np.asarray(model.predict(x_test, batch_size=1024)))\n",
    "print(prediction_classes)\n",
    "print(\"type predictions:\", type(prediction_classes))\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig('pngs/gru_acc_' + str(today) + '_' + embed_name + '.png')\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig('pngs/gru_loss_' + str(today) + '_' + embed_name + '.png')\n",
    "print(sklm.classification_report(prediction_classes))\n",
    "print(\"trying to reshape repdictions\", predictions.reshape(-1, 1))\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to keep training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick up epoch on instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_coefs(word,*arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "embeddings_index = dict(get_coefs(*o.split(' '))\n",
    "        for o in open('/root/SMILESVecProteinRepresentation/source/utils/drug.l8.chembl23.canon.ws20.txt'))\n",
    "\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "\n",
    "print('creating embedding matrix ...')\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ha!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
